{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.163733Z",
     "start_time": "2023-05-31T02:42:22.636891Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim.utils import DimensionlessLearning\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from vics_fcns import top_split_y#, top_split_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#First, we must load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.572932Z",
     "start_time": "2023-05-31T02:42:27.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.ExcelFile(r\"mass_balance_params.xlsx\")\n",
    "## packed\n",
    "\n",
    "packed_or_not = 'packed'\n",
    "\n",
    "if packed_or_not == 'packed':\n",
    "    df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\data\\data_packing\\collected_output_files_packing\\data_from_EB_looping_all_sizes_remove_extraneous_dims.xlsx')\n",
    "## unpacked\n",
    "elif packed_or_not =='unpacked':\n",
    "    df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\data\\data_no_packing_all\\data_from_EB_looping_new_excel_new_code_no_packing_v3.xlsx')\n",
    "else:\n",
    "    print('packed or unpacked?')\n",
    "df_y = pd.ExcelFile(r\"energy_balance_params.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.660878Z",
     "start_time": "2023-05-31T02:42:27.634288Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_x['u_B_m_s'] = df_train_x['u_B_m_s'] / 1000 if np.min(df_train_x['u_B_m_s']) > 0 else df_train_x['u_B_m_s'] / 1000 - (np.min(df_train_x['u_B_m_s']) / 1000-1e-5)\n",
    "# df_train_x['A_tot_m2'] = 1.\n",
    "# df_train_x['t_a_s'] = df_train_x['t_a_s'] / 1e-7 if np.min(df_train_x['t_a_s']) > 0 else df_train_x['t_a_s'] / 1e-7 - (np.min(df_train_x['t_a_s']) / 1e-7-1e-5)\n",
    "# df_train_x['t_b_s'] = df_train_x['t_b_s'] / 1e-4 if np.min(df_train_x['t_b_s']) > 0 else df_train_x['t_b_s'] / 1e-4 - (np.min(df_train_x['t_b_s']) / 1e-4-1e-5)\n",
    "# df_train_x['Volume_m3'] = 1.\n",
    "# df_train_x['K_iz_a_m3_s_atom'] = df_train_x['K_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_iz_a_m3_s_atom']) > 0 else df_train_x['K_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_2_iz_a_m3_s_atom'] = df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_2_iz_a_m3_s_atom']) > 0 else df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_2_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_iz_exc_a_m3_s_atom'] = df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 if np.min(df_train_x['K_iz_exc_a_m3_s_atom']) > 0 else df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 - (np.min(df_train_x['K_iz_exc_a_m3_s_atom']) / 1e-14-1e-5)\n",
    "\n",
    "# df_train_x['n_sa_atoms_m3'] = df_train_x['n_sa_atoms_m3'] / 1e10 if np.min(df_train_x['n_sa_atoms_m3']) > 0 else df_train_x['n_sa_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sa_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['n_sb_atoms_m3'] = df_train_x['n_sb_atoms_m3'] / 1e10 if np.min(df_train_x['n_sb_atoms_m3']) > 0 else df_train_x['n_sb_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sb_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['A_a_m2'] = 1.\n",
    "\n",
    "# df_train_x['A_b_m2'] = 1.\n",
    "\n",
    "# df_train_x['n_He_exc_a_atoms_m3'] = df_train_x['n_He_exc_a_atoms_m3'] / 1e17 if np.min(df_train_x['n_He_exc_a_atoms_m3']) > 0 else df_train_x['n_He_exc_a_atoms_m3'] / 1e17 - (np.min(df_train_x['n_He_exc_a_atoms_m3']) / 1e17-1e-5)\n",
    "\n",
    "def rescale(g):\n",
    "    for i in range(0,g.shape[1]):\n",
    "        if np.min(np.abs(g[:,i])) ==0:\n",
    "            n = 0\n",
    "        else:\n",
    "            n = np.mean((np.log10(np.min(np.abs(g[:,i]))), np.log10(np.max(np.abs(g[:,i])))))\n",
    "        if n<0:\n",
    "                g[:,i] = g[:,i]/10**np.ceil(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.ceil(n) - (np.min(g[:,i]) / 10**np.ceil(n)-1e-5)\n",
    "        else:\n",
    "            g[:,i] = g[:,i]/10**np.floor(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.floor(n) - (np.min(g[:,i]) / 10**np.floor(n)-1e-5)\n",
    "    return g\n",
    "\n",
    "def rescale_vec(g):\n",
    "    if np.min(np.abs(g)) ==0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = np.mean((np.log10(np.min(np.abs(g))), np.log10(np.max(np.abs(g)))))\n",
    "    if n<0:\n",
    "            g= g/10**np.ceil(n) if np.min(g) > 0 else g/10**np.ceil(n) - (np.min(g) / 10**np.ceil(n)-1e-5)\n",
    "    else:\n",
    "        g = g/10**np.floor(n) if np.min(g) > 0 else g/10**np.floor(n) - (np.min(g) / 10**np.floor(n)-1e-5)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.706539Z",
     "start_time": "2023-05-31T02:42:27.668685Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "#                   0             1           2                    3                   4                   5           6                7                    8                     9                        10                  11                  12                      13                          \n",
    "#df1_names = [ 't_a_seconds', 'Q_a_As', 'V_p_ta_kgm2_s3_A','T_e_a_kgm2_s3_A', 'n_He_exc_a_atoms_m3', 'u_B_a_m_s', 'v_e_a_m_s', 'K_2_iz_a_m3_s_atom','K_loss_a_m6_s_atom2', 'K_iz_exc_a_m3_s_atom', 'K_exc_a_m3_s_atom', 'K_iz_a_m3_s_atom', 'K_elastic_a_m3_s_atom', 'E_elastic_a_kgm2_s2' ]\n",
    "\n",
    "\n",
    "df_time_a = rescale(np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:]))\n",
    "df_time_a_units = np.array(pd.read_excel(df_2, sheet_name='time_a_data_units').iloc[:,1:])\n",
    "df_time_a_n = pd.read_excel(df_2, sheet_name='time_a_data_names').iloc[:,1:]\n",
    "time_a_n= [df_time_a_n.iloc[0,i] for i in range(0,df_time_a_n.shape[1])]\n",
    "#print(df_time_a_n)\n",
    "#print(df_time_a_units)\n",
    "\n",
    "\n",
    "#                   0             1                2               3                   4              5          6             7                   8                    9                       10              11                   12                   13                  \n",
    "#df2_names =  [ 't_b_seconds', 'Q_b_As', 'V_p_tb_kgm2_s3_A','T_e_kgm2_s3_A', 'n_He_exc_atoms_m3', 'u_B_m_s', 'v_e_m_s','K_2_iz_m3_s_atom','K_loss_m6_s_atom2', 'K_iz_exc_m3_s_atom', 'K_exc_m3_s_atom', 'K_iz_m3_s_atom', 'K_elastic_m3_s_atom', 'E_elastic_kgm2_s2' ]\n",
    "df_time_b = rescale(np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]))\n",
    "df_time_b_units = np.array(pd.read_excel(df_2, sheet_name='time_b_data_units').iloc[:,1:])\n",
    "df_time_b_n = pd.read_excel(df_2, sheet_name='time_b_data_names').iloc[:,1:]\n",
    "time_b_n= [df_time_b_n.iloc[0,i] for i in range(0,df_time_b_n.shape[1])]\n",
    "\n",
    "\n",
    "#                    0                   1                 2               3                 4               5               6                 7                  8           9        10        11                12                   13               14                   \n",
    "#df3_names = ['E_period_kgm2_s2', 'n_sa_atoms_m3','n_sb_atoms_m3', 'n_e_electrons_m3', 'n_g_atoms_m3', 'T_g_kelvin', 'E_iz_kgm2_s2', 'E_iz_exc_kgm2_s2', 'E_exc_kgm2_s2', 'e_c_As', 'm_e_kg', 'M_He_kg',  'epsilon_A2s4_kg_m3', 'eps_0_A2s4_kg_m3', 'k_b_kgm2_s2_K']\n",
    "df_other = rescale(np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]))\n",
    "df_other_units = np.array(pd.read_excel(df_2, sheet_name='other_data_units').iloc[:,1:])\n",
    "df_other_n = pd.read_excel(df_2, sheet_name='other_data_names').iloc[:,1:]\n",
    "other_n= [df_other_n.iloc[0,i] for i in range(0,df_other_n.shape[1])]\n",
    "\n",
    "#                    0                 1             2        3        4         5            6            7            8               9              10                 11                      12                      \n",
    "#df4_names = ['Volume_rxtor_m2', 'V_all_beads_m2','A_a_m2','A_b_m2', 'h_m', 'Volume_m3', 'A_bead_m2', 'A_tot_m3', 'frequency_Hz', 'Flow_m3_s', 'temp_C_gas_K', 'Set_Voltage_kgm2_s3_A', 'pulse_time_seconds' ]\n",
    "df_exp = rescale(np.array(pd.read_excel(df_2, sheet_name='Experiment_Design_data').iloc[:,1:]))\n",
    "df_exp_units = np.array(pd.read_excel(df_2, sheet_name='Experiment_Data_units').iloc[:,1:])\n",
    "df_exp_n = pd.read_excel(df_2, sheet_name='Experiment_Data_names').iloc[:,1:]\n",
    "exp_n= [df_exp_n.iloc[0,i] for i in range(0,df_exp_n.shape[1])]\n",
    "\n",
    "\n",
    "\n",
    "#energy balance expanded terms\n",
    "#terms to add: epsilon, other[12]\n",
    "a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "\n",
    "\n",
    "\n",
    "# get gamma's!\n",
    "time_a= np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:])\n",
    "time_b = np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:])\n",
    "other_data = np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:])\n",
    "\n",
    "K_iz_exc_a = time_a[:,9]\n",
    "K_exc_a = time_a[:,10]\n",
    "K_iz_a = time_a[:,11]\n",
    "K_ela_a = time_a[:,12]\n",
    "E_ela_a = time_a[:,13]\n",
    "\n",
    "K_iz_exc = time_b[:,9]\n",
    "K_exc = time_b[:,10]\n",
    "K_iz = time_b[:,11]\n",
    "K_ela = time_b[:,12]\n",
    "E_ela = time_b[:,13]\n",
    "\n",
    "E_iz_exc = other_data[:,6] \n",
    "E_exc = other_data[:,7]\n",
    "E_iz = other_data[:,8]\n",
    "\n",
    "\n",
    "# time a gammas\n",
    "y_iz_exc_a = np.multiply(K_iz_exc_a, E_iz_exc).reshape(a,1)\n",
    "y_iz_exc_a_units = df_time_a_units[:,9].reshape(6,1)+df_other_units[:,6].reshape(6,1)\n",
    "#print(y_iz_exc_a_units)\n",
    "\n",
    "y_exc_a = np.multiply(K_exc_a, E_exc).reshape(a,1)\n",
    "y_exc_a_units = df_time_a_units[:,10].reshape(6,1)+df_other_units[:,7].reshape(6,1)\n",
    "#print(y_exc_a_units)\n",
    "\n",
    "y_iz_a = np.multiply(K_iz_a, E_iz).reshape(a,1)\n",
    "y_iz_a_units = df_time_a_units[:,11].reshape(6,1)+df_other_units[:,8].reshape(6,1)\n",
    "#print(y_iz_a_units)\n",
    "\n",
    "y_ela_a = np.multiply(K_ela_a, E_ela_a).reshape(a,1)\n",
    "y_ela_a_units = df_time_a_units[:,12].reshape(6,1)+ df_time_a_units[:,13].reshape(6,1)\n",
    "#print(y_iz_a_units)\n",
    "\n",
    "y_iz_exc = np.multiply(K_iz_exc, E_iz_exc).reshape(a,1)\n",
    "y_iz_exc_units = df_time_b_units[:,9].reshape(6,1)+df_other_units[:,6].reshape(6,1)\n",
    "#print(y_iz_exc_units)\n",
    "\n",
    "y_exc = np.multiply(K_exc, E_exc).reshape(a,1)\n",
    "y_exc_units = df_time_b_units[:,10].reshape(6,1)+df_other_units[:,7].reshape(6,1)\n",
    "#print(y_exc_units)\n",
    "\n",
    "\n",
    "y_iz = np.multiply(K_iz, E_iz).reshape(a,1)\n",
    "y_iz_units = df_time_b_units[:,10].reshape(6,1)+df_other_units[:,7].reshape(6,1)\n",
    "#print(y_iz_units)\n",
    "\n",
    "y_ela = np.multiply(K_ela, E_ela).reshape(a,1)\n",
    "y_ela_units = df_time_b_units[:,12].reshape(6,1)+ df_time_b_units[:,13].reshape(6,1)\n",
    "#print(y_ela_units)\n",
    "\n",
    "df_gamma = rescale(np.hstack((y_iz_exc_a, y_exc_a, y_iz_a, y_ela_a, y_iz_exc, y_exc, y_iz, y_ela)))\n",
    "gamma_units = np.hstack((y_iz_exc_a_units, y_exc_a_units, y_iz_a_units, y_ela_a_units, y_iz_exc_units, y_exc_units, y_iz_units, y_ela_units))\n",
    "gamma_n = ['y_iz_exc_a', 'y_exc_a', 'y_iz_a', 'y_ela_a', 'y_iz_exc', 'y_exc', 'y_iz', 'y_ela' ]\n",
    "\n",
    "# y_exc_a = gammas[:,8] #K_exc_a * E_exc_a\n",
    "# y_exc_a_units = np.array([5,-3,-2,1,0,0])  #WRONG SHOULD BE [5,-3,-2,1,0,0]\n",
    "\n",
    "# y_iz_a = gammas[:,9]#K_iz_a*E_iz\n",
    "# y_iz_a_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# y_iz_exc_a = gammas[:,10]#K_iz_exc_a*E_iz_exc\n",
    "# y_iz_exc_a_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# y_elastic = gammas[:,11]#K_elastic*E_elastic\n",
    "# y_elastic_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# inputs = np.hstack((df_exp[:,5].reshape(a,1),df_other[:,3].reshape(a,1),df_other[:,4].reshape(a,1),df_other[:,9].reshape(a,1),df_exp[:,7].reshape(a,1),df_other[:,0].reshape(a,1),y_exc_a.reshape(a,1),y_iz_a.reshape(a,1),y_iz_exc_a.reshape(a,1),y_elastic.reshape(a,1),df_time_a[:,0].reshape(a,1), df_other[:,1].reshape(a,1), df_other[:,2].reshape(a,1),df_time_a[:,2].reshape(a,1),df_time_b[:,0].reshape(a,1), df_time_b[:,5].reshape(a,1),df_time_b[:,2].reshape(a,1),df_time_b[:,6].reshape(a,1),df_time_a[:,4].reshape(a,1), df_other[:,12].reshape(a,1)))\n",
    "# D_in = np.hstack((df_exp_units[:,5].reshape(6,1),df_other_units[:,3].reshape(6,1),df_other_units[:,4].reshape(6,1),df_other_units[:,9].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_other_units[:,0].reshape(6,1),y_exc_a_units.reshape(6,1),y_iz_a_units.reshape(6,1),y_iz_exc_a_units.reshape(6,1),y_elastic_units.reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,2].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_time_b_units[:,5].reshape(6,1),df_time_b_units[:,2].reshape(6,1),df_time_b_units[:,6].reshape(6,1),df_time_a_units[:,4].reshape(6,1), df_other_units[:,12].reshape(6,1)))\n",
    "# variables = [exp_n[5],other_n[3],other_n[4],other_n[9],exp_n[7],other_n[0],'y_exc_a_kgm5_s3_atoms2','y_iz_a_kgm5_s3_atoms2','y_iz_exc_a_kgm5_s3_atoms2','y_elastic_kgm5_s3_atoms2',time_a_n[0],other_n[1],other_n[2],time_a_n[2],time_b_n[0],time_b_n[5],time_b_n[2],time_b_n[6],time_a_n[4],other_n[12]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#energy balance terms\n",
    "# #now to construct  'Volume_m3', 'n_e_atoms_m3', 'n_g_atoms_m3', 'e_c_As', 'A_tot', 'E_kgm2_s2_atom', 'y_exc_a_kgm5_s3_atoms2', 'y_iz_a_kgm5_s3_atoms2', 'y_iz_exc_a_kgm5_s3_atoms2', 'y_elastic_kgm5_s3_atoms2', 't_a_s', 'n_sa_atoms_m3', 'n_sb_atoms_m3', 'V_p_ta_volts', 't_b_s', 'u_B_m_s', 'V_p_tb_volts', 'v_e_m_s', 'n_He_exc_a_atoms_m3'  \n",
    "# a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "\n",
    "# # get gamma's!\n",
    "# gammas= rescale(np.array(pd.read_excel(df_y, sheet_name='inputs').iloc[:,1:]))\n",
    "\n",
    "# y_exc_a = gammas[:,8] #K_exc_a * E_exc_a\n",
    "# y_exc_a_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# y_iz_a = gammas[:,9]#K_iz_a*E_iz\n",
    "# y_iz_a_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# y_iz_exc_a = gammas[:,10]#K_iz_exc_a*E_iz_exc\n",
    "# y_iz_exc_a_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "# y_elastic = gammas[:,11]#K_elastic*E_elastic\n",
    "# y_elastic_units = np.array([5,-3,-2,1,0,0])\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These next cells are about assembling process variables for dimmensionless numbers. Uncomment the cell you wish to run.\n",
    "\n",
    "# You must prepare inputs (which contain the rescaled data), D_in (the dimensions matrix), and for ease of trackign I recommend your produce variable names (variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # two major rates  for packed \n",
    "# # # include E in, iomization ta, \n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[4], V df_exp[5] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[0], n_e df_other[3]\n",
    "# ion_ta_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,2].reshape(a,1) , df_time_a[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# ion_ta_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,2].reshape(6,1) , df_time_a_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# ion_ta_n = [other_n[4] , exp_n[5], gamma_n[2] , time_a_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((E_in_inputs, ion_ta_inputs))\n",
    "# D_in = np.hstack(( E_in_D_in, ion_ta_D_in ))\n",
    "# variables = E_in_n + ion_ta_n \n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # two major rates  for UNpacked \n",
    "# # # include E in, elastic tb \n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "\n",
    "# # # vol time b elastic:  n_g df_other[4], V df_exp[5] , K_ela + E_ela  df_gamma[7] , t_b df_time_b[0], n_e df_other[3]\n",
    "# elastic_tb_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,7].reshape(a,1) , df_time_b[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# elastic_tb_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,7].reshape(6,1) , df_time_b_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# elastic_tb_n = [other_n[4] , exp_n[5], gamma_n[7] , time_b_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((E_in_inputs, elastic_tb_inputs))\n",
    "# D_in = np.hstack(( E_in_D_in, elastic_tb_D_in ))\n",
    "# variables = E_in_n + elastic_tb_n \n",
    "# print(variables)\n",
    "# print(D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # three major rates  for packed \n",
    "# # # include E in, iomization ta, tb elastic\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[4], V df_exp[5] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[0], n_e df_other[3]\n",
    "# ion_ta_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,2].reshape(a,1) , df_time_a[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# ion_ta_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,2].reshape(6,1) , df_time_a_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# ion_ta_n = [other_n[4] , exp_n[5], gamma_n[2] , time_a_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol elastic energy time b  loss: n_e [na], n_g[na], V [na], K_ela E_ela  df_gamma[7], t_b df_time_b[0]\n",
    "# ela_time_b_inputs = np.hstack((df_gamma[:,7].reshape(a,1), df_time_b[:,0].reshape(a,1) ))\n",
    "# ela_time_b_D_in = np.hstack(( gamma_units[:,7].reshape(6,1), df_time_b_units[:,0].reshape(6,1)  ))\n",
    "# ela_time_b_n = [gamma_n[7], time_b_n[0] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack(( ion_ta_inputs, ela_time_b_inputs, E_in_inputs))\n",
    "# D_in = np.hstack(( ion_ta_D_in, ela_time_b_D_in, E_in_D_in ))\n",
    "# variables = ion_ta_n + ela_time_b_n + E_in_n\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # three major rates  for UNpacked \n",
    "# # # include E in, elastic tb \n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "\n",
    "\n",
    "# # # vol time a elastic:  n_g df_other[4], V df_exp[5] , K_ela + E_ela  df_gamma[7] , t_b df_time_b[0], n_e df_other[3]\n",
    "# elastic_tb_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,7].reshape(a,1) , df_time_b[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# elastic_tb_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,7].reshape(6,1) , df_time_b_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# elastic_tb_n = [other_n[4] , exp_n[5], gamma_n[7] , time_b_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol ta excitation: n_e [na], n_g [na], V[na], K_exc_a * E_exc df_gamma[1] , t_a df_time_a[0] \n",
    "# exc_ta_inputs = np.hstack(( df_gamma[:,1].reshape(a,1), df_time_a[:,0].reshape(a,1)  )) \n",
    "# exc_ta_D_in = np.hstack(( gamma_units[:,1].reshape(6,1) , df_time_a_units[:,0].reshape(6,1)  ))\n",
    "# exc_ta_n = [ gamma_n[1] , time_a_n[0] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((E_in_inputs, elastic_tb_inputs, exc_ta_inputs))\n",
    "# D_in = np.hstack(( E_in_D_in, elastic_tb_D_in , exc_ta_D_in ))\n",
    "# variables = E_in_n + elastic_tb_n + exc_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # four major rates  for packed \n",
    "# # # include E in, iomization ta, tb elastic, ta _excitation\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[4], V df_exp[5] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[0], n_e df_other[3]\n",
    "# ion_ta_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,2].reshape(a,1) , df_time_a[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# ion_ta_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,2].reshape(6,1) , df_time_a_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# ion_ta_n = [other_n[4] , exp_n[5], gamma_n[2] , time_a_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol elastic energy time b  loss: n_e [na], n_g[na], V [na], K_ela E_ela  df_gamma[7], t_b df_time_b[0]\n",
    "# ela_time_b_inputs = np.hstack((df_gamma[:,7].reshape(a,1), df_time_b[:,0].reshape(a,1) ))\n",
    "# ela_time_b_D_in = np.hstack(( gamma_units[:,7].reshape(6,1), df_time_b_units[:,0].reshape(6,1)  ))\n",
    "# ela_time_b_n = [gamma_n[7], time_b_n[0] ]\n",
    "\n",
    "# # # ta excitation energy: ng [na], ne [na], V[na], K_exca_a * E_exc df_gamma[1] , ta [na]\n",
    "# exc_ta_inputs = df_gamma[:,1].reshape(a,1)\n",
    "# exc_ta_D_in =gamma_units[:,1].reshape(6,1)\n",
    "# exc_ta_n = [gamma_n[1]]\n",
    "\n",
    "# inputs = np.hstack(( ion_ta_inputs, ela_time_b_inputs, E_in_inputs, exc_ta_inputs))\n",
    "# D_in = np.hstack(( ion_ta_D_in, ela_time_b_D_in, E_in_D_in, exc_ta_D_in ))\n",
    "# variables = ion_ta_n + ela_time_b_n + E_in_n + exc_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # four major rates  for UNpacked \n",
    "# # # include E in, elastic tb , ta_excitation, ta_ionization\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "\n",
    "\n",
    "# # # vol time a elastic:  n_g df_other[4], V df_exp[5] , K_ela + E_ela  df_gamma[7] , t_b df_time_b[0], n_e df_other[3]\n",
    "# elastic_tb_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,7].reshape(a,1) , df_time_b[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# elastic_tb_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,7].reshape(6,1) , df_time_b_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# elastic_tb_n = [other_n[4] , exp_n[5], gamma_n[7] , time_b_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol ta excitation: n_e [na], n_g [na], V[na], K_exc_a * E_exc df_gamma[1] , t_a df_time_a[0] \n",
    "# exc_ta_inputs = np.hstack(( df_gamma[:,1].reshape(a,1), df_time_a[:,0].reshape(a,1)  )) \n",
    "# exc_ta_D_in = np.hstack(( gamma_units[:,1].reshape(6,1) , df_time_a_units[:,0].reshape(6,1)  ))\n",
    "# exc_ta_n = [ gamma_n[1] , time_a_n[0] ]\n",
    "\n",
    "# # # # vol time a ionization:  n_g df_other[na], V df_exp[na] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[na], n_e df_other[na]\n",
    "# ion_ta_inputs =  df_gamma[:,2].reshape(a,1) \n",
    "# ion_ta_D_in = gamma_units[:,2].reshape(6,1) \n",
    "# ion_ta_n = [ gamma_n[2] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((E_in_inputs, elastic_tb_inputs, exc_ta_inputs, ion_ta_inputs ))\n",
    "# D_in = np.hstack(( E_in_D_in, elastic_tb_D_in , exc_ta_D_in , ion_ta_D_in))\n",
    "# variables = E_in_n + elastic_tb_n + exc_ta_n + ion_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates  for packed \n",
    "# # # include E in, iomization ta, tb elastic, ta exc, ta elastic\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[4], V df_exp[5] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[0], n_e df_other[3]\n",
    "# ion_ta_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,2].reshape(a,1) , df_time_a[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# ion_ta_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,2].reshape(6,1) , df_time_a_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# ion_ta_n = [other_n[4] , exp_n[5], gamma_n[2] , time_a_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol elastic energy time b  loss: n_e [na], n_g[na], V [na], K_ela E_ela  df_gamma[7], t_b df_time_b[0]\n",
    "# ela_time_b_inputs = np.hstack((df_gamma[:,7].reshape(a,1), df_time_b[:,0].reshape(a,1) ))\n",
    "# ela_time_b_D_in = np.hstack(( gamma_units[:,7].reshape(6,1), df_time_b_units[:,0].reshape(6,1)  ))\n",
    "# ela_time_b_n = [gamma_n[7], time_b_n[0] ]\n",
    "\n",
    "# # # ta excitation energy: ng [na], ne [na], V[na], K_exca_a * E_exc df_gamma[1] , ta [na]\n",
    "# exc_ta_inputs = df_gamma[:,1].reshape(a,1)\n",
    "# exc_ta_D_in =gamma_units[:,1].reshape(6,1)\n",
    "# exc_ta_n = [gamma_n[1]]\n",
    "\n",
    "# # # ta elastic: ne [na], ng [na], V [na], K_ela_a * E_ela_a dfz-gamma[3], t_a[na]\n",
    "# ela_a_ta_inputs = df_gamma[:,3].reshape(a,1)\n",
    "# ela_a_D_in = gamma_units[:,3].reshape(6,1)\n",
    "# ela_a_ta_n = [gamma_n[3]]\n",
    "\n",
    "# inputs = np.hstack(( ion_ta_inputs, ela_time_b_inputs, E_in_inputs, exc_ta_inputs, ela_a_ta_inputs))\n",
    "# D_in = np.hstack(( ion_ta_D_in, ela_time_b_D_in, E_in_D_in, exc_ta_D_in, ela_a_D_in ))\n",
    "# variables = ion_ta_n + ela_time_b_n + E_in_n + exc_ta_n + ela_a_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates  for UNpacked \n",
    "# # # include E in, elastic tb , ta_excitation, ta_ionization, ta_elastic\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "\n",
    "\n",
    "# # # vol time a elastic:  n_g df_other[4], V df_exp[5] , K_ela + E_ela  df_gamma[7] , t_b df_time_b[0], n_e df_other[3]\n",
    "# elastic_tb_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,7].reshape(a,1) , df_time_b[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# elastic_tb_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,7].reshape(6,1) , df_time_b_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# elastic_tb_n = [other_n[4] , exp_n[5], gamma_n[7] , time_b_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol ta excitation: n_e [na], n_g [na], V[na], K_exc_a * E_exc df_gamma[1] , t_a df_time_a[0] \n",
    "# exc_ta_inputs = np.hstack(( df_gamma[:,1].reshape(a,1), df_time_a[:,0].reshape(a,1)  )) \n",
    "# exc_ta_D_in = np.hstack(( gamma_units[:,1].reshape(6,1) , df_time_a_units[:,0].reshape(6,1)  ))\n",
    "# exc_ta_n = [ gamma_n[1] , time_a_n[0] ]\n",
    "\n",
    "# # # # vol time a ionization:  n_g df_other[na], V df_exp[na] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[na], n_e df_other[na]\n",
    "# ion_ta_inputs =  df_gamma[:,2].reshape(a,1) \n",
    "# ion_ta_D_in = gamma_units[:,2].reshape(6,1) \n",
    "# ion_ta_n = [ gamma_n[2] ]\n",
    "\n",
    "# # # # ta elastic: ne [na], ng [na], V [na], K_ela_a * E_ela_a dfz-gamma[3], t_a[na]\n",
    "# ela_a_ta_inputs = df_gamma[:,3].reshape(a,1)\n",
    "# ela_a_D_in = gamma_units[:,3].reshape(6,1)\n",
    "# ela_a_ta_n = [gamma_n[3]]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((E_in_inputs, elastic_tb_inputs, exc_ta_inputs, ion_ta_inputs , ela_a_ta_inputs ))\n",
    "# D_in = np.hstack(( E_in_D_in, elastic_tb_D_in , exc_ta_D_in , ion_ta_D_in , ela_a_D_in ))\n",
    "# variables = E_in_n + elastic_tb_n + exc_ta_n + ion_ta_n + ela_a_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # six major rates  for packed; need to add last rate\n",
    "# # # include E in, iomization ta, tb elastic, ta exc, ta elastic, tb floating energy loss\n",
    "\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# # E in\n",
    "# E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "# E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "# E_in_n = [other_n[0]]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[4], V df_exp[5] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[0], n_e df_other[3]\n",
    "# ion_ta_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,2].reshape(a,1) , df_time_a[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "# ion_ta_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,2].reshape(6,1) , df_time_a_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "# ion_ta_n = [other_n[4] , exp_n[5], gamma_n[2] , time_a_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # # vol elastic energy time b  loss: n_e [na], n_g[na], V [na], K_ela E_ela  df_gamma[7], t_b df_time_b[0]\n",
    "# ela_time_b_inputs = np.hstack((df_gamma[:,7].reshape(a,1), df_time_b[:,0].reshape(a,1) ))\n",
    "# ela_time_b_D_in = np.hstack(( gamma_units[:,7].reshape(6,1), df_time_b_units[:,0].reshape(6,1)  ))\n",
    "# ela_time_b_n = [gamma_n[7], time_b_n[0] ]\n",
    "\n",
    "\n",
    "# # # ta excitation energy: ng [na], ne [na], V[na], K_exca_a * E_exc df_gamma[1] , ta [na]\n",
    "# exc_ta_inputs = df_gamma[:,1].reshape(a,1)\n",
    "# exc_ta_D_in =gamma_units[:,1].reshape(6,1)\n",
    "# exc_ta_n = [gamma_n[1]]\n",
    "\n",
    "\n",
    "# # # ta elastic: ne [na], ng [na], V [na], K_ela_a * E_ela_a dfz-gamma[3], t_a[na]\n",
    "# ela_a_ta_inputs = df_gamma[:,3].reshape(a,1)\n",
    "# ela_a_D_in = gamma_units[:,3].reshape(6,1)\n",
    "# ela_a_ta_n = [gamma_n[3]]\n",
    "\n",
    "\n",
    "# # # tb floating loss, energy: ne [na], u_B, e_c, T_e [na], A_tot, t_b [na]\n",
    "# floating_loss_tb_inputs = np.hstack(( df_time_b[:,5].reshape(a,1), df_other[:,9].reshape(a,1) , df_exp[:,7].reshape(a,1) ))\n",
    "# floating_loss_tb_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_other_units[:,9].reshape(6,1) , df_exp_units[:,7].reshape(6,1) ))\n",
    "# floating_loss_tb_n = [ time_b_n[5], other_n[9] , exp_n[7]]\n",
    "                               \n",
    "                               \n",
    "# inputs = np.hstack(( ion_ta_inputs, ela_time_b_inputs, E_in_inputs, exc_ta_inputs, ela_a_ta_inputs, floating_loss_tb_inputs))\n",
    "# D_in = np.hstack(( ion_ta_D_in, ela_time_b_D_in, E_in_D_in, exc_ta_D_in, ela_a_D_in , floating_loss_tb_D_in ))\n",
    "# variables = ion_ta_n + ela_time_b_n + E_in_n + exc_ta_n + ela_a_ta_n + floating_loss_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E_period_kgm2_s2', 'n_g_atoms_m3', 'Volume_m3', 'y_ela', 't_b_seconds', 'n_e_electrons_m3', 'y_exc_a', 't_a_seconds', 'y_iz_a', 'y_ela_a', 'n_sa_atoms_m3', 'u_B_m_s', 'A_a_m2', 'e_c_As', 'V_p_tb_kgm2_s3_A']\n",
      "[[ 2 -3  3  5  0 -3  5  0  5  5 -3  1  2  0  2]\n",
      " [-2  0  0 -3  1  0 -3  1 -3 -3  0 -1  0  1 -3]\n",
      " [ 0  1  0 -1  0  1 -1  0 -1 -1  1  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  1  0  1  1  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# # six major rates  for UNpacked \n",
    "# # include E in, elastic tb , ta_excitation, ta_ionization, ta_elastic, tb_ion_energy_loss_electrode_a\n",
    "\n",
    "a = df_time_b[:,5].shape[0]\n",
    "\n",
    "\n",
    "# E in\n",
    "E_in_inputs = df_other[:,0].reshape(a,1)\n",
    "E_in_D_in = df_other_units[:,0].reshape(6,1)\n",
    "E_in_n = [other_n[0]]\n",
    "\n",
    "\n",
    "\n",
    "# # vol time b elastic:  n_g df_other[4], V df_exp[5] , K_ela + E_ela  df_gamma[7] , t_b df_time_b[0], n_e df_other[3]\n",
    "elastic_tb_inputs = np.hstack(( df_other[:,4].reshape(a,1) , df_exp[:,5].reshape(a,1) , df_gamma[:,7].reshape(a,1) , df_time_b[:,0].reshape(a,1) ,  df_other[:,3].reshape(a,1) ))\n",
    "elastic_tb_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) , gamma_units[:,7].reshape(6,1) , df_time_b_units[:,0].reshape(6,1) , df_other_units[:,3].reshape(6,1) ))\n",
    "elastic_tb_n = [other_n[4] , exp_n[5], gamma_n[7] , time_b_n[0], other_n[3] ]\n",
    "\n",
    "\n",
    "# # vol ta excitation: n_e [na], n_g [na], V[na], K_exc_a * E_exc df_gamma[1] , t_a df_time_a[0] \n",
    "exc_ta_inputs = np.hstack(( df_gamma[:,1].reshape(a,1), df_time_a[:,0].reshape(a,1)  )) \n",
    "exc_ta_D_in = np.hstack(( gamma_units[:,1].reshape(6,1) , df_time_a_units[:,0].reshape(6,1)  ))\n",
    "exc_ta_n = [ gamma_n[1] , time_a_n[0] ]\n",
    "\n",
    "# # # vol time a ionization:  n_g df_other[na], V df_exp[na] , K_iz_a + E_iz  df_gamma[2] , t_a df_time_a[na], n_e df_other[na]\n",
    "ion_ta_inputs =  df_gamma[:,2].reshape(a,1) \n",
    "ion_ta_D_in = gamma_units[:,2].reshape(6,1) \n",
    "ion_ta_n = [ gamma_n[2] ]\n",
    "\n",
    "# # # ta elastic: ne [na], ng [na], V [na], K_ela_a * E_ela_a dfz-gamma[3], t_a[na]\n",
    "ela_a_ta_inputs = df_gamma[:,3].reshape(a,1)\n",
    "ela_a_D_in = gamma_units[:,3].reshape(6,1)\n",
    "ela_a_ta_n = [gamma_n[3]]\n",
    "\n",
    "\n",
    "# # tb ion energy loss electrode a: n_sa df_other[1] , u_B df_time_b [5] , A_a df_exp[2] , t_b [na] , e_c df_other[9], V_p_tb df_time_b[2]\n",
    "ion_electrode_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_time_b[:,5].reshape(a,1) , df_exp[:,2].reshape(a,1) , df_other[:,9].reshape(a,1) , df_time_b[:,2].reshape(a,1) )) \n",
    "ion_electrode_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_time_b_units[:,5].reshape(6,1) , df_exp_units[:,2].reshape(6,1) , df_other_units[:,9].reshape(6,1) , df_time_b_units[:,2].reshape(6,1) )) \n",
    "ion_electrode_a_tb_n = [ other_n[1] , time_b_n[5], exp_n[2] , other_n[9] , time_b_n[2] ]\n",
    "\n",
    "\n",
    "inputs = np.hstack((E_in_inputs, elastic_tb_inputs, exc_ta_inputs, ion_ta_inputs , ela_a_ta_inputs , ion_electrode_a_tb_inputs ))\n",
    "D_in = np.hstack(( E_in_D_in, elastic_tb_D_in , exc_ta_D_in , ion_ta_D_in , ela_a_D_in , ion_electrode_a_tb_D_in))\n",
    "variables = E_in_n + elastic_tb_n + exc_ta_n + ion_ta_n + ela_a_ta_n + ion_electrode_a_tb_n\n",
    "print(variables)\n",
    "print(D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 15)\n",
      "(6, 15)\n",
      "[0.09334615 2.9058364  0.26987067 0.24316137 0.999886   1.51943551\n",
      " 0.33799193 0.114      2.61388493 0.42516901 0.80947362 1.75139783\n",
      " 0.25639062 0.16022    6.75884838]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(D_in.shape)\n",
    "print(inputs[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we must produce our target variables, include their dimensions ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.707010Z",
     "start_time": "2023-05-31T02:42:27.700438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12719865 0.11651852 0.12623569 0.12430976 0.12133333 0.12457239\n",
      " 0.12465993 0.11993266 0.12054545 0.1200202  0.12107071 0.12702357\n",
      " 0.11511785 0.12142088 0.12150842 0.12352189 0.12142088 0.12045791\n",
      " 0.12045791 0.12107071 0.11958249 0.12185859 0.12098316 0.12159596\n",
      " 0.11967003 0.12212121 0.11914478 0.11870707 0.11800673 0.11809428\n",
      " 0.12054545 0.11958249 0.12054545 0.12054545 0.12150842 0.12308418\n",
      " 0.12220875 0.1222963  0.12098316 0.12054545 0.12185859 0.12290909\n",
      " 0.12194613 0.12212121 0.1256229  0.12430976 0.12693603 0.12781145\n",
      " 0.14925926 0.156      0.15389899 0.14645791 0.12973737 0.12124579\n",
      " 0.12194613 0.11975758 0.139367   0.13367677 0.21456566 0.22822222\n",
      " 0.21701684 0.17184512 0.12404714 0.2649899  0.1399798  0.13534007\n",
      " 0.13358923 0.12159596 0.12142088 0.12107071 0.12255892 0.11931987\n",
      " 0.12019529 0.11861953 0.12072054 0.11931987 0.11940741 0.120633\n",
      " 0.12010774 0.13954209 0.12824916 0.12737374 0.12518519 0.11660606\n",
      " 0.11686869 0.11721886 0.12430976 0.11993266 0.1166936  0.11625589\n",
      " 0.12448485 0.11555556 0.1239596  0.12317172 0.12247138 0.12807407\n",
      " 0.11012795 0.11818182 0.12623569 0.12142088 0.12159596 0.12107071\n",
      " 0.12107071 0.11993266 0.12045791 0.12238384 0.12238384 0.13148822\n",
      " 0.13428956 0.1377037  0.13892929 0.13323906 0.13306397 0.1256229\n",
      " 0.12264646 0.12142088 0.13569024 0.13980471 0.16694276 0.18839057\n",
      " 0.15468687 0.1272862  0.1643165  0.21675421 0.14243098 0.13656566\n",
      " 0.12877441 0.1272862  0.1233468  0.12544781 0.11879461 0.12212121\n",
      " 0.12404714 0.12404714 0.12247138 0.1222963  0.12378451 0.12282155\n",
      " 0.12264646 0.12255892 0.12737374 0.1272862  0.1316633  0.1256229\n",
      " 0.12509764 0.12571044 0.12719865 0.12492256 0.12290909 0.12492256\n",
      " 0.12649832 0.12772391 0.14041751 0.14111785 0.14444444 0.12527273\n",
      " 0.12273401 0.12807407 0.12903704 0.14540741 0.18716498 0.12299663\n",
      " 0.12912458 0.12325926 0.13096296 0.12212121 0.12387205 0.13709091\n",
      " 0.12859933 0.12238384 0.13630303 0.12430976 0.1222963  0.12212121\n",
      " 0.11721886 0.12045791 0.11879461 0.12080808 0.1216835  0.12378451\n",
      " 0.12938721 0.11870707 0.11879461 0.12851178 0.12457239 0.12107071\n",
      " 0.14155556 0.1250101  0.12719865 0.13463973 0.12247138 0.12089562\n",
      " 0.12150842 0.12072054 0.12702357 0.1200202  0.11905724 0.12010774\n",
      " 0.12133333 0.11905724 0.11844444 0.12588552 0.12212121 0.12929966\n",
      " 0.12465993 0.12098316 0.12133333 0.12509764 0.12483502 0.12465993\n",
      " 0.12255892 0.12115825 0.13052525 0.13954209 0.13709091 0.17061953\n",
      " 0.1493468  0.14908418 0.18900337 0.19285522 0.16983165 0.1383165\n",
      " 0.12212121 0.13857912 0.16344108 0.17683502 0.32381818 0.21762963\n",
      " 0.15232323 0.1316633  0.12369697 0.12369697 0.15652525 0.13534007\n",
      " 0.13490236]\n",
      "[ 4.84333333  4.43666667  4.80666667  4.73333333  4.62        4.74333333\n",
      "  4.74666667  4.56666667  4.59        4.57        4.61        4.83666667\n",
      "  4.38333333  4.62333333  4.62666667  4.70333333  4.62333333  4.58666667\n",
      "  4.58666667  4.61        4.55333333  4.64        4.60666667  4.63\n",
      "  4.55666667  4.65        4.53666667  4.52        4.49333333  4.49666667\n",
      "  4.59        4.55333333  4.59        4.59        4.62666667  4.68666667\n",
      "  4.65333333  4.65666667  4.60666667  4.59        4.64        4.68\n",
      "  4.64333333  4.65        4.78333333  4.73333333  4.83333333  4.86666667\n",
      "  5.68333333  5.94        5.86        5.57666667  4.94        4.61666667\n",
      "  4.64333333  4.56        5.30666667  5.09        8.17        8.69\n",
      "  8.26333333  6.54333333  4.72333333 10.09        5.33        5.15333333\n",
      "  5.08666667  4.63        4.62333333  4.61        4.66666667  4.54333333\n",
      "  4.57666667  4.51666667  4.59666667  4.54333333  4.54666667  4.59333333\n",
      "  4.57333333  5.31333333  4.88333333  4.85        4.76666667  4.44\n",
      "  4.45        4.46333333  4.73333333  4.56666667  4.44333333  4.42666667\n",
      "  4.74        4.4         4.72        4.69        4.66333333  4.87666667\n",
      "  4.19333333  4.5         4.80666667  4.62333333  4.63        4.61\n",
      "  4.61        4.56666667  4.58666667  4.66        4.66        5.00666667\n",
      "  5.11333333  5.24333333  5.29        5.07333333  5.06666667  4.78333333\n",
      "  4.67        4.62333333  5.16666667  5.32333333  6.35666667  7.17333333\n",
      "  5.89        4.84666667  6.25666667  8.25333333  5.42333333  5.2\n",
      "  4.90333333  4.84666667  4.69666667  4.77666667  4.52333333  4.65\n",
      "  4.72333333  4.72333333  4.66333333  4.65666667  4.71333333  4.67666667\n",
      "  4.67        4.66666667  4.85        4.84666667  5.01333333  4.78333333\n",
      "  4.76333333  4.78666667  4.84333333  4.75666667  4.68        4.75666667\n",
      "  4.81666667  4.86333333  5.34666667  5.37333333  5.5         4.77\n",
      "  4.67333333  4.87666667  4.91333333  5.53666667  7.12666667  4.68333333\n",
      "  4.91666667  4.69333333  4.98666667  4.65        4.71666667  5.22\n",
      "  4.89666667  4.66        5.19        4.73333333  4.65666667  4.65\n",
      "  4.46333333  4.58666667  4.52333333  4.6         4.63333333  4.71333333\n",
      "  4.92666667  4.52        4.52333333  4.89333333  4.74333333  4.61\n",
      "  5.39        4.76        4.84333333  5.12666667  4.66333333  4.60333333\n",
      "  4.62666667  4.59666667  4.83666667  4.57        4.53333333  4.57333333\n",
      "  4.62        4.53333333  4.51        4.79333333  4.65        4.92333333\n",
      "  4.74666667  4.60666667  4.62        4.76333333  4.75333333  4.74666667\n",
      "  4.66666667  4.61333333  4.97        5.31333333  5.22        6.49666667\n",
      "  5.68666667  5.67666667  7.19666667  7.34333333  6.46666667  5.26666667\n",
      "  4.65        5.27666667  6.22333333  6.73333333 12.33        8.28666667\n",
      "  5.8         5.01333333  4.71        4.71        5.96        5.15333333\n",
      "  5.13666667]\n"
     ]
    }
   ],
   "source": [
    "#For predicting Te/Tg: keep top block, comment bottom block. For predicting ne/ng, keep bottom block, comment top block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TOP BLOCK ####\n",
    "\n",
    "df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "T_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "T_e_no_dim = T_e/T_g\n",
    "print(T_e)\n",
    "output = rescale_vec(T_e_no_dim)\n",
    "print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "        [0.]\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#### BOTTOM BLOCK ####\n",
    "\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# n_e = np.array(df_out.iloc[:,3])\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# n_g = np.array(df_out.iloc[:,4])\n",
    "# n_e_no_dim = n_e/n_g\n",
    "# print(n_e)\n",
    "# print(n_e_no_dim)\n",
    "# output = rescale_vec(n_e_no_dim)\n",
    "# print(output)\n",
    "# D_out = np.array(\n",
    "#     [\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0,],\n",
    "#         [0.]\n",
    "#     ],\n",
    "\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.329999999999998\n",
      "4.1933333333333325\n"
     ]
    }
   ],
   "source": [
    "r = T_e/T_g\n",
    "print(max(r))\n",
    "print(min(r))\n",
    "del r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we produce the nullspace basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.754325Z",
     "start_time": "2023-05-31T02:42:27.738571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000, -1.0000],\n",
       "        [ 1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  0.0000, -1.0000, -1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3333,\n",
       "         -0.6667,  0.0000],\n",
       "        [ 1.0000,  0.0000, -1.0000,  1.0000, -1.0000, -1.0000,  0.0000, -1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.     0.     0.    -1.     0.     0.     0.     1.     0.    -1.   ]\n",
      " [ 1.    -1.     0.     1.     0.     0.    -1.    -1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.    -0.333 -0.667  0.   ]\n",
      " [ 1.     0.    -1.     1.    -1.    -1.     0.    -1.     0.     0.   ]\n",
      " [ 1.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     1.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "(15, 10)\n"
     ]
    }
   ],
   "source": [
    "fff.basis_col\n",
    "with np.printoptions(precision = 3):\n",
    "    print(fff.basis_col.numpy())\n",
    "    print(fff.basis_col.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.843064Z",
     "start_time": "2023-05-31T02:42:27.750866Z"
    }
   },
   "outputs": [],
   "source": [
    "fff.read_data(inputs, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we prepare for dimensionless number discovery. \n",
    "# In this next cell, we choose how many different initial guess to start the optimization with, as well as where to store the results.\n",
    "# summary path: where the overview of each result\n",
    "good path: where dimensionless numbers with a r^2 > 0.3 are stored.\n",
    "bad path: where dimensionless numbers with a r^2 < 0.3 are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.883791Z",
     "start_time": "2023-05-31T02:42:27.758677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in globals\n",
      "not in globals\n"
     ]
    }
   ],
   "source": [
    "from PyDBDdim import PiLinearRegressionViaTorch\n",
    "import pandas as pd\n",
    "\n",
    "#the code does five fold to ensure the R2 is worth while.\n",
    "# we then want to rerun the fitting on all data instead of 80% of it. The 5fold find the right number of epochs.\n",
    "# so, we re run to find the right number of epochs, and then we must store the parameters and the R2 from the 5-fold fitting.\n",
    "#\n",
    "small_loop = [i for i in range(1,3)]\n",
    "j =[i for i in range(1,6)]\n",
    "h = [5*i for i in range(6,26)] # rerun for unpacked, 6/7 rates\n",
    "#h = [2*i for i in range(26,150)] #when desperate\n",
    "g = j + h\n",
    "\n",
    "if 'df_loop_reg' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_reg\n",
    "    if 'df_loop_reg' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "    \n",
    "if 'df_loop_ext' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_ext\n",
    "    if 'df_loop_ext' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "\n",
    "    \n",
    "# these variables will be declared below!\n",
    "\n",
    "lambda_gamma = 0.01\n",
    "\n",
    "summary_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_eb_val_test\\unpacked\\5_terms\" + '\\\\' + r\"summary_recheck.xlsx\"\n",
    "good_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_eb_val_test\\unpacked\\5_terms\\good_terms\"\n",
    "bad_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_eb_val_test\\unpacked\\5_terms\\bad_terms\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discovery occurs in the next cell.\n",
    "Parameters:\n",
    "lambda_gamma (prev cell): penalty on the size of the dimensionless number\n",
    "lambda_beta: penalty on the number of basis functions used in the polynomial regression\n",
    "\n",
    "n_dimensionless: # of dimensionless numbers to discover.\n",
    "poly_order: polynomial order. Linear is order 1.\n",
    "poly_mapping: for the dimensionless number, what power each one has in order. See below.\n",
    "\n",
    "para_threshold: threshold for gamma. Below this threshold, the gamma is treated as if it were zero.\n",
    "beta_threshold: threshold for beta. Belwo this threshold, the beta is treated as if it were zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mg\u001b[49m:\n\u001b[0;32m      2\u001b[0m     seed \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#find which hyperparameter performed best\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# idx = np.arange(lg_set.shape[0])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#     print(best_hp.shape)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#     best_hp = best_hp[0,1]\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in g:\n",
    "    seed = i\n",
    "    #find which hyperparameter performed best\n",
    "\n",
    "    # idx = np.arange(lg_set.shape[0])\n",
    "    # best_hp = idx[array_metric_num[:,0]==np.max(array_metric_num)]\n",
    "\n",
    "    # if len(best_hp.shape)>1:\n",
    "    #     print('the following are equivalent ',best_hp)\n",
    "    #     print(best_hp.shape)\n",
    "    #     best_hp = best_hp[0,1]\n",
    "\n",
    "\n",
    "\n",
    "    ndimensionless = 1\n",
    "    #lambda_gamma = 0.003 #lambda_gamma = 0.01, #replaced\n",
    "\n",
    "    poly_order = 1 #was 2? but linear is a first order poly.\n",
    "\n",
    "    poly_mapping = np.array([[0],\n",
    "                            [1]])\n",
    "    \n",
    "    # for 2nd order with 2 dim #'s'\n",
    "    #poly_mapping = np.array([[0, 0],\n",
    "    #                         [1, 0],\n",
    "    #                         [0, 1],\n",
    "    #                         [2, 0],\n",
    "    #                         [1, 1],\n",
    "    #                         [0, 2]])\n",
    "    lambda_beta = 0.01 #maybe cut in half idk\n",
    "    w_array = np.array(fff._basis_col)  #this is the w array, that is, the columns in Null(D)\n",
    "    gamma_name = ['y'+str(id) for id in range(0,fff.basis_col.shape[1]) ]\n",
    "    beta_name = ['b'+str(id) for id in range(0,poly_mapping.shape[0]) ]\n",
    "    poly_name = ['dim'+str(id+1) for id in range(0,ndimensionless)]\n",
    "\n",
    "\n",
    "\n",
    "    metric = 'r2'\n",
    "    para_threshold = 0.01 #vs 0.1?\n",
    "    beta_threshold = 0.005\n",
    "    training_epochs =10000\n",
    "    score = []\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "    \n",
    "    #This first part of the cell, we fit without testing the performance on extrapolation. That comes later.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #create test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.15, random_state=42)\n",
    "\n",
    "    # create validation and training set  ##unused!\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    r2_reg = r2\n",
    "    print(r2)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### COMMENTED CODE BELOW MUST BE CORRECTED#####\n",
    "#     if ndimensionless >=2:\n",
    "        \n",
    "#         best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "#         df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "    \n",
    "    \n",
    "    # store the data in data_frames\n",
    "    df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    \n",
    "\n",
    "    \n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "    \n",
    "    \n",
    "\n",
    "    # delete old variables\n",
    "\n",
    "    variable_names = ['X_test', 'Y_test', 'X_train', 'X_val', 'y_train', 'y_val', 'X_train_val', 'Y_train_val']\n",
    "    for var_name in variable_names:\n",
    "        if var_name in locals() or var_name in globals():\n",
    "            target_dict = locals() if var_name in locals() else globals()\n",
    "            del target_dict[var_name]\n",
    "            #print(f\"Deleted variable: {var_name}\")\n",
    "            \n",
    "    \n",
    "    # Refit, but we test extrapolation.\n",
    "    \n",
    "    X_test, Y_test, X_train_val, y_train_val = top_split_y(fff.X,fff.y,5)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "\n",
    "    \n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "\n",
    "    x_test = X_test\n",
    "    y_test = Y_test\n",
    "    print(y_test)\n",
    "    print(y_train_val)\n",
    "    training_epochs =10000\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train, y_train, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_val, val_y =y_val , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_val,y_val,metric)\n",
    "    r2_ext = r2\n",
    "    print(r2)\n",
    "    \n",
    "    #check its performance on the test set\n",
    "    r2_ext_test = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    print(r2_ext_test)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack(( r2_ext_test.reshape(1,1), best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name = ['r2_ext_test'] + [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1_ext = pd.DataFrame(data=best_data_dim1, columns =['r2_ext_test'] + [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### COMMENTED CODE BELOW MUST BE CORRECTED#####\n",
    "#     if ndimensionless >=2:\n",
    "        \n",
    "#         best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "#         df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "\n",
    "    ori_pis[0]\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # put save_path decision here\n",
    "        #set save path--remove this chunk of code to the bottom?\n",
    "\n",
    "    #current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\"\n",
    "    \n",
    "    #here, we store the data in excel.\n",
    "    \n",
    "    if r2_reg >0.3 or r2_ext >0.3 or r2_ext_test:\n",
    "        current_path = good_path\n",
    "    else:\n",
    "        current_path = bad_path\n",
    "    #new_folder =r'mb_1dim_v1_pt0001_pt_01_order1_golden_child'\n",
    "    new_folder = r'clamp'+str(para_threshold)+r'_' + str(ndimensionless)+r'dim_order'+str(poly_order)+r'_lambda'+str(lambda_gamma)+r\"_seed\"+str(seed)\n",
    "    new_path = current_path+'\\\\'+new_folder\n",
    "    if os.path.exists(new_path):\n",
    "        print(f\"File '{new_path}' already exists.\")\n",
    "    else:\n",
    "        os.mkdir(new_path)       \n",
    "        print(f\"File '{new_path}' did not exist.\")\n",
    "\n",
    "    file_path = new_path+'\\\\' +'_seed'+str(seed) + r\"_\"+packed_or_not + '.xlsx' #+new_folder was right before +'_seed'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "    # File path of the Excel file\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4.to_excel(writer, sheet_name='beta')\n",
    "        df5.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary and performance of this 'reg dim #'\n",
    "    if 'df_loop_reg' in globals():\n",
    "        df_loop_reg = pd.concat([df_loop_reg, df_best_dim1 ], ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        df_loop_reg = df_best_dim1.copy(deep=True)\n",
    "    \n",
    "            \n",
    "    # Now store the data from the extrapolation attempt. \n",
    "    # select file path to save xlsx sheet\n",
    "    # and overwrite the dataframes (df's) \n",
    "    # then store these new data frames in a separate excel file\n",
    "    \n",
    "    \n",
    "    # File path of the Excel file\n",
    "    file_path = new_path+'\\\\' + '_extrapolation' +'_seed'+str(seed) + r\"_\"+packed_or_not + '.xlsx' #+new_folder was right before +'_seed'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "    df0_ext = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1_ext = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4_ext =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5_ext = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0_ext.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1_ext.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4_ext.to_excel(writer, sheet_name='beta')\n",
    "        df5_ext.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1_ext.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary of the 'ext dim #' for placement in an excel so I can compare them all!\n",
    "    if 'df_loop_ext' in globals():\n",
    "        df_loop_ext = pd.concat([df_loop_ext, df_best_dim1_ext ], ignore_index = True)\n",
    "        \n",
    "    else:\n",
    "        df_loop_ext = df_best_dim1_ext.copy(deep=True)\n",
    "    \n",
    "        \n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "#write df loops to good path!\n",
    "with pd.ExcelWriter(summary_path) as writer:  \n",
    "    df_loop_reg.to_excel(writer, sheet_name='reg dimension number')\n",
    "    df_loop_ext.to_excel(writer, sheet_name='ext dimension number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 150  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beta_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to check the D matrix used in the paper (6 rates, packed) and the one used above are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3  3  5  0 -3  5  0  2  5  5  1  0  2]\n",
      " [ 0  0 -3  1  0 -3  1 -2 -3 -3 -1  1  0]\n",
      " [ 1  0 -1  0  1 -1  0  0 -1 -1  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  1  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0]]\n",
      "[[ 1  3  2  2  0  0  0 -3 -3  5  5  5  5]\n",
      " [-1  0  0 -2  1  1  1  0  0 -3 -3 -3 -3]\n",
      " [ 0  0  0  0  0  0  0  1  1 -1 -1 -1 -1]\n",
      " [ 0  0  0  1  0  0  0  0  0  1  1  1  1]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "D_in = np.array(\n",
    "[\n",
    " [-3,  3,  5,  0, -3,  5,  0,  2,  5,  5,  1,  0,  2],\n",
    " [ 0,  0, -3,  1,  0, -3,  1, -2, -3, -3, -1,  1,  0],\n",
    " [ 1,  0, -1,  0,  1, -1,  0,  0, -1, -1,  0,  0,  0],\n",
    " [ 0,  0,  1,  0,  0,  1,  0,  1,  1,  1,  0,  0,  0],\n",
    " [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0]\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# D_in: find W for paper\n",
    "D_in_paper = np.array(\n",
    "[\n",
    "    [ 1,  3,  2,  2,  0,  0,  0, -3, -3,  5,  5,  5,  5],\n",
    "    [-1,  0,  0, -2,  1,  1,  1,  0,  0, -3, -3, -3, -3],\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  1,  1, -1, -1, -1, -1],\n",
    "    [ 0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1],\n",
    "    [ 0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "]\n",
    ")\n",
    "\n",
    "print(D_in)\n",
    "print(D_in_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.     0.     0.    -1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.333 -0.667]\n",
      " [ 0.    -1.     0.    -1.    -1.    -1.     0.     0.   ]\n",
      " [ 0.     0.    -1.    -1.     0.     0.     1.     0.   ]\n",
      " [ 1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "(13, 8)\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "T_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "T_e_no_dim = T_e/T_g\n",
    "#print(T_e)\n",
    "output = rescale_vec(T_e_no_dim)\n",
    "#print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "with np.printoptions(precision = 3):\n",
    "    print(fff.basis_col.numpy())\n",
    "    print(fff.basis_col.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -2. -2. -8. -5.  3.  6.  3.]\n",
      " [ 2.  1.  3.  3.  0. -3. -5. -3.]\n",
      " [ 0.  0.  0.  1.  1. -1. -1. -1.]\n",
      " [ 0.  0. -1. -1.  0.  1.  2.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "mat = fff.basis_col.numpy()\n",
    "print(np.matmul(D_in_paper,mat))\n",
    "print(np.matmul(D_in,mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.329999999999998\n",
      "4.1933333333333325\n"
     ]
    }
   ],
   "source": [
    "r = T_e/T_g\n",
    "print(max(r))\n",
    "print(min(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def have_same_columns(a, b):\n",
    "    # Set of column-tuples from each array\n",
    "    set_a = {tuple(col) for col in a.T}\n",
    "    set_b = {tuple(col) for col in b.T}\n",
    "    return set_a == set_b\n",
    "result = have_same_columns(D_in, D_in_paper)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "array1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "array2 = np.array([[3, 1, 2], [6, 4, 5]])\n",
    "\n",
    "# Function to check if two arrays have the same columns regardless of order\n",
    "def have_same_columns(a, b):\n",
    "    # Set of column-tuples from each array\n",
    "    set_a = {tuple(col) for col in a.T}\n",
    "    set_b = {tuple(col) for col in b.T}\n",
    "    return set_a == set_b\n",
    "\n",
    "# Check if the arrays have the same columns\n",
    "result = have_same_columns(array1, array2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = np.delete(D_in, 0, axis = 1)\n",
    "D_in_paper = np.delete(D_in_paper, 4, axis = 1)\n",
    "print(D_in)\n",
    "print(D_in_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Permutation_matrix = np.eye(13)[:,[10, 1, 12, 7, 11, 3, 6, 0, 4, 2, 5, 9, 8]]\n",
    "print(Permutation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  2  2  0  0  0 -3 -3  5  5  5  5]\n",
      " [-1  0  0 -2  1  1  1  0  0 -3 -3 -3 -3]\n",
      " [ 0  0  0  0  0  0  0  1  1 -1 -1 -1 -1]\n",
      " [ 0  0  0  1  0  0  0  0  0  1  1  1  1]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(D_in_paper)\n",
    "D_in_paper_check = np.matmul(D_in, Permutation_matrix)\n",
    "print(D_in_paper - D_in_paper_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_one = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
