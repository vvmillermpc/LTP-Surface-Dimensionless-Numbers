{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.163733Z",
     "start_time": "2023-05-31T02:42:22.636891Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim.utils import DimensionlessLearning\n",
    "from vics_fcns import top_split_y#, top_split_x\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.572932Z",
     "start_time": "2023-05-31T02:42:27.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.ExcelFile(r\"mass_balance_params.xlsx\")\n",
    "#df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\data\\data_no_packing_all\\data_from_EB_looping_new_excel_new_code_no_packing_v3.xlsx')\n",
    "df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\collected_output_files_packing\\data_from_EB_looping_all_sizes_remove_extraneous_dims.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.660878Z",
     "start_time": "2023-05-31T02:42:27.634288Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_x['u_B_m_s'] = df_train_x['u_B_m_s'] / 1000 if np.min(df_train_x['u_B_m_s']) > 0 else df_train_x['u_B_m_s'] / 1000 - (np.min(df_train_x['u_B_m_s']) / 1000-1e-5)\n",
    "# df_train_x['A_tot_m2'] = 1.\n",
    "# df_train_x['t_a_s'] = df_train_x['t_a_s'] / 1e-7 if np.min(df_train_x['t_a_s']) > 0 else df_train_x['t_a_s'] / 1e-7 - (np.min(df_train_x['t_a_s']) / 1e-7-1e-5)\n",
    "# df_train_x['t_b_s'] = df_train_x['t_b_s'] / 1e-4 if np.min(df_train_x['t_b_s']) > 0 else df_train_x['t_b_s'] / 1e-4 - (np.min(df_train_x['t_b_s']) / 1e-4-1e-5)\n",
    "# df_train_x['Volume_m3'] = 1.\n",
    "# df_train_x['K_iz_a_m3_s_atom'] = df_train_x['K_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_iz_a_m3_s_atom']) > 0 else df_train_x['K_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_2_iz_a_m3_s_atom'] = df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_2_iz_a_m3_s_atom']) > 0 else df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_2_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_iz_exc_a_m3_s_atom'] = df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 if np.min(df_train_x['K_iz_exc_a_m3_s_atom']) > 0 else df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 - (np.min(df_train_x['K_iz_exc_a_m3_s_atom']) / 1e-14-1e-5)\n",
    "\n",
    "# df_train_x['n_sa_atoms_m3'] = df_train_x['n_sa_atoms_m3'] / 1e10 if np.min(df_train_x['n_sa_atoms_m3']) > 0 else df_train_x['n_sa_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sa_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['n_sb_atoms_m3'] = df_train_x['n_sb_atoms_m3'] / 1e10 if np.min(df_train_x['n_sb_atoms_m3']) > 0 else df_train_x['n_sb_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sb_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['A_a_m2'] = 1.\n",
    "\n",
    "# df_train_x['A_b_m2'] = 1.\n",
    "\n",
    "# df_train_x['n_He_exc_a_atoms_m3'] = df_train_x['n_He_exc_a_atoms_m3'] / 1e17 if np.min(df_train_x['n_He_exc_a_atoms_m3']) > 0 else df_train_x['n_He_exc_a_atoms_m3'] / 1e17 - (np.min(df_train_x['n_He_exc_a_atoms_m3']) / 1e17-1e-5)\n",
    "\n",
    "def rescale(g):\n",
    "    for i in range(0,g.shape[1]):\n",
    "        if np.min(np.abs(g[:,i])) ==0:\n",
    "            n = 0\n",
    "        else:\n",
    "            n = np.mean((np.log10(np.min(np.abs(g[:,i]))), np.log10(np.max(np.abs(g[:,i])))))\n",
    "        if n<0:\n",
    "                g[:,i] = g[:,i]/10**np.ceil(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.ceil(n) - (np.min(g[:,i]) / 10**np.ceil(n)-1e-5)\n",
    "        else:\n",
    "            g[:,i] = g[:,i]/10**np.floor(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.floor(n) - (np.min(g[:,i]) / 10**np.floor(n)-1e-5)\n",
    "    return g\n",
    "\n",
    "def rescale_vec(g):\n",
    "    if np.min(np.abs(g)) ==0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = np.mean((np.log10(np.min(np.abs(g))), np.log10(np.max(np.abs(g)))))\n",
    "    if n<0:\n",
    "            g= g/10**np.ceil(n) if np.min(g) > 0 else g/10**np.ceil(n) - (np.min(g) / 10**np.ceil(n)-1e-5)\n",
    "    else:\n",
    "        g = g/10**np.floor(n) if np.min(g) > 0 else g/10**np.floor(n) - (np.min(g) / 10**np.floor(n)-1e-5)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.706539Z",
     "start_time": "2023-05-31T02:42:27.668685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#                   0             1           2                    3                   4                   5           6                7                    8                     9                        10                  11                  12                      13                          \n",
    "#df1_names = [ 't_a_seconds', 'Q_a_As', 'V_p_ta_kgm2_s3_A','T_e_a_kgm2_s3_A', 'n_He_exc_a_atoms_m3', 'u_B_a_m_s', 'v_e_a_m_s', 'K_2_iz_a_m3_s_atom','K_loss_a_m6_s_atom2', 'K_iz_exc_a_m3_s_atom', 'K_exc_a_m3_s_atom', 'K_iz_a_m3_s_atom', 'K_elastic_a_m3_s_atom', 'E_elastic_a_kgm2_s2' ]\n",
    "\n",
    "\n",
    "df_time_a = rescale(np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:]))\n",
    "df_time_a_units = np.array(pd.read_excel(df_2, sheet_name='time_a_data_units').iloc[:,1:])\n",
    "df_time_a_n = pd.read_excel(df_2, sheet_name='time_a_data_names').iloc[:,1:]\n",
    "time_a_n= [df_time_a_n.iloc[0,i] for i in range(0,df_time_a_n.shape[1])]\n",
    "#print(df_time_a_n)\n",
    "#print(df_time_a_units)\n",
    "\n",
    "\n",
    "#                   0             1                2               3                   4              5          6             7                   8                    9                       10              11                   12                   13                  \n",
    "#df2_names =  [ 't_b_seconds', 'Q_b_As', 'V_p_tb_kgm2_s3_A','T_e_kgm2_s3_A', 'n_He_exc_atoms_m3', 'u_B_m_s', 'v_e_m_s','K_2_iz_m3_s_atom','K_loss_m6_s_atom2', 'K_iz_exc_m3_s_atom', 'K_exc_m3_s_atom', 'K_iz_m3_s_atom', 'K_elastic_m3_s_atom', 'E_elastic_kgm2_s2' ]\n",
    "df_time_b = rescale(np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]))\n",
    "df_time_b_units = np.array(pd.read_excel(df_2, sheet_name='time_b_data_units').iloc[:,1:])\n",
    "df_time_b_n = pd.read_excel(df_2, sheet_name='time_b_data_names').iloc[:,1:]\n",
    "time_b_n= [df_time_b_n.iloc[0,i] for i in range(0,df_time_b_n.shape[1])]\n",
    "\n",
    "\n",
    "#                    0                   1                 2               3                 4               5               6                 7                  8           9        10        11                12                   13               14                   \n",
    "#df3_names = ['E_period_kgm2_s2', 'n_sa_atoms_m3','n_sb_atoms_m3', 'n_e_electrons_m3', 'n_g_atoms_m3', 'T_g_kelvin', 'E_iz_kgm2_s2', 'E_iz_exc_kgm2_s2', 'E_exc_kgm2_s2', 'e_c_As', 'm_e_kg', 'M_He_kg',  'epsilon_A2s4_kg_m3', 'eps_0_A2s4_kg_m3', 'k_b_kgm2_s2_K']\n",
    "df_other = rescale(np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]))\n",
    "df_other_units = np.array(pd.read_excel(df_2, sheet_name='other_data_units').iloc[:,1:])\n",
    "df_other_n = pd.read_excel(df_2, sheet_name='other_data_names').iloc[:,1:]\n",
    "other_n= [df_other_n.iloc[0,i] for i in range(0,df_other_n.shape[1])]\n",
    "\n",
    "#                    0                 1             2        3        4         5            6            7            8               9              10                 11                      12                      \n",
    "#df4_names = ['Volume_rxtor_m2', 'V_all_beads_m2','A_a_m2','A_b_m2', 'h_m', 'Volume_m3', 'A_bead_m2', 'A_tot_m3', 'frequency_Hz', 'Flow_m3_s', 'temp_C_gas_K', 'Set_Voltage_kgm2_s3_A', 'pulse_time_seconds' ]\n",
    "df_exp = rescale(np.array(pd.read_excel(df_2, sheet_name='Experiment_Design_data').iloc[:,1:]))\n",
    "df_exp_units = np.array(pd.read_excel(df_2, sheet_name='Experiment_Data_units').iloc[:,1:])\n",
    "df_exp_n = pd.read_excel(df_2, sheet_name='Experiment_Data_names').iloc[:,1:]\n",
    "exp_n= [df_exp_n.iloc[0,i] for i in range(0,df_exp_n.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all terms\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack([df_time_a,df_time_b, df_other, df_exp])\n",
    "# D_in = np.hstack([df_time_a_units, df_time_b_units, df_other_units, df_exp_units])\n",
    "# variables = time_a_n + time_b_n + other_n + exp_n\n",
    "# print(D_in.shape)\n",
    "# print(len(variables))\n",
    "# print(variables[31]) #delete #31 for the MB.\n",
    "# inputs = np.delete(inputs, 31, axis = 1)\n",
    "# D_in = np.delete(D_in, 31, axis = 1)\n",
    "# variables.pop(31)\n",
    "# print(variables[31]) #good check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all MB terms\n",
    "# # time a:\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# # u_B_a [5], K_iz_a [11], K_iz_exc_a [9], n_He_exc_a [4], K_2_iz_a [7], t_a [0]\n",
    "# ta_inputs = np.hstack((df_time_a[:,5].reshape(a,1), df_time_a[:,11].reshape(a,1), df_time_a[:,9].reshape(a,1), df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1), df_time_a[:,0].reshape(a,1)))\n",
    "# ta_D_in = np.hstack((df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_time_a_units[:,9].reshape(6,1), df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1), df_time_a_units[:,0].reshape(6,1)))\n",
    "# ta_n = [time_a_n[5], time_a_n[11], time_a_n[9], time_a_n[4], time_a_n[7], time_a_n[0] ]\n",
    "\n",
    "\n",
    "# # time b\n",
    "# # u_B [5], K_iz [11], n_He_exc [4], K_iz_exc [9], t_b [0], K_2_iz [7]       \n",
    "# tb_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,11].reshape(a,1), df_time_b[:,4].reshape(a,1), df_time_b[:,9].reshape(a,1), df_time_b[:,0].reshape(a,1), df_time_b[:,7].reshape(a,1)))\n",
    "# tb_D_in = np.hstack((df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,11].reshape(6,1), df_time_b_units[:,4].reshape(6,1), df_time_b_units[:,9].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_time_b_units[:,7].reshape(6,1)))\n",
    "# tb_n = [time_b_n[5], time_b_n[11], time_b_n[4], time_b_n[9], time_b_n[0], time_b_n[7] ]\n",
    "\n",
    "\n",
    "# # df other\n",
    "# # ng [4] , n_sa [1], n_sb [2], \n",
    "# other_inputs = np.hstack(( df_other[:,4].reshape(a,1), df_other[:,1].reshape(a,1), df_other[:,2].reshape(a,1) ))\n",
    "# other_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_other_units[:,1].reshape(6,1), df_other_units[:,2].reshape(6,1)  ))\n",
    "# other_n_in = [ other_n[4], other_n[1], other_n[2] ]\n",
    "# print(other_n_in)\n",
    "# # print(len(other_n_in))\n",
    "# print(other_D_in.shape)\n",
    "# print(other_inputs.shape)\n",
    "\n",
    "# #df experimental\n",
    "# # A_tot [7], Volume [5], A_a [2]\n",
    "# exp_inputs = np.hstack(( df_exp[:,7].reshape(a,1), df_exp[:,5].reshape(a,1), df_exp[:,2].reshape(a,1)  ))\n",
    "# exp_D_in = np.hstack(( df_exp_units[:,7].reshape(6,1), df_exp_units[:,5].reshape(6,1), df_exp_units[:,2].reshape(6,1) ))\n",
    "# exp_n_in = [ exp_n[7], exp_n[5], exp_n[2] ]\n",
    "\n",
    "# #all together\n",
    "# inputs = np.hstack(( ta_inputs, tb_inputs, other_inputs, exp_inputs))\n",
    "# D_in = np.hstack(( ta_D_in, tb_D_in, other_D_in, exp_D_in ))\n",
    "# variables = ta_n+tb_n+other_n_in+exp_n_in\n",
    "# print(variables)\n",
    "# print(len(variables))\n",
    "# # A_tot, Volume, A_a\n",
    "\n",
    "\n",
    "# #try again without n_He_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate + u_B_ms - n_He_exc_a\n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,5].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[5], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates  \n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # three major rates  \n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in ))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # four major rates  \n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates  \n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  and ta packing loss\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "# packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "# packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "# packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_a_seconds', 'K_iz_a_m3_s_atom', 'n_g_atoms_m3', 'Volume_m3', 'u_B_m_s', 't_b_seconds', 'A_tot_m3', 'n_sa_atoms_m3', 'A_a_m2', 'n_sb_atoms_m3', 'u_B_a_m_s', 'n_He_exc_a_atoms_m3', 'K_2_iz_a_m3_s_atom']\n",
      "[[ 0  3 -3  3  1  0  2 -3  2 -3  1 -3  3]\n",
      " [ 1 -1  0  0 -1  1  0  0  0  0 -1  0 -1]\n",
      " [ 0 -1  1  0  0  0  0  1  0  1  0  1 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# # six major rates  \n",
    "# # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "#  # ta packing loss, ta Vol double He exc ionization\n",
    "\n",
    "# vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "a = df_time_b[:,5].shape[0]\n",
    "ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "#tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "# ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "double_He_exc_ion_ta_inputs =  np.hstack(( df_time_a[:,4].reshape(a,1) , df_time_a[:,7].reshape(a,1) ))\n",
    "double_He_exc_ion_ta_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "double_He_exc_ion_ta_n = [time_a_n[4], time_a_n[7]]\n",
    "\n",
    "\n",
    "inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs, double_He_exc_ion_ta_inputs))\n",
    "D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in, double_He_exc_ion_ta_D_in))\n",
    "variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n + double_He_exc_ion_ta_n\n",
    "print(variables)\n",
    "print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + just the variable n_He_exc_a\n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7], n_He_exc_a  ta[4]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) , df_time_a[:,4].reshape(a,1)))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1), df_time_a_units[:,4].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7], time_a_n[4] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate \n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[4], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base terms and will add h and uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa', 'h'\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1),df_exp[:,4].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1),df_exp_units[:,4].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5],exp_n[4]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "# base terms with uB, minus nHeexca\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "# base terms and will add uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F +K_iz, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9], K_iz is time_b_n[11]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1),df_time_b[:,11].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1),df_time_b_units[:,11].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9], time_b_n[11]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "# base terms  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#base terms -A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],time_a_n[0],time_b_n[0],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#to compare to ketong's original code only\n",
    "#not to construct  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb', 'Aa', 'Ab', 'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_other[:,2].reshape(a,1),df_exp[:,2].reshape(a,1),df_exp[:,3].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_exp_units[:,2].reshape(6,1),df_exp_units[:,3].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],exp_n[2],exp_n[3],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 13)\n",
      "(6, 13)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(D_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.75139783 1.67625864 1.74475572 1.73139506 1.7105415  1.73322303\n",
      " 1.73383193 1.70063959 1.70497875 1.70126015 1.70868927 1.75019205\n",
      " 1.66615299 1.71115847 1.71177522 1.72589952 1.71115847 1.70435955\n",
      " 1.70435955 1.70868927 1.69815509 1.71423998 1.70807141 1.71239174\n",
      " 1.69877655 1.71608622 1.69504434 1.69192788 1.68692956 1.68755516\n",
      " 1.70497875 1.69815509 1.70497875 1.70497875 1.71177522 1.72283887\n",
      " 1.7167012  1.71731595 1.70807141 1.70497875 1.71423998 1.72161309\n",
      " 1.71485561 1.71608622 1.74051573 1.73139506 1.74958885 1.75561155\n",
      " 1.8972047  1.93957175 1.92646639 1.87931669 1.7687893  1.70992431\n",
      " 1.71485561 1.69939779 1.83325776 1.79544258 2.27469753 2.34597034\n",
      " 2.28765361 2.03569238 1.72956515 2.52789023 1.83728375 1.80657812\n",
      " 1.79485458 1.71239174 1.71115847 1.70868927 1.7191589  1.69628933\n",
      " 1.70250058 1.69130389 1.70621649 1.69628933 1.69691147 1.70559773\n",
      " 1.70188048 1.83440895 1.75861516 1.75260279 1.73748082 1.67688822\n",
      " 1.67877554 1.68128868 1.73139506 1.70063959 1.67751756 1.67436848\n",
      " 1.73261392 1.66931758 1.72895475 1.72345143 1.7185448  1.75741433\n",
      " 1.62964242 1.68818052 1.74475572 1.71115847 1.71239174 1.70868927\n",
      " 1.70868927 1.70063959 1.70435955 1.71793049 1.71793049 1.78068446\n",
      " 1.79955316 1.82228526 1.83037664 1.79250067 1.79132256 1.74051573\n",
      " 1.71977277 1.71115847 1.80891371 1.83613437 2.00644541 2.13144011\n",
      " 1.93139132 1.75200041 1.99060063 2.28626897 1.85330022 1.81473953\n",
      " 1.76221274 1.75200041 1.72467591 1.7393024  1.69255163 1.71608622\n",
      " 1.72956515 1.72956515 1.7185448  1.71731595 1.72773331 1.72099987\n",
      " 1.71977277 1.7191589  1.75260279 1.75200041 1.7818696  1.74051573\n",
      " 1.73687321 1.74112207 1.75139783 1.73565733 1.72161309 1.73565733\n",
      " 1.74656971 1.75501021 1.84015406 1.84473726 1.86635379 1.73808823\n",
      " 1.72038643 1.75741433 1.76400878 1.87256464 2.12449568 1.72222609\n",
      " 1.76460706 1.72406378 1.77712427 1.71608622 1.72834414 1.81822607\n",
      " 1.76101437 1.71793049 1.81299375 1.73139506 1.71731595 1.71608622\n",
      " 1.68128868 1.70435955 1.69255163 1.70683502 1.71300804 1.72773331\n",
      " 1.76640066 1.69192788 1.69255163 1.76041487 1.73322303 1.70868927\n",
      " 1.84759599 1.73626538 1.75139783 1.80189786 1.7185448  1.70745332\n",
      " 1.71177522 1.70621649 1.75019205 1.70126015 1.69442151 1.70188048\n",
      " 1.7105415  1.69442151 1.69005524 1.74233413 1.71608622 1.76580299\n",
      " 1.73383193 1.70807141 1.7105415  1.73687321 1.73504908 1.73383193\n",
      " 1.7191589  1.7093069  1.77415199 1.83440895 1.81822607 2.02842017\n",
      " 1.89776099 1.89609165 2.13490386 2.1565486  2.02373138 1.82633543\n",
      " 1.71608622 1.82806847 1.98529094 2.06503628 2.794436   2.29088117\n",
      " 1.91657856 1.7818696  1.72712226 1.72712226 1.94283428 1.80657812\n",
      " 1.80365438]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.707010Z",
     "start_time": "2023-05-31T02:42:27.700438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51943551e+12 3.56158345e+12 2.97162120e+12 1.84458103e+12\n",
      " 2.12489975e+12 2.14464374e+12 1.30196278e+11 2.26762174e+12\n",
      " 2.38919966e+12 2.91474460e+12 2.72643688e+12 3.06167083e+12\n",
      " 3.30360641e+12 3.40169380e+12 3.52347974e+12 4.13614561e+12\n",
      " 3.70259953e+12 4.15545809e+12 4.53375060e+12 4.79507193e+12\n",
      " 4.77606346e+12 5.49205960e+12 5.64787710e+12 6.00907749e+12\n",
      " 6.18808818e+12 5.95379963e+12 5.43290755e+12 5.22390848e+12\n",
      " 5.48725682e+12 5.37510813e+12 5.75448094e+12 6.36396974e+12\n",
      " 6.55629012e+12 8.23842378e+12 6.12069725e+12 6.49801372e+12\n",
      " 7.03946594e+12 6.49837947e+12 6.77210997e+12 6.97867152e+12\n",
      " 9.90481641e+12 8.79427363e+12 1.04523066e+13 8.07011423e+12\n",
      " 8.74244337e+12 8.82365196e+12 1.03730443e+13 1.30630586e+13\n",
      " 1.08544228e+13 1.26822673e+13 2.20932942e+12 1.95906509e+12\n",
      " 4.47077675e+12 5.69270370e+12 9.15441961e+12 1.48275640e+13\n",
      " 1.07098332e+13 1.29769077e+13 1.05798410e+13 1.53147809e+13\n",
      " 1.51329505e+13 1.37703511e+13 7.70741667e+12 1.30296257e+13\n",
      " 9.14850804e+11 1.04810966e+12 1.02715085e+12 2.18005913e+12\n",
      " 1.75213965e+12 2.69959168e+12 2.62167832e+12 3.19631557e+12\n",
      " 4.56930111e+12 4.74192901e+12 5.48149592e+12 4.23674581e+12\n",
      " 5.17267386e+12 5.57984457e+12 6.26058028e+12 5.76553381e+11\n",
      " 8.88426721e+11 9.94577143e+11 1.01217705e+12 1.94521839e+12\n",
      " 2.67116703e+12 3.12125832e+12 3.38279465e+12 2.44293369e+12\n",
      " 3.45317073e+12 3.50628212e+12 4.73871750e+12 4.57189464e+12\n",
      " 5.15879892e+12 5.79336138e+12 6.45957445e+12 9.66300234e+11\n",
      " 1.57471770e+12 1.39912937e+11 1.79999272e+12 3.07331908e+12\n",
      " 3.54412026e+12 3.80105708e+12 4.39928494e+12 4.95250272e+12\n",
      " 6.21261730e+12 5.64842343e+12 9.70302641e+12 6.38919602e+12\n",
      " 7.40369841e+12 7.78904817e+12 1.17484807e+13 1.51725321e+12\n",
      " 2.12717367e+12 4.81244824e+12 3.92950920e+12 4.70103986e+12\n",
      " 6.08237388e+12 1.06295715e+13 8.24738043e+12 1.30428256e+13\n",
      " 2.14072322e+12 5.25979413e+12 1.23787571e+13 1.41491452e+13\n",
      " 8.10405464e+11 1.12919270e+12 1.23643754e+12 1.37293008e+12\n",
      " 1.97686001e+12 2.72691751e+12 2.53754953e+12 3.09140983e+12\n",
      " 3.16742747e+12 4.29636114e+12 4.38787257e+12 5.35318047e+12\n",
      " 4.34792800e+12 5.77192503e+12 6.59604060e+12 7.40743999e+12\n",
      " 1.21965906e+12 1.49463987e+12 1.68275890e+12 2.53012713e+12\n",
      " 2.80343381e+12 3.89697353e+12 4.46071214e+12 7.11045372e+12\n",
      " 5.05946295e+12 6.12258878e+12 5.74445821e+12 1.41212206e+13\n",
      " 6.23105385e+12 8.54751509e+12 8.71475347e+12 1.55355842e+12\n",
      " 2.59347955e+12 4.41397485e+12 8.20085117e+12 5.92840409e+12\n",
      " 8.22116604e+12 2.94884111e+12 9.19373487e+12 6.97640034e+12\n",
      " 1.09829725e+12 1.91783070e+12 4.00661917e+12 9.68882208e+11\n",
      " 2.16339817e+12 3.43983646e+12 1.55524339e+12 3.27875913e+12\n",
      " 5.64757830e+12 9.76494353e+11 1.62905254e+12 2.78213184e+12\n",
      " 4.79108450e+12 2.11336102e+12 3.54210839e+12 1.48131428e+12\n",
      " 8.15561522e+11 3.70754442e+12 5.35814393e+12 8.76562753e+11\n",
      " 1.90356579e+12 3.22809399e+12 8.68877703e+11 2.50561372e+12\n",
      " 6.04931615e+12 1.21679370e+12 3.55851366e+12 5.36533280e+12\n",
      " 6.88979366e+12 6.75732917e+12 1.22784927e+12 6.81890781e+12\n",
      " 5.51922890e+12 1.60821673e+12 5.14494044e+12 5.72694274e+12\n",
      " 3.93321464e+12 1.30303315e+12 3.66744845e+12 7.76311225e+12\n",
      " 6.24459986e+12 1.82694661e+12 5.72502848e+12 1.04852806e+13\n",
      " 9.32500318e+11 2.16478840e+12 3.19557759e+12 2.09409365e+12\n",
      " 1.95196864e+12 9.12445594e+11 1.53091961e+12 8.65070804e+11\n",
      " 1.96236453e+12 1.72584488e+12 7.05156074e+11 1.50460030e+12\n",
      " 1.31475034e+12 1.77389788e+12 2.16316308e+12 2.13327264e+12\n",
      " 1.01912615e+12 1.51950304e+12 7.84878748e+11 4.14208329e+11\n",
      " 1.13311248e+12 1.86028434e+12 3.02923400e+12 3.48460750e+12\n",
      " 3.82698903e+11 1.19743191e+12 2.33575196e+12]\n",
      "[5.22890936e-14 1.22566551e-13 1.02263885e-13 6.34784888e-14\n",
      " 7.31252369e-14 7.38046969e-14 4.48050958e-15 7.80367999e-14\n",
      " 8.22207218e-14 1.00306562e-13 9.38262348e-14 1.05362808e-13\n",
      " 1.13688658e-13 1.17064189e-13 1.21255269e-13 1.42339246e-13\n",
      " 1.27419408e-13 1.43003856e-13 1.56022225e-13 1.65015206e-13\n",
      " 1.64361059e-13 1.89000991e-13 1.94363217e-13 2.06793386e-13\n",
      " 2.12953771e-13 2.04891082e-13 1.86965363e-13 1.79772973e-13\n",
      " 1.88835711e-13 1.84976282e-13 1.98031828e-13 2.19006471e-13\n",
      " 2.25624888e-13 2.83512994e-13 2.10634613e-13 2.23619393e-13\n",
      " 2.42252659e-13 2.23631980e-13 2.33052005e-13 2.40160510e-13\n",
      " 3.40859397e-13 3.02641733e-13 3.59700451e-13 2.77720874e-13\n",
      " 3.00858072e-13 3.03652744e-13 3.56972757e-13 4.49545562e-13\n",
      " 3.73538677e-13 4.36441203e-13 7.60307575e-14 6.74182859e-14\n",
      " 1.53855074e-13 1.95905857e-13 3.15035617e-13 5.10268370e-13\n",
      " 3.68562843e-13 4.46580809e-13 3.64089354e-13 5.27035208e-13\n",
      " 5.20777789e-13 4.73885974e-13 2.65239181e-13 4.48395020e-13\n",
      " 3.14832179e-14 3.60691214e-14 3.53478556e-14 7.50234641e-14\n",
      " 6.02972574e-14 9.29023974e-14 9.02211261e-14 1.09996405e-13\n",
      " 1.57245642e-13 1.63186373e-13 1.88637458e-13 1.45801251e-13\n",
      " 1.78009810e-13 1.92021979e-13 2.15448478e-13 1.98412196e-14\n",
      " 3.05738727e-14 3.42268802e-14 3.48325545e-14 6.69417725e-14\n",
      " 9.19242057e-14 1.07413422e-13 1.16413803e-13 8.40698978e-14\n",
      " 1.18835690e-13 1.20663439e-13 1.63075853e-13 1.57334895e-13\n",
      " 1.77532325e-13 1.99369840e-13 2.22296563e-13 3.32537728e-14\n",
      " 5.41915471e-14 4.81489382e-15 6.19440488e-14 1.05763665e-13\n",
      " 1.21965581e-13 1.30807677e-13 1.51394791e-13 1.70432951e-13\n",
      " 2.13797904e-13 1.94382018e-13 3.33915097e-13 2.19874595e-13\n",
      " 2.54787173e-13 2.68048407e-13 4.04306338e-13 5.22139929e-14\n",
      " 7.32034904e-14 1.65613186e-13 1.35228164e-13 1.61779234e-13\n",
      " 2.09315772e-13 3.65800754e-13 2.83821224e-13 4.48849275e-13\n",
      " 7.36697778e-14 1.81007924e-13 4.25996354e-13 4.86921606e-13\n",
      " 2.78888882e-14 3.88594725e-14 4.25501430e-14 4.72473288e-14\n",
      " 6.80306714e-14 9.38427750e-14 8.73259602e-14 1.06386231e-13\n",
      " 1.09002264e-13 1.47852823e-13 1.51002051e-13 1.84221674e-13\n",
      " 1.49627419e-13 1.98632140e-13 2.26992841e-13 2.54915934e-13\n",
      " 4.19727368e-14 5.14357888e-14 5.79096229e-14 8.70705293e-14\n",
      " 9.64759685e-14 1.34108497e-13 1.53508716e-13 2.44695597e-13\n",
      " 1.74113827e-13 2.10699707e-13 1.97686911e-13 4.85960620e-13\n",
      " 2.14432370e-13 2.94149908e-13 2.99905166e-13 5.34633822e-14\n",
      " 8.92507077e-14 1.51900322e-13 2.82219989e-13 2.04017132e-13\n",
      " 2.82919095e-13 1.01479943e-13 3.16388592e-13 2.40082351e-13\n",
      " 3.77962522e-14 6.59992662e-14 1.37881788e-13 3.33426276e-14\n",
      " 7.44501023e-14 1.18376811e-13 5.35213679e-14 1.12833576e-13\n",
      " 1.94352934e-13 3.36045881e-14 5.60613993e-14 9.57428934e-14\n",
      " 1.64877985e-13 7.27281489e-14 1.21896346e-13 5.09772086e-14\n",
      " 2.80663262e-14 1.27589579e-13 1.84392485e-13 3.01655921e-14\n",
      " 6.55083607e-14 1.11090012e-13 2.99011226e-14 8.62269369e-14\n",
      " 2.08178140e-13 4.18741297e-14 1.22460909e-13 1.84639879e-13\n",
      " 2.37101912e-13 2.32543345e-13 4.22545905e-14 2.34662482e-13\n",
      " 1.89935982e-13 5.53443659e-14 1.77055406e-13 1.97084142e-13\n",
      " 1.35355681e-13 4.48419309e-14 1.26209736e-13 2.67155861e-13\n",
      " 2.14898536e-13 6.28716266e-14 1.97018266e-13 3.60835201e-13\n",
      " 3.20905994e-14 7.44979449e-14 1.09971009e-13 7.20650913e-14\n",
      " 6.71740724e-14 3.14004462e-14 5.26843016e-14 2.97701139e-14\n",
      " 6.75318312e-14 5.93923622e-14 2.42668883e-14 5.17785621e-14\n",
      " 4.52451606e-14 6.10460343e-14 7.44420119e-14 7.34133774e-14\n",
      " 3.50716975e-14 5.22914174e-14 2.70104246e-14 1.42543582e-14\n",
      " 3.89943660e-14 6.40188947e-14 1.04246543e-13 1.19917539e-13\n",
      " 1.31700086e-14 4.12078227e-14 8.03813993e-14]\n",
      "[0.52289094 1.22566551 1.02263885 0.63478489 0.73125237 0.73804697\n",
      " 0.0448051  0.780368   0.82220722 1.00306562 0.93826235 1.05362808\n",
      " 1.13688658 1.17064189 1.21255269 1.42339246 1.27419408 1.43003856\n",
      " 1.56022225 1.65015206 1.64361059 1.89000991 1.94363217 2.06793386\n",
      " 2.12953771 2.04891082 1.86965363 1.79772973 1.88835711 1.84976282\n",
      " 1.98031828 2.19006471 2.25624888 2.83512994 2.10634613 2.23619393\n",
      " 2.42252659 2.2363198  2.33052005 2.4016051  3.40859397 3.02641733\n",
      " 3.59700451 2.77720874 3.00858072 3.03652744 3.56972757 4.49545562\n",
      " 3.73538677 4.36441203 0.76030758 0.67418286 1.53855074 1.95905857\n",
      " 3.15035617 5.1026837  3.68562843 4.46580809 3.64089354 5.27035208\n",
      " 5.20777789 4.73885974 2.65239181 4.4839502  0.31483218 0.36069121\n",
      " 0.35347856 0.75023464 0.60297257 0.92902397 0.90221126 1.09996405\n",
      " 1.57245642 1.63186373 1.88637458 1.45801251 1.7800981  1.92021979\n",
      " 2.15448478 0.1984122  0.30573873 0.3422688  0.34832554 0.66941772\n",
      " 0.91924206 1.07413422 1.16413803 0.84069898 1.1883569  1.20663439\n",
      " 1.63075853 1.57334895 1.77532325 1.9936984  2.22296563 0.33253773\n",
      " 0.54191547 0.04814894 0.61944049 1.05763665 1.21965581 1.30807677\n",
      " 1.51394791 1.70432951 2.13797904 1.94382018 3.33915097 2.19874595\n",
      " 2.54787173 2.68048407 4.04306338 0.52213993 0.7320349  1.65613186\n",
      " 1.35228164 1.61779234 2.09315772 3.65800754 2.83821224 4.48849275\n",
      " 0.73669778 1.81007924 4.25996354 4.86921606 0.27888888 0.38859472\n",
      " 0.42550143 0.47247329 0.68030671 0.93842775 0.8732596  1.06386231\n",
      " 1.09002264 1.47852823 1.51002051 1.84221674 1.49627419 1.9863214\n",
      " 2.26992841 2.54915934 0.41972737 0.51435789 0.57909623 0.87070529\n",
      " 0.96475968 1.34108497 1.53508716 2.44695597 1.74113827 2.10699707\n",
      " 1.97686911 4.8596062  2.1443237  2.94149908 2.99905166 0.53463382\n",
      " 0.89250708 1.51900322 2.82219989 2.04017132 2.82919095 1.01479943\n",
      " 3.16388592 2.40082351 0.37796252 0.65999266 1.37881788 0.33342628\n",
      " 0.74450102 1.18376811 0.53521368 1.12833576 1.94352934 0.33604588\n",
      " 0.56061399 0.95742893 1.64877985 0.72728149 1.21896346 0.50977209\n",
      " 0.28066326 1.27589579 1.84392485 0.30165592 0.65508361 1.11090012\n",
      " 0.29901123 0.86226937 2.0817814  0.4187413  1.22460909 1.84639879\n",
      " 2.37101912 2.32543345 0.42254591 2.34662482 1.89935982 0.55344366\n",
      " 1.77055406 1.97084142 1.35355681 0.44841931 1.26209736 2.67155861\n",
      " 2.14898536 0.62871627 1.97018266 3.60835201 0.32090599 0.74497945\n",
      " 1.09971009 0.72065091 0.67174072 0.31400446 0.52684302 0.29770114\n",
      " 0.67531831 0.59392362 0.24266888 0.51778562 0.45245161 0.61046034\n",
      " 0.74442012 0.73413377 0.35071698 0.52291417 0.27010425 0.14254358\n",
      " 0.38994366 0.64018895 1.04246543 1.19917539 0.13170009 0.41207823\n",
      " 0.80381399]\n"
     ]
    }
   ],
   "source": [
    "#For predicting Te/Tg: keep top block, comment bottom block. For predicting ne/ng, keep bottom block, comment top block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TOP BLOCK ####\n",
    "\n",
    "# df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "# T_e = np.array(df_out.iloc[:,3])\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "# T_e_no_dim = T_e/T_g\n",
    "# print(T_e)\n",
    "# output = rescale_vec(T_e_no_dim)\n",
    "# print(output)\n",
    "# D_out = np.array(\n",
    "#     [\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### BOTTOM BLOCK ####\n",
    "\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_g = np.array(df_out.iloc[:,4])\n",
    "n_e_no_dim = n_e/n_g\n",
    "print(n_e)\n",
    "print(n_e_no_dim)\n",
    "output = rescale_vec(n_e_no_dim)\n",
    "print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "        [0.]\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.754325Z",
     "start_time": "2023-05-31T02:42:27.738571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.843064Z",
     "start_time": "2023-05-31T02:42:27.750866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1140,  0.8232,  2.9058,  ...,  1.3640, 11.5320,  0.2030],\n",
      "        [ 0.1400,  0.6274,  2.9058,  ...,  1.3415, 23.2932,  0.2030],\n",
      "        [ 0.1550,  0.5893,  2.9058,  ...,  1.3365, 18.8110,  0.2030],\n",
      "        ...,\n",
      "        [ 0.1650,  0.5585,  2.9058,  ...,  1.3322,  2.3629,  0.2030],\n",
      "        [ 0.1650,  0.1736,  2.9058,  ...,  1.2470,  3.9271,  0.2030],\n",
      "        [ 0.1700,  0.0746,  2.9058,  ...,  1.1936,  4.8272,  0.2030]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "fff.read_data(inputs, output)\n",
    "print(fff.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.883791Z",
     "start_time": "2023-05-31T02:42:27.758677Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim import PiLinearRegressionViaTorch\n",
    "\n",
    "\n",
    "#the code does five fold to ensure the R2 is worth while.\n",
    "# we then want to rerun the fitting on all data instead of 80% of it. The 5fold find the right number of epochs.\n",
    "# so, we re run to find the right number of epochs, and then we must store the parameters and the R2 from the 5-fold fitting.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.947167Z",
     "start_time": "2023-05-31T02:42:27.779139Z"
    }
   },
   "outputs": [],
   "source": [
    "#UPDATE RUN PARAMETERS. POLYNOMIAL ORDER? # of dim groups? Which variables are called? etc.\n",
    "\n",
    "\n",
    "ndimensionless = 1\n",
    "\n",
    "lambda_gamma = 0.003 #lambda_gamma = 0.01, #replaced\n",
    "seed = 1#3 is good! identical\n",
    "\n",
    "poly_order = 1 #was 2? but linear is a first order poly.\n",
    "\n",
    "poly_mapping = np.array([[0],\n",
    "                         [1]])\n",
    "\n",
    "#poly_mapping = np.array([[0, 0],\n",
    "#                         [1, 0],\n",
    "#                         [0, 1],\n",
    "#                         [2, 0],\n",
    "#                         [1, 1],\n",
    "#                         [0, 2]])\n",
    "lambda_beta = 0.01 #maybe cut in half idk\n",
    "w_array = np.array(fff._basis_col)  #this is the w array, that is, the columns in Null(D)\n",
    "gamma_name = ['y'+str(id) for id in range(0,fff.basis_col.shape[1]) ]\n",
    "beta_name = ['b'+str(id) for id in range(0,poly_mapping.shape[0]) ]\n",
    "poly_name = ['dim'+str(id+1) for id in range(0,ndimensionless)]\n",
    "\n",
    "\n",
    "\n",
    "metric = 'r2'\n",
    "para_threshold = 0.01\n",
    "beta_threshold = 0.005\n",
    "training_epochs =10000\n",
    "score = []\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#create test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create validation and training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "####################################################\n",
    "# SOLVE FOR DIMENSIONLESS NUMBERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        , -1.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  1.        ,  1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        , -1.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.33333333,  0.        , -0.66666667,  0.        ,\n",
       "        -0.66666667,  0.        , -0.33333333,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\\clamp0.01_1dim_order1_lambda0.003_seed1' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1dim1\n"
     ]
    }
   ],
   "source": [
    "#set save path\n",
    "\n",
    "#current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\"\n",
    "current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\"\n",
    "#new_folder =r'mb_1dim_v1_pt0001_pt_01_order1_golden_child'\n",
    "new_folder = r'clamp'+str(para_threshold)+r'_' + str(ndimensionless)+r'dim_order'+str(poly_order)+r'_lambda'+str(lambda_gamma)+r\"_seed\"+str(seed)\n",
    "new_path = current_path+'\\\\'+new_folder\n",
    "if os.path.exists(new_path):\n",
    "    print(f\"File '{new_path}' already exists.\")\n",
    "else:\n",
    "    os.mkdir(new_path)       \n",
    "    print(f\"File '{new_path}' did not exist.\")\n",
    "\n",
    "file_path = new_path+'\\\\'+new_folder +'_seed'+str(seed)+ '.xlsx'\n",
    "fig_path_dim1 = new_path+'\\\\'+new_folder+\"dim1\"\n",
    "fig_path_dim2 = new_path+'\\\\'+new_folder+\"dim2\"\n",
    "print(fig_path_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.010455233451386315 at iteration 9999, val_loss: 0.9999579239206965, best_val_loss: 0.9999698413560977: 100%|██████████| 10000/10000 [00:52<00:00, 190.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999698413560977\n",
      "tensor([[-1.0102],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0007],\n",
      "        [ 0.5385]], dtype=torch.float64)\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#find which hyperparameter performed best\n",
    "\n",
    "# idx = np.arange(lg_set.shape[0])\n",
    "# best_hp = idx[array_metric_num[:,0]==np.max(array_metric_num)]\n",
    "\n",
    "# if len(best_hp.shape)>1:\n",
    "#     print('the following are equivalent ',best_hp)\n",
    "#     print(best_hp.shape)\n",
    "#     best_hp = best_hp[0,1]\n",
    "\n",
    "\n",
    "#22 best so far, R2 = 0.998 for lambda = 0.001\n",
    "#14 best so far, R2 = 0.945\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "print(r2)\n",
    "\n",
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "#store values from training\n",
    "best_metric_num = np.array(metric_num)\n",
    "best_beta = beta_prune\n",
    "\n",
    "best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "if ndimensionless >=2:\n",
    "    best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "    best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "#r2, lambda, beta vector , cardinality \n",
    "best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "##### BELOW MUST BE CORRECTED#####\n",
    "if ndimensionless >=2:\n",
    "    \n",
    "    best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "    df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "#df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1)\n",
      "torch.Size([1, 2])\n",
      "(1, 1)\n",
      "(1, 13)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "print(best_metric_num.reshape(1,1).shape)\n",
    "print(np.array([best_hp]).reshape(1,1).shape)\n",
    "print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 13)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,    best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "#data_name =  [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "#print(best_metric_num.reshape(1,1).shape)\n",
    "#print(np.array([best_hp]).reshape(1,1).shape)\n",
    "#print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "#print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "#print(best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#[ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])print(len([ metric]))\n",
    "#print(len([metric]))\n",
    "#print(len( ['lambda']))\n",
    "#print(len( [beta_name[i] for i in range(0,len(beta_name))]))\n",
    "#print(len(['cardinality_of_gamma_dim1']))\n",
    "#print(len([gamma_name[i] for i in range(0,len(gamma_name))]))\n",
    "print(len([variables[i] for i in range(0,len(variables))]))\n",
    "print(len(['cardinality_of_w_dim1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[-0.00950858 -0.54803561 -1.01018625  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.00067767\n",
      "  0.53852703]\n"
     ]
    }
   ],
   "source": [
    "print(best_cardinality_dim1_w)\n",
    "print(best_dim1_w)\n",
    "#best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-1.0102e+00],\n",
      "        [-5.1788e-03],\n",
      "        [ 2.9934e-04],\n",
      "        [ 3.9781e-03],\n",
      "        [ 3.4776e-04],\n",
      "        [ 6.4994e-03],\n",
      "        [-4.5485e-05],\n",
      "        [-5.7584e-03],\n",
      "        [ 1.0007e+00],\n",
      "        [ 5.3853e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.1348e-04, 2.7466e-01]], dtype=torch.float64, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "print([i for i in model_train.model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0102],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0007],\n",
      "        [ 0.5385]], dtype=torch.float64)\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n"
     ]
    }
   ],
   "source": [
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{K_{2 iz a m3 s atom}^{0.538527029625826} n_{He exc a atoms m3}^{1.00067767177052}}{K_{iz a m3 s atom}^{0.548035611910215} n_{g atoms m3}^{1.01018625405491} t_{a seconds}^{0.00950858228438856}}$"
      ],
      "text/plain": [
       "K_2_iz_a_m3_s_atom**0.538527029625826*n_He_exc_a_atoms_m3**1.00067767177052/(K_iz_a_m3_s_atom**0.548035611910215*n_g_atoms_m3**1.01018625405491*t_a_seconds**0.00950858228438856)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you need the orignial pis\n",
    "readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "ori_pis[0]\n",
    "#ori_pis[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1_seed1.xlsx' does not exist.\n",
      "tensor([[-1.0102],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0007],\n",
      "        [ 0.5385]], dtype=torch.float64)\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n",
      "[-1.01018625 -0.          0.          0.          0.          0.\n",
      " -0.         -0.          1.00067767  0.53852703]\n"
     ]
    }
   ],
   "source": [
    "# File path of the Excel file\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "with pd.ExcelWriter(file_path) as writer:  \n",
    "    df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "    df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "    df4.to_excel(writer, sheet_name='beta')\n",
    "    df5.to_excel(writer, sheet_name='polynomial')\n",
    "    df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "\n",
    "    \n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW DO TOP 15% FOR D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1140,  0.8232,  2.9058,  ...,  1.3640, 11.5320,  0.2030],\n",
      "        [ 0.1400,  0.6274,  2.9058,  ...,  1.3415, 23.2932,  0.2030],\n",
      "        [ 0.1550,  0.5893,  2.9058,  ...,  1.3365, 18.8110,  0.2030],\n",
      "        ...,\n",
      "        [ 0.1650,  0.5585,  2.9058,  ...,  1.3322,  2.3629,  0.2030],\n",
      "        [ 0.1650,  0.1736,  2.9058,  ...,  1.2470,  3.9271,  0.2030],\n",
      "        [ 0.1700,  0.0746,  2.9058,  ...,  1.1936,  4.8272,  0.2030]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(fff.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete old variables\n",
    "\n",
    "variable_names = ['X_test', 'Y_test', 'X_train', 'X_val', 'y_train', 'y_val', 'X_train_val', 'Y_train_val']\n",
    "for var_name in variable_names:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        target_dict = locals() if var_name in locals() else globals()\n",
    "        del target_dict[var_name]\n",
    "        #print(f\"Deleted variable: {var_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  35\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49 122 110  48  56 117  58 207\n",
      "  42  46  40 106 162  54  45  41  44 154 153 118  33 160 158  43 109]\n",
      "flag 2\n",
      "Y_test.shape  =  (35, 1)\n",
      "X_test.shape  = (35, 13)\n",
      "flag 3\n",
      "Y_train.shape  = (200, 1)\n",
      "X_train.shape  = (200, 13)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([35, 1])\n",
      "X_test.shape  = torch.Size([35, 13])\n",
      "Y_train.shape  = torch.Size([200, 1])\n",
      "X_train.shape  = torch.Size([200, 13])\n",
      "torch.Size([35, 13])\n",
      "torch.Size([35, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644],\n",
      "        [4.2600],\n",
      "        [4.0431],\n",
      "        [3.7354],\n",
      "        [3.6856],\n",
      "        [3.6580],\n",
      "        [3.6409],\n",
      "        [3.6084],\n",
      "        [3.5970],\n",
      "        [3.5697],\n",
      "        [3.4086],\n",
      "        [3.3392],\n",
      "        [3.1639],\n",
      "        [3.1504],\n",
      "        [3.0365],\n",
      "        [3.0264],\n",
      "        [3.0086],\n",
      "        [2.9991],\n",
      "        [2.9415],\n",
      "        [2.8382],\n",
      "        [2.8351],\n",
      "        [2.8292],\n",
      "        [2.8222],\n",
      "        [2.7772],\n",
      "        [2.6805]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.0402],\n",
      "        [1.0148],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test, X_train_val, y_train_val = top_split_y(fff.X,fff.y,15)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "x_test = X_test\n",
    "y_test = Y_test\n",
    "print(y_test)\n",
    "print(y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4808246102968535 at iteration 49999, val_loss: -10.33678436182598, best_val_loss: -10.314942232547198: 100%|██████████| 50000/50000 [04:32<00:00, 183.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.314942232547198\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_epochs =50000\n",
    "\n",
    "\n",
    "best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "print(r2)\n",
    "\n",
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "#store values from training\n",
    "best_metric_num = np.array(metric_num)\n",
    "best_beta = beta_prune\n",
    "\n",
    "best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "if ndimensionless >=2:\n",
    "    best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "    best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "#r2, lambda, beta vector , cardinality \n",
    "best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "##### BELOW MUST BE CORRECTED#####\n",
    "if ndimensionless >=2:\n",
    "    \n",
    "    best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "    df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "#df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1_extrapolation_dim1\n"
     ]
    }
   ],
   "source": [
    "file_path = new_path+'\\\\'+new_folder + '_extrapolation' +'_seed'+str(seed)+ '.xlsx'\n",
    "fig_path_dim1 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim1\"\n",
    "fig_path_dim2 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim2\"\n",
    "print(fig_path_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1)\n",
      "torch.Size([1, 2])\n",
      "(1, 1)\n",
      "(1, 13)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "print(best_metric_num.reshape(1,1).shape)\n",
    "print(np.array([best_hp]).reshape(1,1).shape)\n",
    "print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 13)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,    best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "#data_name =  [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "#print(best_metric_num.reshape(1,1).shape)\n",
    "#print(np.array([best_hp]).reshape(1,1).shape)\n",
    "#print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "#print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "#print(best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#[ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])print(len([ metric]))\n",
    "#print(len([metric]))\n",
    "#print(len( ['lambda']))\n",
    "#print(len( [beta_name[i] for i in range(0,len(beta_name))]))\n",
    "#print(len(['cardinality_of_gamma_dim1']))\n",
    "#print(len([gamma_name[i] for i in range(0,len(gamma_name))]))\n",
    "print(len([variables[i] for i in range(0,len(variables))]))\n",
    "print(len(['cardinality_of_w_dim1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(best_cardinality_dim1_w)\n",
    "print(best_dim1_w)\n",
    "#best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0021],\n",
      "        [-0.0027],\n",
      "        [ 0.0017],\n",
      "        [ 0.0057],\n",
      "        [ 0.0027],\n",
      "        [ 0.0001],\n",
      "        [ 0.0012],\n",
      "        [-0.0002],\n",
      "        [ 0.0008],\n",
      "        [-0.0006]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.1990, -0.0021]], dtype=torch.float64, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "print([i for i in model_train.model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle K_{2 iz a m3 s atom}^{0.0}$"
      ],
      "text/plain": [
       "K_2_iz_a_m3_s_atom**0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you need the orignial pis\n",
    "readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "ori_pis[0]\n",
    "#ori_pis[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "ori_pis[0]\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.matmul(D_in,best_dim1_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_no_val\\packed\\top_6_rates\\diagnostic\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1_extrapolation_seed1.xlsx' does not exist.\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n",
      "[-0. -0.  0.  0.  0.  0.  0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "# File path of the Excel file\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "with pd.ExcelWriter(file_path) as writer:  \n",
    "    df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "    df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "    df4.to_excel(writer, sheet_name='beta')\n",
    "    df5.to_excel(writer, sheet_name='polynomial')\n",
    "    df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "\n",
    "    \n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(2+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
