{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.163733Z",
     "start_time": "2023-05-31T02:42:22.636891Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim.utils import DimensionlessLearning\n",
    "from vics_fcns import top_split_y#, top_split_x\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.572932Z",
     "start_time": "2023-05-31T02:42:27.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.ExcelFile(r\"mass_balance_params.xlsx\")\n",
    "df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\collected_output_files_packing\\data_from_EB_looping_all_sizes_remove_extraneous_dims.xlsx')\n",
    "#df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\data\\data_no_packing_all\\data_from_EB_looping_new_excel_new_code_no_packing_v3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.660878Z",
     "start_time": "2023-05-31T02:42:27.634288Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_x['u_B_m_s'] = df_train_x['u_B_m_s'] / 1000 if np.min(df_train_x['u_B_m_s']) > 0 else df_train_x['u_B_m_s'] / 1000 - (np.min(df_train_x['u_B_m_s']) / 1000-1e-5)\n",
    "# df_train_x['A_tot_m2'] = 1.\n",
    "# df_train_x['t_a_s'] = df_train_x['t_a_s'] / 1e-7 if np.min(df_train_x['t_a_s']) > 0 else df_train_x['t_a_s'] / 1e-7 - (np.min(df_train_x['t_a_s']) / 1e-7-1e-5)\n",
    "# df_train_x['t_b_s'] = df_train_x['t_b_s'] / 1e-4 if np.min(df_train_x['t_b_s']) > 0 else df_train_x['t_b_s'] / 1e-4 - (np.min(df_train_x['t_b_s']) / 1e-4-1e-5)\n",
    "# df_train_x['Volume_m3'] = 1.\n",
    "# df_train_x['K_iz_a_m3_s_atom'] = df_train_x['K_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_iz_a_m3_s_atom']) > 0 else df_train_x['K_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_2_iz_a_m3_s_atom'] = df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_2_iz_a_m3_s_atom']) > 0 else df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_2_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_iz_exc_a_m3_s_atom'] = df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 if np.min(df_train_x['K_iz_exc_a_m3_s_atom']) > 0 else df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 - (np.min(df_train_x['K_iz_exc_a_m3_s_atom']) / 1e-14-1e-5)\n",
    "\n",
    "# df_train_x['n_sa_atoms_m3'] = df_train_x['n_sa_atoms_m3'] / 1e10 if np.min(df_train_x['n_sa_atoms_m3']) > 0 else df_train_x['n_sa_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sa_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['n_sb_atoms_m3'] = df_train_x['n_sb_atoms_m3'] / 1e10 if np.min(df_train_x['n_sb_atoms_m3']) > 0 else df_train_x['n_sb_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sb_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['A_a_m2'] = 1.\n",
    "\n",
    "# df_train_x['A_b_m2'] = 1.\n",
    "\n",
    "# df_train_x['n_He_exc_a_atoms_m3'] = df_train_x['n_He_exc_a_atoms_m3'] / 1e17 if np.min(df_train_x['n_He_exc_a_atoms_m3']) > 0 else df_train_x['n_He_exc_a_atoms_m3'] / 1e17 - (np.min(df_train_x['n_He_exc_a_atoms_m3']) / 1e17-1e-5)\n",
    "\n",
    "def rescale(g):\n",
    "    for i in range(0,g.shape[1]):\n",
    "        if np.min(np.abs(g[:,i])) ==0:\n",
    "            n = 0\n",
    "        else:\n",
    "            n = np.mean((np.log10(np.min(np.abs(g[:,i]))), np.log10(np.max(np.abs(g[:,i])))))\n",
    "        if n<0:\n",
    "                g[:,i] = g[:,i]/10**np.ceil(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.ceil(n) - (np.min(g[:,i]) / 10**np.ceil(n)-1e-5)\n",
    "        else:\n",
    "            g[:,i] = g[:,i]/10**np.floor(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.floor(n) - (np.min(g[:,i]) / 10**np.floor(n)-1e-5)\n",
    "    return g\n",
    "\n",
    "def rescale_vec(g):\n",
    "    if np.min(np.abs(g)) ==0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = np.mean((np.log10(np.min(np.abs(g))), np.log10(np.max(np.abs(g)))))\n",
    "    if n<0:\n",
    "            g= g/10**np.ceil(n) if np.min(g) > 0 else g/10**np.ceil(n) - (np.min(g) / 10**np.ceil(n)-1e-5)\n",
    "    else:\n",
    "        g = g/10**np.floor(n) if np.min(g) > 0 else g/10**np.floor(n) - (np.min(g) / 10**np.floor(n)-1e-5)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.706539Z",
     "start_time": "2023-05-31T02:42:27.668685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#                   0             1           2                    3                   4                   5           6                7                    8                     9                        10                  11                  12                      13                          \n",
    "#df1_names = [ 't_a_seconds', 'Q_a_As', 'V_p_ta_kgm2_s3_A','T_e_a_kgm2_s3_A', 'n_He_exc_a_atoms_m3', 'u_B_a_m_s', 'v_e_a_m_s', 'K_2_iz_a_m3_s_atom','K_loss_a_m6_s_atom2', 'K_iz_exc_a_m3_s_atom', 'K_exc_a_m3_s_atom', 'K_iz_a_m3_s_atom', 'K_elastic_a_m3_s_atom', 'E_elastic_a_kgm2_s2' ]\n",
    "\n",
    "\n",
    "df_time_a = rescale(np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:]))\n",
    "df_time_a_units = np.array(pd.read_excel(df_2, sheet_name='time_a_data_units').iloc[:,1:])\n",
    "df_time_a_n = pd.read_excel(df_2, sheet_name='time_a_data_names').iloc[:,1:]\n",
    "time_a_n= [df_time_a_n.iloc[0,i] for i in range(0,df_time_a_n.shape[1])]\n",
    "#print(df_time_a_n)\n",
    "#print(df_time_a_units)\n",
    "\n",
    "\n",
    "#                   0             1                2               3                   4              5          6             7                   8                    9                       10              11                   12                   13                  \n",
    "#df2_names =  [ 't_b_seconds', 'Q_b_As', 'V_p_tb_kgm2_s3_A','T_e_kgm2_s3_A', 'n_He_exc_atoms_m3', 'u_B_m_s', 'v_e_m_s','K_2_iz_m3_s_atom','K_loss_m6_s_atom2', 'K_iz_exc_m3_s_atom', 'K_exc_m3_s_atom', 'K_iz_m3_s_atom', 'K_elastic_m3_s_atom', 'E_elastic_kgm2_s2' ]\n",
    "df_time_b = rescale(np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]))\n",
    "df_time_b_units = np.array(pd.read_excel(df_2, sheet_name='time_b_data_units').iloc[:,1:])\n",
    "df_time_b_n = pd.read_excel(df_2, sheet_name='time_b_data_names').iloc[:,1:]\n",
    "time_b_n= [df_time_b_n.iloc[0,i] for i in range(0,df_time_b_n.shape[1])]\n",
    "\n",
    "\n",
    "#                    0                   1                 2               3                 4               5               6                 7                  8           9        10        11                12                   13               14                   \n",
    "#df3_names = ['E_period_kgm2_s2', 'n_sa_atoms_m3','n_sb_atoms_m3', 'n_e_electrons_m3', 'n_g_atoms_m3', 'T_g_kelvin', 'E_iz_kgm2_s2', 'E_iz_exc_kgm2_s2', 'E_exc_kgm2_s2', 'e_c_As', 'm_e_kg', 'M_He_kg',  'epsilon_A2s4_kg_m3', 'eps_0_A2s4_kg_m3', 'k_b_kgm2_s2_K']\n",
    "df_other = rescale(np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]))\n",
    "df_other_units = np.array(pd.read_excel(df_2, sheet_name='other_data_units').iloc[:,1:])\n",
    "df_other_n = pd.read_excel(df_2, sheet_name='other_data_names').iloc[:,1:]\n",
    "other_n= [df_other_n.iloc[0,i] for i in range(0,df_other_n.shape[1])]\n",
    "\n",
    "#                    0                 1             2        3        4         5            6            7            8               9              10                 11                      12                      \n",
    "#df4_names = ['Volume_rxtor_m2', 'V_all_beads_m2','A_a_m2','A_b_m2', 'h_m', 'Volume_m3', 'A_bead_m2', 'A_tot_m3', 'frequency_Hz', 'Flow_m3_s', 'temp_C_gas_K', 'Set_Voltage_kgm2_s3_A', 'pulse_time_seconds' ]\n",
    "df_exp = rescale(np.array(pd.read_excel(df_2, sheet_name='Experiment_Design_data').iloc[:,1:]))\n",
    "df_exp_units = np.array(pd.read_excel(df_2, sheet_name='Experiment_Data_units').iloc[:,1:])\n",
    "df_exp_n = pd.read_excel(df_2, sheet_name='Experiment_Data_names').iloc[:,1:]\n",
    "exp_n= [df_exp_n.iloc[0,i] for i in range(0,df_exp_n.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all terms\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack([df_time_a,df_time_b, df_other, df_exp])\n",
    "# D_in = np.hstack([df_time_a_units, df_time_b_units, df_other_units, df_exp_units])\n",
    "# variables = time_a_n + time_b_n + other_n + exp_n\n",
    "# print(D_in.shape)\n",
    "# print(len(variables))\n",
    "# print(variables[31]) #delete #31 for the MB.\n",
    "# inputs = np.delete(inputs, 31, axis = 1)\n",
    "# D_in = np.delete(D_in, 31, axis = 1)\n",
    "# variables.pop(31)\n",
    "# print(variables[31]) #good check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all MB terms\n",
    "# # time a:\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# # u_B_a [5], K_iz_a [11], K_iz_exc_a [9], n_He_exc_a [4], K_2_iz_a [7], t_a [0]\n",
    "# ta_inputs = np.hstack((df_time_a[:,5].reshape(a,1), df_time_a[:,11].reshape(a,1), df_time_a[:,9].reshape(a,1), df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1), df_time_a[:,0].reshape(a,1)))\n",
    "# ta_D_in = np.hstack((df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_time_a_units[:,9].reshape(6,1), df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1), df_time_a_units[:,0].reshape(6,1)))\n",
    "# ta_n = [time_a_n[5], time_a_n[11], time_a_n[9], time_a_n[4], time_a_n[7], time_a_n[0] ]\n",
    "\n",
    "\n",
    "# # time b\n",
    "# # u_B [5], K_iz [11], n_He_exc [4], K_iz_exc [9], t_b [0], K_2_iz [7]       \n",
    "# tb_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,11].reshape(a,1), df_time_b[:,4].reshape(a,1), df_time_b[:,9].reshape(a,1), df_time_b[:,0].reshape(a,1), df_time_b[:,7].reshape(a,1)))\n",
    "# tb_D_in = np.hstack((df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,11].reshape(6,1), df_time_b_units[:,4].reshape(6,1), df_time_b_units[:,9].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_time_b_units[:,7].reshape(6,1)))\n",
    "# tb_n = [time_b_n[5], time_b_n[11], time_b_n[4], time_b_n[9], time_b_n[0], time_b_n[7] ]\n",
    "\n",
    "\n",
    "# # df other\n",
    "# # ng [4] , n_sa [1], n_sb [2], \n",
    "# other_inputs = np.hstack(( df_other[:,4].reshape(a,1), df_other[:,1].reshape(a,1), df_other[:,2].reshape(a,1) ))\n",
    "# other_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_other_units[:,1].reshape(6,1), df_other_units[:,2].reshape(6,1)  ))\n",
    "# other_n_in = [ other_n[4], other_n[1], other_n[2] ]\n",
    "# print(other_n_in)\n",
    "# # print(len(other_n_in))\n",
    "# print(other_D_in.shape)\n",
    "# print(other_inputs.shape)\n",
    "\n",
    "# #df experimental\n",
    "# # A_tot [7], Volume [5], A_a [2]\n",
    "# exp_inputs = np.hstack(( df_exp[:,7].reshape(a,1), df_exp[:,5].reshape(a,1), df_exp[:,2].reshape(a,1)  ))\n",
    "# exp_D_in = np.hstack(( df_exp_units[:,7].reshape(6,1), df_exp_units[:,5].reshape(6,1), df_exp_units[:,2].reshape(6,1) ))\n",
    "# exp_n_in = [ exp_n[7], exp_n[5], exp_n[2] ]\n",
    "\n",
    "# #all together\n",
    "# inputs = np.hstack(( ta_inputs, tb_inputs, other_inputs, exp_inputs))\n",
    "# D_in = np.hstack(( ta_D_in, tb_D_in, other_D_in, exp_D_in ))\n",
    "# variables = ta_n+tb_n+other_n_in+exp_n_in\n",
    "# print(variables)\n",
    "# print(len(variables))\n",
    "# # A_tot, Volume, A_a\n",
    "\n",
    "\n",
    "# #try again without n_He_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates  same for packed / unpacked\n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate + u_B_ms - n_He_exc_a\n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,5].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[5], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # three major rates  same for packed / unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in ))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # four major rates, same for packed / unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates--packed\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  and ta packing loss\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "# packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "# packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "# packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates--unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  and ta loss electrode a\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta loss electrode a: n_sa[na] * u_B_a df_time_a[:,5] * A_a[na] * t_a [na]\n",
    "# ta_loss_elec_a_inputs = df_time_a[:,5].reshape(a,1)\n",
    "# ta_loss_elec_a_D_in = df_time_a_units[:,5].reshape(6,1)\n",
    "# ta_loss_elec_a_n = [ time_a_n[5] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, ta_loss_elec_a_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, ta_loss_elec_a_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+ ta_loss_elec_a_n \n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # six major rates--packed case\n",
    "# # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "#  # ta packing loss, ta Vol double He exc ionization\n",
    "\n",
    "# vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "a = df_time_b[:,5].shape[0]\n",
    "ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "#tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "# ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "double_He_exc_ion_ta_inputs =  np.hstack(( df_time_a[:,4].reshape(a,1) , df_time_a[:,7].reshape(a,1) ))\n",
    "double_He_exc_ion_ta_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "double_He_exc_ion_ta_n = [time_a_n[4], time_a_n[7]]\n",
    "\n",
    "\n",
    "inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs, double_He_exc_ion_ta_inputs))\n",
    "D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in, double_He_exc_ion_ta_D_in))\n",
    "variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n + double_He_exc_ion_ta_n\n",
    "print(variables)\n",
    "print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_a_seconds', 'K_iz_a_m3_s_atom', 'n_g_atoms_m3', 'Volume_m3', 'u_B_m_s', 't_b_seconds', 'A_tot_m3', 'n_sa_atoms_m3', 'A_a_m2', 'n_sb_atoms_m3', 'u_B_a_m_s', 'n_He_exc_a_atoms_m3', 'K_2_iz_a_m3_s_atom']\n",
      "[[ 0  3 -3  3  1  0  2 -3  2 -3  1 -3  3]\n",
      " [ 1 -1  0  0 -1  1  0  0  0  0 -1  0 -1]\n",
      " [ 0 -1  1  0  0  0  0  1  0  1  0  1 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# # # six and seven major rates--unpacked. The sixth major rate, ta packing wall loss, has no new terms and\n",
    "# # # hence provides no new information. \n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# # #  ta loss electrode a, ta packing wall loss, ta_vol_double_He_exc_ion\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta loss electrode a: n_sa[na] * u_B_a df_time_a[:,5] * A_a[na] * t_a [na]\n",
    "# ta_loss_elec_a_inputs = df_time_a[:,5].reshape(a,1)\n",
    "# ta_loss_elec_a_D_in = df_time_a_units[:,5].reshape(6,1)\n",
    "# ta_loss_elec_a_n = [ time_a_n[5] ]\n",
    "\n",
    "# # ta packing wall loss: n_e [na], u_B_a [na], A_tot [na], t_a [na]\n",
    "# # # all redundant\n",
    "\n",
    "# # # ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "# double_He_exc_ion_ta_inputs =  np.hstack(( df_time_a[:,4].reshape(a,1) , df_time_a[:,7].reshape(a,1) ))\n",
    "# double_He_exc_ion_ta_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# double_He_exc_ion_ta_n = [time_a_n[4], time_a_n[7]]\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, ta_loss_elec_a_inputs, double_He_exc_ion_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, ta_loss_elec_a_D_in, double_He_exc_ion_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+ ta_loss_elec_a_n +double_He_exc_ion_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + just the variable n_He_exc_a\n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7], n_He_exc_a  ta[4]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) , df_time_a[:,4].reshape(a,1)))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1), df_time_a_units[:,4].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7], time_a_n[4] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate \n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[4], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base terms and will add h and uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa', 'h'\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1),df_exp[:,4].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1),df_exp_units[:,4].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5],exp_n[4]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "# base terms with uB, minus nHeexca\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "# base terms and will add uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F +K_iz, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9], K_iz is time_b_n[11]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1),df_time_b[:,11].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1),df_time_b_units[:,11].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9], time_b_n[11]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "# base terms  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#base terms -A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],time_a_n[0],time_b_n[0],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#to compare to ketong's original code only\n",
    "#not to construct  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb', 'Aa', 'Ab', 'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_other[:,2].reshape(a,1),df_exp[:,2].reshape(a,1),df_exp[:,3].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_exp_units[:,2].reshape(6,1),df_exp_units[:,3].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],exp_n[2],exp_n[3],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 13)\n",
      "(6, 13)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(D_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73383193 1.72345143 1.92042988 1.98369526 2.00907405 1.75139783\n",
      " 1.71300804 1.92975108 2.00065023 2.17409766 1.78660233 1.8877228\n",
      " 1.67247618 2.05581503 1.91216756 1.67562882 1.87763096 1.69815509\n",
      " 1.7818696  1.92372487 1.84301989 1.69691147 1.6975334  1.726511\n",
      " 1.72345143 1.73322303 1.9286568  2.0211218  2.00012256 2.157038\n",
      " 2.08993244 1.79603038 1.93684878 1.93029798 1.87875495 1.99695363\n",
      " 2.15262938 2.16046071 1.98209829 2.157038   1.90774635 1.87030853\n",
      " 2.07268931 2.64062231 2.39495204 1.90719298 2.3201822  2.52120047]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.707010Z",
     "start_time": "2023-05-31T02:42:27.700438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88496592e+12 8.40777158e+12 1.84935323e+12 4.25936461e+12\n",
      " 1.52310600e+12 4.80836688e+12 3.18989622e+12 3.88829674e+12\n",
      " 3.41572870e+12 1.60334052e+12 2.27147985e+12 1.43269087e+12\n",
      " 5.45898365e+12 1.89631062e+12 3.99972174e+12 8.99767792e+12\n",
      " 2.62653575e+12 9.81651028e+12 3.21371282e+12 2.41836809e+12\n",
      " 3.88826310e+12 3.00058897e+12 9.85800589e+12 6.53727255e+12\n",
      " 1.60799513e+13 2.91132946e+13 1.53315588e+12 2.52187327e+12\n",
      " 2.11925785e+12 1.18618153e+12 1.83666202e+12 3.74674399e+12\n",
      " 1.24121146e+12 5.16922311e+11 1.81490079e+12 1.01293491e+12\n",
      " 1.46015934e+12 8.21923413e+11 7.66117709e+11 9.93852477e+11\n",
      " 1.83129375e+12 2.00980064e+12 2.11766942e+12 4.04949778e+11\n",
      " 8.91117102e+11 1.40750655e+12 3.30500172e+11 7.00186721e+11]\n",
      "[1.33695274e-13 2.89340845e-13 6.36427167e-14 1.46579643e-13\n",
      " 5.24154078e-14 1.65472732e-13 1.09775493e-13 1.33809899e-13\n",
      " 1.17547179e-13 5.51765584e-14 7.81695712e-14 4.93039068e-14\n",
      " 1.87862732e-13 6.52586848e-14 1.37644423e-13 3.09641586e-13\n",
      " 9.03882873e-14 3.37820474e-13 1.10595105e-13 8.32245094e-14\n",
      " 1.33808741e-13 1.03260768e-13 3.39248483e-13 2.24970427e-13\n",
      " 5.53367398e-13 1.00189035e-12 5.27612596e-14 8.67864852e-14\n",
      " 7.29310794e-14 4.08206579e-14 6.32059680e-14 1.28938573e-13\n",
      " 4.27144303e-14 1.77891058e-14 6.24570878e-14 3.48586353e-14\n",
      " 5.02491931e-14 2.82852611e-14 2.63647916e-14 3.42019419e-14\n",
      " 6.30212271e-14 6.91642736e-14 7.28764159e-14 1.39357391e-14\n",
      " 3.06664581e-14 4.84372262e-14 1.13736676e-14 2.40958755e-14]\n",
      "[0.13369527 0.28934084 0.06364272 0.14657964 0.05241541 0.16547273\n",
      " 0.10977549 0.1338099  0.11754718 0.05517656 0.07816957 0.04930391\n",
      " 0.18786273 0.06525868 0.13764442 0.30964159 0.09038829 0.33782047\n",
      " 0.11059511 0.08322451 0.13380874 0.10326077 0.33924848 0.22497043\n",
      " 0.5533674  1.00189035 0.05276126 0.08678649 0.07293108 0.04082066\n",
      " 0.06320597 0.12893857 0.04271443 0.01778911 0.06245709 0.03485864\n",
      " 0.05024919 0.02828526 0.02636479 0.03420194 0.06302123 0.06916427\n",
      " 0.07287642 0.01393574 0.03066646 0.04843723 0.01137367 0.02409588]\n"
     ]
    }
   ],
   "source": [
    "#For predicting Te/Tg: keep top block, comment bottom block. For predicting ne/ng, keep bottom block, comment top block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TOP BLOCK ####\n",
    "\n",
    "# df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "# T_e = np.array(df_out.iloc[:,3])\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "# T_e_no_dim = T_e/T_g\n",
    "# print(T_e)\n",
    "# output = rescale_vec(T_e_no_dim)\n",
    "# print(output)\n",
    "# D_out = np.array(\n",
    "#     [\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### BOTTOM BLOCK ####\n",
    "\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_g = np.array(df_out.iloc[:,4])\n",
    "n_e_no_dim = n_e/n_g\n",
    "print(n_e)\n",
    "print(n_e_no_dim)\n",
    "output = rescale_vec(n_e_no_dim)\n",
    "print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "        [0.]\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.754325Z",
     "start_time": "2023-05-31T02:42:27.738571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.843064Z",
     "start_time": "2023-05-31T02:42:27.750866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2000e-01, 1.1779e-01, 2.9058e+00, 5.1278e-01, 1.7338e+00, 1.6655e-01,\n",
      "         3.5908e-01, 7.4896e+00, 2.5639e-01, 5.7977e+01, 1.0957e+00, 2.8979e+00,\n",
      "         2.0300e-01],\n",
      "        [1.7000e-01, 5.7396e-02, 2.9058e+00, 5.1278e-01, 1.7235e+00, 1.6650e-01,\n",
      "         3.5908e-01, 9.1353e+00, 2.5639e-01, 7.0717e+01, 1.0630e+00, 4.1955e+00,\n",
      "         2.0300e-01],\n",
      "        [1.7000e-01, 2.2831e-01, 2.9058e+00, 5.1278e-01, 1.9204e+00, 4.9983e-01,\n",
      "         3.5908e-01, 2.6878e+00, 2.5639e-01, 2.0806e+01, 1.1283e+00, 1.9927e+00,\n",
      "         2.0300e-01],\n",
      "        [1.3500e-01, 1.0229e-01, 2.9058e+00, 5.1278e-01, 1.9837e+00, 1.2486e-01,\n",
      "         3.5908e-01, 9.8228e+00, 2.5639e-01, 7.6038e+01, 1.0891e+00, 2.9368e+00,\n",
      "         2.0300e-01],\n",
      "        [1.3000e-01, 3.4774e-01, 2.9058e+00, 5.1278e-01, 2.0091e+00, 4.9987e-01,\n",
      "         3.5908e-01, 2.6081e+00, 2.5639e-01, 2.0190e+01, 1.1504e+00, 2.0709e+00,\n",
      "         2.0300e-01],\n",
      "        [1.4000e-01, 1.5548e-01, 2.9058e+00, 5.1278e-01, 1.7514e+00, 4.9986e-01,\n",
      "         3.5908e-01, 2.7788e+00, 2.5639e-01, 2.1511e+01, 1.1090e+00, 4.1845e+00,\n",
      "         2.0300e-01],\n",
      "        [1.1500e-01, 2.9459e-01, 2.9058e+00, 5.1278e-01, 1.7130e+00, 9.9989e-01,\n",
      "         3.5908e-01, 9.3035e-01, 2.5639e-01, 7.2019e+00, 1.1415e+00, 3.9558e+00,\n",
      "         2.0300e-01],\n",
      "        [5.5000e-02, 2.5787e-01, 2.9058e+00, 5.1278e-01, 1.9298e+00, 9.9945e-02,\n",
      "         3.5908e-01, 1.2894e+01, 2.5639e-01, 9.9813e+01, 1.1346e+00, 4.4791e+00,\n",
      "         2.0300e-01],\n",
      "        [6.5000e-02, 2.2661e-01, 2.9058e+00, 5.1278e-01, 2.0007e+00, 9.9935e-02,\n",
      "         3.5908e-01, 1.1344e+01, 2.5639e-01, 8.7817e+01, 1.1279e+00, 3.6638e+00,\n",
      "         2.0300e-01],\n",
      "        [1.1500e-01, 2.1373e-01, 2.9058e+00, 5.1278e-01, 2.1741e+00, 9.9885e-02,\n",
      "         3.5908e-01, 8.8604e+00, 2.5639e-01, 6.8589e+01, 1.1249e+00, 1.6658e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 3.3315e-01, 2.9058e+00, 5.1278e-01, 1.7866e+00, 6.6655e-01,\n",
      "         3.5908e-01, 2.4274e+00, 2.5639e-01, 1.8791e+01, 1.1481e+00, 3.0156e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2500e-01, 4.6693e-01, 2.9058e+00, 5.1278e-01, 1.8877e+00, 9.9988e-01,\n",
      "         3.5908e-01, 1.3173e+00, 2.5639e-01, 1.0197e+01, 1.1666e+00, 2.2913e+00,\n",
      "         2.0300e-01],\n",
      "        [2.3000e-01, 5.6796e-02, 2.9058e+00, 5.1278e-01, 1.6725e+00, 3.3310e-01,\n",
      "         3.5908e-01, 2.7001e+00, 2.5639e-01, 2.0901e+01, 1.0626e+00, 2.7090e+00,\n",
      "         2.0300e-01],\n",
      "        [1.0500e-01, 3.2896e-01, 2.9058e+00, 5.1278e-01, 2.0558e+00, 3.3323e-01,\n",
      "         3.5908e-01, 3.8244e+00, 2.5639e-01, 2.9605e+01, 1.1474e+00, 2.5003e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.4503e-01, 2.9058e+00, 5.1278e-01, 1.9122e+00, 1.9988e-01,\n",
      "         3.5908e-01, 6.9546e+00, 2.5639e-01, 5.3836e+01, 1.1057e+00, 3.3494e+00,\n",
      "         2.0300e-01],\n",
      "        [1.4000e-01, 7.5334e-02, 2.9058e+00, 5.1278e-01, 1.6756e+00, 1.9986e-01,\n",
      "         3.5908e-01, 8.5396e+00, 2.5639e-01, 6.6105e+01, 1.0751e+00, 5.2269e+00,\n",
      "         2.0300e-01],\n",
      "        [1.7500e-01, 3.2723e-01, 2.9058e+00, 5.1278e-01, 1.8776e+00, 1.9998e+00,\n",
      "         3.5908e-01, 9.4938e-02, 2.5639e-01, 7.3492e-01, 1.1471e+00, 3.4522e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.2340e-01, 2.9058e+00, 5.1278e-01, 1.6982e+00, 4.9988e-01,\n",
      "         3.5908e-01, 1.5824e+00, 2.5639e-01, 1.2249e+01, 1.0979e+00, 7.5063e+00,\n",
      "         2.0300e-01],\n",
      "        [5.5000e-02, 3.1350e-01, 2.9058e+00, 5.1278e-01, 1.7819e+00, 1.2495e-01,\n",
      "         3.5908e-01, 1.1368e+01, 2.5639e-01, 8.7997e+01, 1.1448e+00, 4.1245e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2500e-01, 1.4392e-01, 2.9058e+00, 5.1278e-01, 1.9237e+00, 9.9875e-02,\n",
      "         3.5908e-01, 1.0699e+01, 2.5639e-01, 8.2822e+01, 1.1053e+00, 2.0172e+00,\n",
      "         2.0300e-01],\n",
      "        [1.1000e-01, 1.2194e-01, 2.9058e+00, 5.1278e-01, 1.8430e+00, 9.9890e-02,\n",
      "         3.5908e-01, 1.2691e+01, 2.5639e-01, 9.8241e+01, 1.0973e+00, 2.9567e+00,\n",
      "         2.0300e-01],\n",
      "        [1.6500e-01, 2.3733e-01, 2.9058e+00, 5.1278e-01, 1.6969e+00, 9.9983e-01,\n",
      "         3.5908e-01, 1.4326e+00, 2.5639e-01, 1.1089e+01, 1.1303e+00, 3.3023e+00,\n",
      "         2.0300e-01],\n",
      "        [1.0500e-01, 7.2428e-02, 2.9058e+00, 5.1278e-01, 1.6975e+00, 9.9895e-02,\n",
      "         3.5908e-01, 1.6723e+01, 2.5639e-01, 1.2945e+02, 1.0733e+00, 5.6016e+00,\n",
      "         2.0300e-01],\n",
      "        [1.4000e-01, 9.3139e-02, 2.9058e+00, 5.1278e-01, 1.7265e+00, 1.9986e-01,\n",
      "         3.5908e-01, 8.5135e+00, 2.5639e-01, 6.5904e+01, 1.0847e+00, 4.2765e+00,\n",
      "         2.0300e-01],\n",
      "        [1.9000e-01, 3.9843e-02, 2.9058e+00, 7.6917e-01, 1.7235e+00, 2.4981e-01,\n",
      "         5.3863e-01, 4.0382e+00, 2.5639e-01, 3.1260e+01, 1.0475e+00, 6.5336e+00,\n",
      "         2.0300e-01],\n",
      "        [1.8500e-01, 3.7628e-02, 2.9058e+00, 7.6917e-01, 1.7332e+00, 2.4982e-01,\n",
      "         5.3863e-01, 3.7531e+00, 2.5639e-01, 2.9053e+01, 1.0451e+00, 1.1441e+01,\n",
      "         2.0300e-01],\n",
      "        [1.1000e-01, 1.9024e-01, 2.9058e+00, 7.6917e-01, 1.9287e+00, 9.9890e-02,\n",
      "         5.3863e-01, 1.2114e+01, 2.5639e-01, 9.3772e+01, 1.1190e+00, 1.4933e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2500e-01, 1.1689e-01, 2.9058e+00, 7.6917e-01, 2.0211e+00, 9.9875e-02,\n",
      "         5.3863e-01, 1.2236e+01, 2.5639e-01, 9.4722e+01, 1.0953e+00, 1.8736e+00,\n",
      "         2.0300e-01],\n",
      "        [1.6500e-01, 1.2605e-01, 2.9058e+00, 7.6917e-01, 2.0001e+00, 1.4269e-01,\n",
      "         5.3863e-01, 1.0374e+01, 2.5639e-01, 8.0305e+01, 1.0989e+00, 1.6421e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 3.0055e-01, 2.9058e+00, 7.6917e-01, 2.1570e+00, 2.4988e-01,\n",
      "         5.3863e-01, 5.2348e+00, 2.5639e-01, 4.0523e+01, 1.1426e+00, 1.4882e+00,\n",
      "         2.0300e-01],\n",
      "        [1.0000e-01, 2.8841e-01, 2.9058e+00, 7.6917e-01, 2.0899e+00, 2.4990e-01,\n",
      "         5.3863e-01, 6.2977e+00, 2.5639e-01, 4.8751e+01, 1.1404e+00, 2.2520e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.3698e-01, 2.9058e+00, 7.6917e-01, 1.7960e+00, 2.4988e-01,\n",
      "         5.3863e-01, 6.9588e+00, 2.5639e-01, 5.3868e+01, 1.1029e+00, 3.0397e+00,\n",
      "         2.0300e-01],\n",
      "        [1.8500e-01, 3.3476e-01, 2.9058e+00, 7.6917e-01, 1.9368e+00, 9.9982e-01,\n",
      "         5.3863e-01, 1.8216e+00, 2.5639e-01, 1.4101e+01, 1.1483e+00, 1.6527e+00,\n",
      "         2.0300e-01],\n",
      "        [1.7000e-01, 2.3184e-01, 2.9058e+00, 1.2820e+00, 1.9303e+00, 9.9830e-02,\n",
      "         8.9771e-01, 1.3700e+01, 2.5639e-01, 1.0606e+02, 1.1291e+00, 5.6195e-01,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.1529e-01, 2.9058e+00, 1.2820e+00, 1.8788e+00, 9.9880e-02,\n",
      "         8.9771e-01, 1.5009e+01, 2.5639e-01, 1.1619e+02, 1.0947e+00, 1.3382e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.8537e-01, 2.9058e+00, 1.2820e+00, 1.9970e+00, 9.9880e-02,\n",
      "         8.9771e-01, 1.3743e+01, 2.5639e-01, 1.0639e+02, 1.1177e+00, 9.7266e-01,\n",
      "         2.0300e-01],\n",
      "        [1.2500e-01, 1.6710e-01, 2.9058e+00, 1.2820e+00, 2.1526e+00, 1.6654e-01,\n",
      "         8.9771e-01, 8.9478e+00, 2.5639e-01, 6.9265e+01, 1.1126e+00, 1.3235e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 2.8666e-01, 2.9058e+00, 1.2820e+00, 2.1605e+00, 1.9988e-01,\n",
      "         8.9771e-01, 7.5128e+00, 2.5639e-01, 5.8157e+01, 1.1401e+00, 1.0047e+00,\n",
      "         2.0300e-01],\n",
      "        [2.3000e-01, 1.2264e-01, 2.9058e+00, 1.2820e+00, 1.9821e+00, 9.9770e-02,\n",
      "         8.9771e-01, 1.3642e+01, 2.5639e-01, 1.0560e+02, 1.0976e+00, 5.8479e-01,\n",
      "         2.0300e-01],\n",
      "        [1.1500e-01, 2.3218e-01, 2.9058e+00, 1.2820e+00, 2.1570e+00, 1.6655e-01,\n",
      "         8.9771e-01, 8.3343e+00, 2.5639e-01, 6.4516e+01, 1.1291e+00, 1.0811e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.2303e-01, 2.9058e+00, 1.2820e+00, 1.9077e+00, 9.9880e-02,\n",
      "         8.9771e-01, 1.6110e+01, 2.5639e-01, 1.2471e+02, 1.0978e+00, 1.4000e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 1.1256e-01, 2.9058e+00, 1.2820e+00, 1.8703e+00, 9.9880e-02,\n",
      "         8.9771e-01, 1.6222e+01, 2.5639e-01, 1.2557e+02, 1.0936e+00, 1.4622e+00,\n",
      "         2.0300e-01],\n",
      "        [1.2500e-01, 1.3964e-01, 2.9058e+00, 1.2820e+00, 2.0727e+00, 1.6654e-01,\n",
      "         8.9771e-01, 1.0687e+01, 2.5639e-01, 8.2730e+01, 1.1038e+00, 1.7371e+00,\n",
      "         2.0300e-01],\n",
      "        [1.8000e-01, 4.7889e-01, 2.9058e+00, 1.2820e+00, 2.6406e+00, 4.9982e-01,\n",
      "         8.9771e-01, 2.8898e+00, 2.5639e-01, 2.2370e+01, 1.1680e+00, 6.5698e-01,\n",
      "         2.0300e-01],\n",
      "        [1.3000e-01, 3.1671e-01, 2.9058e+00, 1.2820e+00, 2.3950e+00, 3.3320e-01,\n",
      "         8.9771e-01, 4.6370e+00, 2.5639e-01, 3.5895e+01, 1.1454e+00, 1.1509e+00,\n",
      "         2.0300e-01],\n",
      "        [1.7500e-01, 1.0012e-01, 2.9058e+00, 1.2820e+00, 1.9072e+00, 9.9825e-02,\n",
      "         8.9771e-01, 1.5194e+01, 2.5639e-01, 1.1762e+02, 1.0881e+00, 9.5942e-01,\n",
      "         2.0300e-01],\n",
      "        [1.2000e-01, 4.5214e-01, 2.9058e+00, 1.2820e+00, 2.3202e+00, 9.9880e-02,\n",
      "         8.9771e-01, 1.0167e+01, 2.5639e-01, 7.8706e+01, 1.1648e+00, 5.1952e-01,\n",
      "         2.0300e-01],\n",
      "        [1.7000e-01, 1.7059e-01, 2.9058e+00, 1.2820e+00, 2.5212e+00, 9.9830e-02,\n",
      "         8.9771e-01, 9.8523e+00, 2.5639e-01, 7.6267e+01, 1.1136e+00, 6.4210e-01,\n",
      "         2.0300e-01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "fff.read_data(inputs, output)\n",
    "print(fff.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.883791Z",
     "start_time": "2023-05-31T02:42:27.758677Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim import PiLinearRegressionViaTorch\n",
    "import pandas as pd\n",
    "\n",
    "#the code does five fold to ensure the R2 is worth while.\n",
    "# we then want to rerun the fitting on all data instead of 80% of it. The 5fold find the right number of epochs.\n",
    "# so, we re run to find the right number of epochs, and then we must store the parameters and the R2 from the 5-fold fitting.\n",
    "#\n",
    "j =[i for i in range(1,6)]\n",
    "h = [5*i for i in range(2,25)]\n",
    "g = j + h\n",
    "\n",
    "if 'df_loop_reg' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_reg\n",
    "    if 'df_loop_reg' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "    \n",
    "if 'df_loop_ext' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_reg\n",
    "    if 'df_loop_reg' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "\n",
    "    \n",
    "# these variables will be declared below!\n",
    "\n",
    "summary_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\6_rates\"\n",
    "good_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\6_rates\\good_terms\"\n",
    "bad_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\6_rates\\bad_terms\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mg\u001b[49m:\n\u001b[0;32m      2\u001b[0m     seed \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#find which hyperparameter performed best\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# idx = np.arange(lg_set.shape[0])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#22 best so far, R2 = 0.998 for lambda = 0.001\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#14 best so far, R2 = 0.945\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in g:\n",
    "    seed = i\n",
    "    #find which hyperparameter performed best\n",
    "\n",
    "    # idx = np.arange(lg_set.shape[0])\n",
    "    # best_hp = idx[array_metric_num[:,0]==np.max(array_metric_num)]\n",
    "\n",
    "    # if len(best_hp.shape)>1:\n",
    "    #     print('the following are equivalent ',best_hp)\n",
    "    #     print(best_hp.shape)\n",
    "    #     best_hp = best_hp[0,1]\n",
    "\n",
    "\n",
    "    #22 best so far, R2 = 0.998 for lambda = 0.001\n",
    "    #14 best so far, R2 = 0.945\n",
    "    ndimensionless = 1\n",
    "    lambda_gamma = 0.003 #lambda_gamma = 0.01, #replaced\n",
    "\n",
    "    poly_order = 1 #was 2? but linear is a first order poly.\n",
    "\n",
    "    poly_mapping = np.array([[0],\n",
    "                            [1]])\n",
    "\n",
    "    #poly_mapping = np.array([[0, 0],\n",
    "    #                         [1, 0],\n",
    "    #                         [0, 1],\n",
    "    #                         [2, 0],\n",
    "    #                         [1, 1],\n",
    "    #                         [0, 2]])\n",
    "    lambda_beta = 0.01 #maybe cut in half idk\n",
    "    w_array = np.array(fff._basis_col)  #this is the w array, that is, the columns in Null(D)\n",
    "    gamma_name = ['y'+str(id) for id in range(0,fff.basis_col.shape[1]) ]\n",
    "    beta_name = ['b'+str(id) for id in range(0,poly_mapping.shape[0]) ]\n",
    "    poly_name = ['dim'+str(id+1) for id in range(0,ndimensionless)]\n",
    "\n",
    "\n",
    "\n",
    "    metric = 'r2'\n",
    "    para_threshold = 0.1\n",
    "    beta_threshold = 0.005\n",
    "    training_epochs =10000\n",
    "    score = []\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #create test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.15, random_state=42)\n",
    "\n",
    "    # create validation and training set  ##unused!\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    r2_reg = r2\n",
    "    print(r2)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### BELOW MUST BE CORRECTED#####\n",
    "    if ndimensionless >=2:\n",
    "        \n",
    "        best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "        df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "    \n",
    "    \n",
    "    # store the data in data_frames\n",
    "    df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    \n",
    "\n",
    "    \n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "    \n",
    "    \n",
    "\n",
    "    # delete old variables\n",
    "\n",
    "    variable_names = ['X_test', 'Y_test', 'X_train', 'X_val', 'y_train', 'y_val', 'X_train_val', 'Y_train_val']\n",
    "    for var_name in variable_names:\n",
    "        if var_name in locals() or var_name in globals():\n",
    "            target_dict = locals() if var_name in locals() else globals()\n",
    "            del target_dict[var_name]\n",
    "            #print(f\"Deleted variable: {var_name}\")\n",
    "    X_test, Y_test, X_train_val, y_train_val = top_split_y(fff.X,fff.y,15)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "\n",
    "    x_test = X_test\n",
    "    y_test = Y_test\n",
    "    print(y_test)\n",
    "    print(y_train_val)\n",
    "    training_epochs =10000\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    r2_ext = r2\n",
    "    print(r2)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1_ext = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### BELOW MUST BE CORRECTED#####\n",
    "    if ndimensionless >=2:\n",
    "        \n",
    "        best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "        df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "\n",
    "    ori_pis[0]\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # put save_path decision here\n",
    "        #set save path--remove this chunk of code to the bottom?\n",
    "\n",
    "    #current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\"\n",
    "    if r2_reg >0.3 or r2_ext >0.3:\n",
    "        current_path = good_path\n",
    "    else:\n",
    "        current_path = bad_path\n",
    "    #new_folder =r'mb_1dim_v1_pt0001_pt_01_order1_golden_child'\n",
    "    new_folder = r'clamp'+str(para_threshold)+r'_' + str(ndimensionless)+r'dim_order'+str(poly_order)+r'_lambda'+str(lambda_gamma)+r\"_seed\"+str(seed)\n",
    "    new_path = current_path+'\\\\'+new_folder\n",
    "    if os.path.exists(new_path):\n",
    "        print(f\"File '{new_path}' already exists.\")\n",
    "    else:\n",
    "        os.mkdir(new_path)       \n",
    "        print(f\"File '{new_path}' did not exist.\")\n",
    "\n",
    "    file_path = new_path+'\\\\'+new_folder +'_seed'+str(seed)+ '.xlsx'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "    # File path of the Excel file\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4.to_excel(writer, sheet_name='beta')\n",
    "        df5.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary and performance of this 'reg dim #'\n",
    "    if 'df_loop_reg' in globals():\n",
    "        df_loop_reg.append(df_best_dim1, ignore_index = True)\n",
    "    else:\n",
    "        df_loop_reg = df_best_dim1.copy(deep=True)\n",
    "    \n",
    "            \n",
    "    # Now store the data from the extrapolation attempt. \n",
    "    # select file path to save xlsx sheet\n",
    "    # and overwrite the dataframes (df's) \n",
    "    # then store these new data frames in a separate excel file\n",
    "    \n",
    "    \n",
    "    # File path of the Excel file\n",
    "    file_path = new_path+'\\\\'+new_folder + '_extrapolation' +'_seed'+str(seed)+ '.xlsx'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "    df0_ext = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1_ext = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4_ext =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5_ext = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0_ext.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1_ext.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4_ext.to_excel(writer, sheet_name='beta')\n",
    "        df5_ext.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1_ext.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary of the 'ext dim #' for placement in an excel so I can compare them all!\n",
    "    if 'df_loop_ext' in globals():\n",
    "        df_loop_ext.append(df_best_dim1_ext, ignore_index = True)\n",
    "    else:\n",
    "        df_loop_ext = df_best_dim1_ext.copy(deep=True)\n",
    "    \n",
    "        \n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "#write df loops to good path!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Define the sample rate and duration of each tone\n",
    "sample_rate = 44100  # Adjust as needed\n",
    "duration = 0.5  # Adjust as needed\n",
    "\n",
    "# Define notes and their corresponding frequencies\n",
    "notes = {\n",
    "    'C4': 261.63,\n",
    "    'D4': 293.66,\n",
    "    'E4': 329.63,\n",
    "    'F4': 349.23,\n",
    "    'G4': 392.00,\n",
    "    'A4': 440.00,\n",
    "    'B4': 493.88,\n",
    "    'C5': 523.25\n",
    "}\n",
    "\n",
    "# Define the melody\n",
    "melody = ['E4', 'D4', 'C4', 'D4', 'E4', 'E4', 'E4', 'D4', 'D4', 'D4', 'E4', 'G4', 'G4', 'E4', 'D4', 'C4', 'D4', 'E4', 'E4', 'E4', 'E4', 'D4', 'D4', 'E4', 'D4', 'C4']\n",
    "\n",
    "# Play the melody\n",
    "for note in melody:\n",
    "    frequency = notes[note]\n",
    "    waveform = pygame.sndarray.make_sound(pygame.sndarray.array([[int(4096 * 0.5 * np.sin(2.0 * np.pi * frequency * x / sample_rate)) for _ in range(2)] for x in range(int(sample_rate * duration))]))\n",
    "    waveform.play()\n",
    "    time.sleep(duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Define notes and their corresponding frequencies\n",
    "notes = {\n",
    "    'D4': 293.66,\n",
    "    'A4': 440.00,\n",
    "    'B4': 493.88,\n",
    "    'F#4': 369.99,\n",
    "    'G4': 392.00,\n",
    "    'E4': 329.63\n",
    "}\n",
    "\n",
    "# Define the melody for the first few bars of Pachelbel's Canon\n",
    "melody = ['D4', 'A4', 'B4', 'F#4', 'G4', 'D4', 'G4', 'A4', 'D4', 'B4', 'G4', 'D4', 'F#4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'F#4', 'E4', 'A4', 'D4', 'D4', 'B4', 'G4', 'D4', 'A4', 'A4', 'B4', 'F#4', 'G4', 'D4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'F#4', 'E4']\n",
    "\n",
    "# Play the melody\n",
    "for note in melody:\n",
    "    frequency = notes[note]\n",
    "    waveform = pygame.sndarray.make_sound(pygame.sndarray.array([[int(4096 * 0.5 * np.sin(2.0 * np.pi * frequency * x / sample_rate)) for _ in range(2)] for x in range(int(sample_rate * duration))]))\n",
    "    waveform.play()\n",
    "    time.sleep(0.3)  # Adjust the duration of each note as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "g =[i for i in range(0,30)]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(g[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115]\n"
     ]
    }
   ],
   "source": [
    "j =[i for i in range(1,6)]\n",
    "h = [5*i for i in range(2,24)]\n",
    "g = j + h\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this var exists\n",
      "delete success\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_loop_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot in globals\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mdf_loop_reg\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_loop_reg' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_loop_reg = pd.DataFrame(columns = [ 'clamp', 'random seed', 'r2' ] )\n",
    "if 'df_loop_reg' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_reg\n",
    "    if 'df_loop_reg' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "\n",
    "df_loop_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
