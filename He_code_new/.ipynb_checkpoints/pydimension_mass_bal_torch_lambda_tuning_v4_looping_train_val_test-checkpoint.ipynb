{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.163733Z",
     "start_time": "2023-05-31T02:42:22.636891Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim.utils import DimensionlessLearning\n",
    "from vics_fcns import top_split_y#, top_split_x\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#First, we must load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.572932Z",
     "start_time": "2023-05-31T02:42:27.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.ExcelFile(r\"mass_balance_params.xlsx\")\n",
    "#df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\collected_output_files_packing\\data_from_EB_looping_all_sizes_remove_extraneous_dims.xlsx')\n",
    "df_2 = pd.ExcelFile(r'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\data\\data_no_packing_all\\data_from_EB_looping_new_excel_new_code_no_packing_v3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.660878Z",
     "start_time": "2023-05-31T02:42:27.634288Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_x['u_B_m_s'] = df_train_x['u_B_m_s'] / 1000 if np.min(df_train_x['u_B_m_s']) > 0 else df_train_x['u_B_m_s'] / 1000 - (np.min(df_train_x['u_B_m_s']) / 1000-1e-5)\n",
    "# df_train_x['A_tot_m2'] = 1.\n",
    "# df_train_x['t_a_s'] = df_train_x['t_a_s'] / 1e-7 if np.min(df_train_x['t_a_s']) > 0 else df_train_x['t_a_s'] / 1e-7 - (np.min(df_train_x['t_a_s']) / 1e-7-1e-5)\n",
    "# df_train_x['t_b_s'] = df_train_x['t_b_s'] / 1e-4 if np.min(df_train_x['t_b_s']) > 0 else df_train_x['t_b_s'] / 1e-4 - (np.min(df_train_x['t_b_s']) / 1e-4-1e-5)\n",
    "# df_train_x['Volume_m3'] = 1.\n",
    "# df_train_x['K_iz_a_m3_s_atom'] = df_train_x['K_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_iz_a_m3_s_atom']) > 0 else df_train_x['K_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_2_iz_a_m3_s_atom'] = df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_2_iz_a_m3_s_atom']) > 0 else df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_2_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_iz_exc_a_m3_s_atom'] = df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 if np.min(df_train_x['K_iz_exc_a_m3_s_atom']) > 0 else df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 - (np.min(df_train_x['K_iz_exc_a_m3_s_atom']) / 1e-14-1e-5)\n",
    "\n",
    "# df_train_x['n_sa_atoms_m3'] = df_train_x['n_sa_atoms_m3'] / 1e10 if np.min(df_train_x['n_sa_atoms_m3']) > 0 else df_train_x['n_sa_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sa_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['n_sb_atoms_m3'] = df_train_x['n_sb_atoms_m3'] / 1e10 if np.min(df_train_x['n_sb_atoms_m3']) > 0 else df_train_x['n_sb_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sb_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['A_a_m2'] = 1.\n",
    "\n",
    "# df_train_x['A_b_m2'] = 1.\n",
    "\n",
    "# df_train_x['n_He_exc_a_atoms_m3'] = df_train_x['n_He_exc_a_atoms_m3'] / 1e17 if np.min(df_train_x['n_He_exc_a_atoms_m3']) > 0 else df_train_x['n_He_exc_a_atoms_m3'] / 1e17 - (np.min(df_train_x['n_He_exc_a_atoms_m3']) / 1e17-1e-5)\n",
    "\n",
    "def rescale(g):\n",
    "    for i in range(0,g.shape[1]):\n",
    "        if np.min(np.abs(g[:,i])) ==0:\n",
    "            n = 0\n",
    "        else:\n",
    "            n = np.mean((np.log10(np.min(np.abs(g[:,i]))), np.log10(np.max(np.abs(g[:,i])))))\n",
    "        if n<0:\n",
    "                g[:,i] = g[:,i]/10**np.ceil(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.ceil(n) - (np.min(g[:,i]) / 10**np.ceil(n)-1e-5)\n",
    "        else:\n",
    "            g[:,i] = g[:,i]/10**np.floor(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.floor(n) - (np.min(g[:,i]) / 10**np.floor(n)-1e-5)\n",
    "    return g\n",
    "\n",
    "def rescale_vec(g):\n",
    "    if np.min(np.abs(g)) ==0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = np.mean((np.log10(np.min(np.abs(g))), np.log10(np.max(np.abs(g)))))\n",
    "    if n<0:\n",
    "            g= g/10**np.ceil(n) if np.min(g) > 0 else g/10**np.ceil(n) - (np.min(g) / 10**np.ceil(n)-1e-5)\n",
    "    else:\n",
    "        g = g/10**np.floor(n) if np.min(g) > 0 else g/10**np.floor(n) - (np.min(g) / 10**np.floor(n)-1e-5)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.706539Z",
     "start_time": "2023-05-31T02:42:27.668685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#                   0             1           2                    3                   4                   5           6                7                    8                     9                        10                  11                  12                      13                          \n",
    "#df1_names = [ 't_a_seconds', 'Q_a_As', 'V_p_ta_kgm2_s3_A','T_e_a_kgm2_s3_A', 'n_He_exc_a_atoms_m3', 'u_B_a_m_s', 'v_e_a_m_s', 'K_2_iz_a_m3_s_atom','K_loss_a_m6_s_atom2', 'K_iz_exc_a_m3_s_atom', 'K_exc_a_m3_s_atom', 'K_iz_a_m3_s_atom', 'K_elastic_a_m3_s_atom', 'E_elastic_a_kgm2_s2' ]\n",
    "\n",
    "\n",
    "df_time_a = rescale(np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:]))\n",
    "df_time_a_units = np.array(pd.read_excel(df_2, sheet_name='time_a_data_units').iloc[:,1:])\n",
    "df_time_a_n = pd.read_excel(df_2, sheet_name='time_a_data_names').iloc[:,1:]\n",
    "time_a_n= [df_time_a_n.iloc[0,i] for i in range(0,df_time_a_n.shape[1])]\n",
    "#print(df_time_a_n)\n",
    "#print(df_time_a_units)\n",
    "\n",
    "\n",
    "#                   0             1                2               3                   4              5          6             7                   8                    9                       10              11                   12                   13                  \n",
    "#df2_names =  [ 't_b_seconds', 'Q_b_As', 'V_p_tb_kgm2_s3_A','T_e_kgm2_s3_A', 'n_He_exc_atoms_m3', 'u_B_m_s', 'v_e_m_s','K_2_iz_m3_s_atom','K_loss_m6_s_atom2', 'K_iz_exc_m3_s_atom', 'K_exc_m3_s_atom', 'K_iz_m3_s_atom', 'K_elastic_m3_s_atom', 'E_elastic_kgm2_s2' ]\n",
    "df_time_b = rescale(np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]))\n",
    "df_time_b_units = np.array(pd.read_excel(df_2, sheet_name='time_b_data_units').iloc[:,1:])\n",
    "df_time_b_n = pd.read_excel(df_2, sheet_name='time_b_data_names').iloc[:,1:]\n",
    "time_b_n= [df_time_b_n.iloc[0,i] for i in range(0,df_time_b_n.shape[1])]\n",
    "\n",
    "\n",
    "#                    0                   1                 2               3                 4               5               6                 7                  8           9        10        11                12                   13               14                   \n",
    "#df3_names = ['E_period_kgm2_s2', 'n_sa_atoms_m3','n_sb_atoms_m3', 'n_e_electrons_m3', 'n_g_atoms_m3', 'T_g_kelvin', 'E_iz_kgm2_s2', 'E_iz_exc_kgm2_s2', 'E_exc_kgm2_s2', 'e_c_As', 'm_e_kg', 'M_He_kg',  'epsilon_A2s4_kg_m3', 'eps_0_A2s4_kg_m3', 'k_b_kgm2_s2_K']\n",
    "df_other = rescale(np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]))\n",
    "df_other_units = np.array(pd.read_excel(df_2, sheet_name='other_data_units').iloc[:,1:])\n",
    "df_other_n = pd.read_excel(df_2, sheet_name='other_data_names').iloc[:,1:]\n",
    "other_n= [df_other_n.iloc[0,i] for i in range(0,df_other_n.shape[1])]\n",
    "\n",
    "#                    0                 1             2        3        4         5            6            7            8               9              10                 11                      12                      \n",
    "#df4_names = ['Volume_rxtor_m2', 'V_all_beads_m2','A_a_m2','A_b_m2', 'h_m', 'Volume_m3', 'A_bead_m2', 'A_tot_m3', 'frequency_Hz', 'Flow_m3_s', 'temp_C_gas_K', 'Set_Voltage_kgm2_s3_A', 'pulse_time_seconds' ]\n",
    "df_exp = rescale(np.array(pd.read_excel(df_2, sheet_name='Experiment_Design_data').iloc[:,1:]))\n",
    "df_exp_units = np.array(pd.read_excel(df_2, sheet_name='Experiment_Data_units').iloc[:,1:])\n",
    "df_exp_n = pd.read_excel(df_2, sheet_name='Experiment_Data_names').iloc[:,1:]\n",
    "exp_n= [df_exp_n.iloc[0,i] for i in range(0,df_exp_n.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These next cells are about assembling process variables for dimmensionless numbers. Uncomment the cell you wish to run.\n",
    "\n",
    "# You must prepare inputs (which contain the rescaled data), D_in (the dimensions matrix), and for ease of trackign I recommend your produce variable names (variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all terms\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack([df_time_a,df_time_b, df_other, df_exp])\n",
    "# D_in = np.hstack([df_time_a_units, df_time_b_units, df_other_units, df_exp_units])\n",
    "# variables = time_a_n + time_b_n + other_n + exp_n\n",
    "# print(D_in.shape)\n",
    "# print(len(variables))\n",
    "# print(variables[31]) #delete #31 for the MB.\n",
    "# inputs = np.delete(inputs, 31, axis = 1)\n",
    "# D_in = np.delete(D_in, 31, axis = 1)\n",
    "# variables.pop(31)\n",
    "# print(variables[31]) #good check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_g_atoms_m3', 'n_sa_atoms_m3', 'n_sb_atoms_m3']\n",
      "(6, 3)\n",
      "(48, 3)\n",
      "['u_B_a_m_s', 'K_iz_a_m3_s_atom', 'K_iz_exc_a_m3_s_atom', 'n_He_exc_a_atoms_m3', 'K_2_iz_a_m3_s_atom', 't_a_seconds', 'u_B_m_s', 'K_iz_m3_s_atom', 'n_He_exc_atoms_m3', 'K_iz_exc_m3_s_atom', 't_b_seconds', 'K_2_iz_m3_s_atom', 'n_g_atoms_m3', 'n_sa_atoms_m3', 'n_sb_atoms_m3', 'A_tot_m3', 'Volume_m3', 'A_a_m2']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# all MB terms\n",
    "# time a:\n",
    "a = df_time_b[:,5].shape[0]\n",
    "# u_B_a [5], K_iz_a [11], K_iz_exc_a [9], n_He_exc_a [4], K_2_iz_a [7], t_a [0]\n",
    "ta_inputs = np.hstack((df_time_a[:,5].reshape(a,1), df_time_a[:,11].reshape(a,1), df_time_a[:,9].reshape(a,1), df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1), df_time_a[:,0].reshape(a,1)))\n",
    "ta_D_in = np.hstack((df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_time_a_units[:,9].reshape(6,1), df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1), df_time_a_units[:,0].reshape(6,1)))\n",
    "ta_n = [time_a_n[5], time_a_n[11], time_a_n[9], time_a_n[4], time_a_n[7], time_a_n[0] ]\n",
    "\n",
    "\n",
    "# time b\n",
    "# u_B [5], K_iz [11], n_He_exc [4], K_iz_exc [9], t_b [0], K_2_iz [7]       \n",
    "tb_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,11].reshape(a,1), df_time_b[:,4].reshape(a,1), df_time_b[:,9].reshape(a,1), df_time_b[:,0].reshape(a,1), df_time_b[:,7].reshape(a,1)))\n",
    "tb_D_in = np.hstack((df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,11].reshape(6,1), df_time_b_units[:,4].reshape(6,1), df_time_b_units[:,9].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_time_b_units[:,7].reshape(6,1)))\n",
    "tb_n = [time_b_n[5], time_b_n[11], time_b_n[4], time_b_n[9], time_b_n[0], time_b_n[7] ]\n",
    "\n",
    "\n",
    "# df other\n",
    "# ng [4] , n_sa [1], n_sb [2], \n",
    "other_inputs = np.hstack(( df_other[:,4].reshape(a,1), df_other[:,1].reshape(a,1), df_other[:,2].reshape(a,1) ))\n",
    "other_D_in = np.hstack(( df_other_units[:,4].reshape(6,1), df_other_units[:,1].reshape(6,1), df_other_units[:,2].reshape(6,1)  ))\n",
    "other_n_in = [ other_n[4], other_n[1], other_n[2] ]\n",
    "print(other_n_in)\n",
    "# print(len(other_n_in))\n",
    "print(other_D_in.shape)\n",
    "print(other_inputs.shape)\n",
    "\n",
    "#df experimental\n",
    "# A_tot [7], Volume [5], A_a [2]\n",
    "exp_inputs = np.hstack(( df_exp[:,7].reshape(a,1), df_exp[:,5].reshape(a,1), df_exp[:,2].reshape(a,1)  ))\n",
    "exp_D_in = np.hstack(( df_exp_units[:,7].reshape(6,1), df_exp_units[:,5].reshape(6,1), df_exp_units[:,2].reshape(6,1) ))\n",
    "exp_n_in = [ exp_n[7], exp_n[5], exp_n[2] ]\n",
    "\n",
    "#all together\n",
    "inputs = np.hstack(( ta_inputs, tb_inputs, other_inputs, exp_inputs))\n",
    "D_in = np.hstack(( ta_D_in, tb_D_in, other_D_in, exp_D_in ))\n",
    "variables = ta_n+tb_n+other_n_in+exp_n_in\n",
    "print(variables)\n",
    "print(len(variables))\n",
    "# A_tot, Volume, A_a\n",
    "\n",
    "\n",
    "#try again without n_He_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates  same for packed / unpacked\n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate + u_B_ms - n_He_exc_a\n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,5].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,5].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[5], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # three major rates  same for packed / unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in ))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # four major rates, same for packed / unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates--packed\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  and ta packing loss\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "# packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "# packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "# packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # five major rates--unpacked\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  and ta loss electrode a\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta loss electrode a: n_sa[na] * u_B_a df_time_a[:,5] * A_a[na] * t_a [na]\n",
    "# ta_loss_elec_a_inputs = df_time_a[:,5].reshape(a,1)\n",
    "# ta_loss_elec_a_D_in = df_time_a_units[:,5].reshape(6,1)\n",
    "# ta_loss_elec_a_n = [ time_a_n[5] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, ta_loss_elec_a_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, ta_loss_elec_a_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+ ta_loss_elec_a_n \n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # six major rates--packed case\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  # ta packing loss, ta Vol double He exc ionization\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "# packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "# packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "# packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "# # ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "# double_He_exc_ion_ta_inputs =  np.hstack(( df_time_a[:,4].reshape(a,1) , df_time_a[:,7].reshape(a,1) ))\n",
    "# double_He_exc_ion_ta_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# double_He_exc_ion_ta_n = [time_a_n[4], time_a_n[7]]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs, double_He_exc_ion_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in, double_He_exc_ion_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n + double_He_exc_ion_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NO n_he_exc_a --- six major rates--packed case--- NO n_he_exc_a\n",
    "# # # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  # ta packing loss, ta Vol double He exc ionization\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "# elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "# elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "# elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "# #tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "# elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "# elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "# elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# # ta packing loss: n_e [na], u_B_a df_time_a[:,5], A_tot [na], ta [na]\n",
    "# packing_ta_inputs =  df_time_a[:,5].reshape(a,1)\n",
    "# packing_ta_D_in = df_time_a_units[:,5].reshape(6,1) \n",
    "# packing_ta_n = [time_a_n[5]]\n",
    "\n",
    "# # ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "# double_He_exc_ion_ta_inputs = df_time_a[:,7].reshape(a,1) \n",
    "# double_He_exc_ion_ta_D_in =  df_time_a_units[:,7].reshape(6,1) \n",
    "# double_He_exc_ion_ta_n = [time_a_n[7]]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, packing_ta_inputs, double_He_exc_ion_ta_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, packing_ta_D_in, double_He_exc_ion_ta_D_in))\n",
    "# variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+packing_ta_n + double_He_exc_ion_ta_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# # #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_a_seconds', 'K_iz_a_m3_s_atom', 'n_g_atoms_m3', 'Volume_m3', 'u_B_m_s', 't_b_seconds', 'A_tot_m3', 'n_sa_atoms_m3', 'A_a_m2', 'n_sb_atoms_m3', 'u_B_a_m_s', 'n_He_exc_a_atoms_m3', 'K_2_iz_a_m3_s_atom']\n",
      "[[ 0  3 -3  3  1  0  2 -3  2 -3  1 -3  3]\n",
      " [ 1 -1  0  0 -1  1  0  0  0  0 -1  0 -1]\n",
      " [ 0 -1  1  0  0  0  0  1  0  1  0  1 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# # six and seven major rates--unpacked. The sixth major rate, ta packing wall loss, has no new terms and\n",
    "# # hence provides no new information. \n",
    "# # include iomization ta, wall loss tb, tb loss to electrode a, tb loss electrode b\n",
    "# #  ta loss electrode a, ta packing wall loss, ta_vol_double_He_exc_ion\n",
    "\n",
    "# vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "a = df_time_b[:,5].shape[0]\n",
    "ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# tb loss to electrode a : n_sa df_other[1] ,    A_a df_exp[2]  ,   u_B   df_time_b[na], t_b [na]\n",
    "elec_a_tb_inputs = np.hstack(( df_other[:,1].reshape(a,1) , df_exp[:,2].reshape(a,1) ))\n",
    "elec_a_tb_D_in = np.hstack(( df_other_units[:,1].reshape(6,1) , df_exp_units[:,2].reshape(6,1) ))\n",
    "elec_a_tb_n = [ other_n[1], exp_n[2]]\n",
    "\n",
    "#tb loss to electrode b: n_sb df_other[2], A_a, [na], u_B [na], t_b [na]\n",
    "elec_b_tb_inputs =  df_other[:,2].reshape(a,1)\n",
    "elec_b_tb_D_in = df_other_units[:,2].reshape(6,1) \n",
    "elec_b_tb_n = [other_n[2]]\n",
    "\n",
    "# ta loss electrode a: n_sa[na] * u_B_a df_time_a[:,5] * A_a[na] * t_a [na]\n",
    "ta_loss_elec_a_inputs = df_time_a[:,5].reshape(a,1)\n",
    "ta_loss_elec_a_D_in = df_time_a_units[:,5].reshape(6,1)\n",
    "ta_loss_elec_a_n = [ time_a_n[5] ]\n",
    "\n",
    "# ta packing wall loss: n_e [na], u_B_a [na], A_tot [na], t_a [na]\n",
    "# # all redundant\n",
    "\n",
    "# # ta vol double He exc ionization: V exp[na], n_He_exc_a df_time_a[:,4]  ,  K_2_iz_a  df_time_a[:,7], ta [na]\n",
    "double_He_exc_ion_ta_inputs =  np.hstack(( df_time_a[:,4].reshape(a,1) , df_time_a[:,7].reshape(a,1) ))\n",
    "double_He_exc_ion_ta_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "double_He_exc_ion_ta_n = [time_a_n[4], time_a_n[7]]\n",
    "\n",
    "inputs = np.hstack((ion_inputs, wall_inputs, elec_a_tb_inputs, elec_b_tb_inputs, ta_loss_elec_a_inputs, double_He_exc_ion_ta_inputs))\n",
    "D_in = np.hstack(( ion_D_in, wall_D_in, elec_a_tb_D_in , elec_b_tb_D_in, ta_loss_elec_a_D_in, double_He_exc_ion_ta_D_in))\n",
    "variables = ion_n+wall_n + elec_a_tb_n + elec_b_tb_n+ ta_loss_elec_a_n +double_He_exc_ion_ta_n\n",
    "print(variables)\n",
    "print(D_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + just the variable n_He_exc_a\n",
    "# # include iomization ta, wall loss tb\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7], n_He_exc_a  ta[4]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) , df_time_a[:,4].reshape(a,1)))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1), df_time_a_units[:,4].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7], time_a_n[4] ]\n",
    "\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in ))\n",
    "# variables = ion_n+wall_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two major rates + n_He_exca_rate \n",
    "# # include iomization ta, wall loss tb, and he exc ion ta\n",
    "\n",
    "# # vol time a ionization: t_a ta[0],, K_iz_a   ta[11], n_e [na], n_g other[4], V exp[5]\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# ion_inputs = np.hstack((df_time_a[:,0].reshape(a,1), df_time_a[:,11].reshape(a,1), df_other[:,4].reshape(a,1), df_exp[:,5].reshape(a,1) ))\n",
    "# ion_D_in = np.hstack(( df_time_a_units[:,0].reshape(6,1), df_time_a_units[:,11].reshape(6,1), df_other_units[:,4].reshape(6,1), df_exp_units[:,5].reshape(6,1) ))\n",
    "# ion_n = [time_a_n[0], time_a_n[11], other_n[4], exp_n[5] ]\n",
    "\n",
    "# # vol time b packing wall loss: n_e [na], u_B tb[5], t_b  tb[0], A_tot exp[7]\n",
    "# wall_inputs = np.hstack((df_time_b[:,5].reshape(a,1), df_time_b[:,0].reshape(a,1), df_exp[:,7].reshape(a,1) ))\n",
    "# wall_D_in = np.hstack(( df_time_b_units[:,5].reshape(6,1), df_time_b_units[:,0].reshape(6,1), df_exp_units[:,7].reshape(6,1) ))\n",
    "# wall_n = [time_b_n[5], time_b_n[0], exp_n[7] ]\n",
    "\n",
    "# # ta He double ioniz : V [na], n_He_exc_a  ta[4], K_2_iz_a  ta[7], t_a [na],\n",
    "# he_inputs = np.hstack(( df_time_a[:,4].reshape(a,1), df_time_a[:,7].reshape(a,1) ))\n",
    "# he_D_in = np.hstack(( df_time_a_units[:,4].reshape(6,1), df_time_a_units[:,7].reshape(6,1) ))\n",
    "# he_n = [time_a_n[4], time_a_n[7] ]\n",
    "\n",
    "\n",
    "# inputs = np.hstack((ion_inputs, wall_inputs, he_inputs))\n",
    "# D_in = np.hstack(( ion_D_in, wall_D_in, he_D_in ))\n",
    "# variables = ion_n+wall_n+he_n\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "# #A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base terms and will add h and uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa', 'h'\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1),df_exp[:,4].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1),df_exp_units[:,4].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5],exp_n[4]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "# base terms with uB, minus nHeexca\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "# base terms and will add uB\n",
    "#base terms + uba,  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca', 'uBa'\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_time_a[:,5].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_time_a_units[:,5].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],time_a_n[5]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F +K_iz, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9], K_iz is time_b_n[11]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1),df_time_b[:,11].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1),df_time_b_units[:,11].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9], time_b_n[11]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "# base terms  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#base terms -A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],time_a_n[0],time_b_n[0],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#to compare to ketong's original code only\n",
    "#not to construct  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb', 'Aa', 'Ab', 'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_other[:,2].reshape(a,1),df_exp[:,2].reshape(a,1),df_exp[:,3].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_exp_units[:,2].reshape(6,1),df_exp_units[:,3].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],exp_n[2],exp_n[3],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 13)\n",
      "(6, 13)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(D_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73383193 1.72345143 1.92042988 1.98369526 2.00907405 1.75139783\n",
      " 1.71300804 1.92975108 2.00065023 2.17409766 1.78660233 1.8877228\n",
      " 1.67247618 2.05581503 1.91216756 1.67562882 1.87763096 1.69815509\n",
      " 1.7818696  1.92372487 1.84301989 1.69691147 1.6975334  1.726511\n",
      " 1.72345143 1.73322303 1.9286568  2.0211218  2.00012256 2.157038\n",
      " 2.08993244 1.79603038 1.93684878 1.93029798 1.87875495 1.99695363\n",
      " 2.15262938 2.16046071 1.98209829 2.157038   1.90774635 1.87030853\n",
      " 2.07268931 2.64062231 2.39495204 1.90719298 2.3201822  2.52120047]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we must produce our target variables, include their dimensions ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.707010Z",
     "start_time": "2023-05-31T02:42:27.700438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88496592e+12 8.40777158e+12 1.84935323e+12 4.25936461e+12\n",
      " 1.52310600e+12 4.80836688e+12 3.18989622e+12 3.88829674e+12\n",
      " 3.41572870e+12 1.60334052e+12 2.27147985e+12 1.43269087e+12\n",
      " 5.45898365e+12 1.89631062e+12 3.99972174e+12 8.99767792e+12\n",
      " 2.62653575e+12 9.81651028e+12 3.21371282e+12 2.41836809e+12\n",
      " 3.88826310e+12 3.00058897e+12 9.85800589e+12 6.53727255e+12\n",
      " 1.60799513e+13 2.91132946e+13 1.53315588e+12 2.52187327e+12\n",
      " 2.11925785e+12 1.18618153e+12 1.83666202e+12 3.74674399e+12\n",
      " 1.24121146e+12 5.16922311e+11 1.81490079e+12 1.01293491e+12\n",
      " 1.46015934e+12 8.21923413e+11 7.66117709e+11 9.93852477e+11\n",
      " 1.83129375e+12 2.00980064e+12 2.11766942e+12 4.04949778e+11\n",
      " 8.91117102e+11 1.40750655e+12 3.30500172e+11 7.00186721e+11]\n",
      "[1.33695274e-13 2.89340845e-13 6.36427167e-14 1.46579643e-13\n",
      " 5.24154078e-14 1.65472732e-13 1.09775493e-13 1.33809899e-13\n",
      " 1.17547179e-13 5.51765584e-14 7.81695712e-14 4.93039068e-14\n",
      " 1.87862732e-13 6.52586848e-14 1.37644423e-13 3.09641586e-13\n",
      " 9.03882873e-14 3.37820474e-13 1.10595105e-13 8.32245094e-14\n",
      " 1.33808741e-13 1.03260768e-13 3.39248483e-13 2.24970427e-13\n",
      " 5.53367398e-13 1.00189035e-12 5.27612596e-14 8.67864852e-14\n",
      " 7.29310794e-14 4.08206579e-14 6.32059680e-14 1.28938573e-13\n",
      " 4.27144303e-14 1.77891058e-14 6.24570878e-14 3.48586353e-14\n",
      " 5.02491931e-14 2.82852611e-14 2.63647916e-14 3.42019419e-14\n",
      " 6.30212271e-14 6.91642736e-14 7.28764159e-14 1.39357391e-14\n",
      " 3.06664581e-14 4.84372262e-14 1.13736676e-14 2.40958755e-14]\n",
      "[0.13369527 0.28934084 0.06364272 0.14657964 0.05241541 0.16547273\n",
      " 0.10977549 0.1338099  0.11754718 0.05517656 0.07816957 0.04930391\n",
      " 0.18786273 0.06525868 0.13764442 0.30964159 0.09038829 0.33782047\n",
      " 0.11059511 0.08322451 0.13380874 0.10326077 0.33924848 0.22497043\n",
      " 0.5533674  1.00189035 0.05276126 0.08678649 0.07293108 0.04082066\n",
      " 0.06320597 0.12893857 0.04271443 0.01778911 0.06245709 0.03485864\n",
      " 0.05024919 0.02828526 0.02636479 0.03420194 0.06302123 0.06916427\n",
      " 0.07287642 0.01393574 0.03066646 0.04843723 0.01137367 0.02409588]\n"
     ]
    }
   ],
   "source": [
    "#For predicting Te/Tg: keep top block, comment bottom block. For predicting ne/ng, keep bottom block, comment top block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TOP BLOCK ####\n",
    "\n",
    "# df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "# T_e = np.array(df_out.iloc[:,3])\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "# T_e_no_dim = T_e/T_g\n",
    "# print(T_e)\n",
    "# output = rescale_vec(T_e_no_dim)\n",
    "# print(output)\n",
    "# D_out = np.array(\n",
    "#     [\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### BOTTOM BLOCK ####\n",
    "\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_g = np.array(df_out.iloc[:,4])\n",
    "n_e_no_dim = n_e/n_g\n",
    "print(n_e)\n",
    "print(n_e_no_dim)\n",
    "output = rescale_vec(n_e_no_dim)\n",
    "print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "        [0.]\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0018903547745756e-12\n",
      "1.1373667577272438e-14\n"
     ]
    }
   ],
   "source": [
    "r = n_e/n_g\n",
    "print(max(r))\n",
    "print(min(r))\n",
    "del r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we produce the nullspace basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.754325Z",
     "start_time": "2023-05-31T02:42:27.738571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
       "          1.0000, -1.0000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000, -0.3333,  0.0000, -0.6667,  0.0000, -0.6667,  0.0000, -0.3333,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.843064Z",
     "start_time": "2023-05-31T02:42:27.750866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1140,  0.8232,  2.9058,  ...,  0.2564,  0.6266,  1.3640],\n",
      "        [ 0.1400,  0.6274,  2.9058,  ...,  0.2564,  0.5287,  1.3415],\n",
      "        [ 0.1550,  0.5893,  2.9058,  ...,  0.2564,  0.3972,  1.3365],\n",
      "        ...,\n",
      "        [ 0.1650,  0.5585,  2.9058,  ...,  0.2564,  1.2821,  1.3322],\n",
      "        [ 0.1650,  0.1736,  2.9058,  ...,  0.2564,  4.0607,  1.2470],\n",
      "        [ 0.1700,  0.0746,  2.9058,  ...,  0.2564, 10.0573,  1.1936]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "fff.read_data(inputs, output)\n",
    "print(fff.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we prepare for dimensionless number discovery. \n",
    "# In this next cell, we choose how many different initial guess to start the optimization with, as well as where to store the results.\n",
    "# summary path: where the overview of each result\n",
    "good path: where dimensionless numbers with a r^2 > 0.3 are stored.\n",
    "bad path: where dimensionless numbers with a r^2 < 0.3 are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.883791Z",
     "start_time": "2023-05-31T02:42:27.758677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in globals\n",
      "not in globals\n"
     ]
    }
   ],
   "source": [
    "from PyDBDdim import PiLinearRegressionViaTorch\n",
    "import pandas as pd\n",
    "\n",
    "#the code does five fold to ensure the R2 is worth while.\n",
    "# we then want to rerun the fitting on all data instead of 80% of it. The 5fold find the right number of epochs.\n",
    "# so, we re run to find the right number of epochs, and then we must store the parameters and the R2 from the 5-fold fitting.\n",
    "#\n",
    "small_loop = [i for i in range(1,3)]\n",
    "j =[i for i in range(1,6)]\n",
    "h = [5*i for i in range(6,25)] # rerun for unpacked, 6/7 rates\n",
    "#h = [2*i for i in range(26,150)] #when desperate\n",
    "g = j + h\n",
    "\n",
    "if 'df_loop_reg' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_reg\n",
    "    if 'df_loop_reg' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "    \n",
    "if 'df_loop_ext' in globals():\n",
    "    print('this var exists')\n",
    "    del df_loop_ext\n",
    "    if 'df_loop_ext' in globals():\n",
    "        print('delete failed')\n",
    "    else:\n",
    "        print('delete success')\n",
    "else:\n",
    "    print('not in globals')\n",
    "\n",
    "    \n",
    "# these variables will be declared below!\n",
    "\n",
    "lambda_gamma = 0.003\n",
    "\n",
    "summary_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\" + '\\\\' + r\"summary_recheck.xlsx\"\n",
    "good_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\"\n",
    "bad_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discovery occurs in the next cell.\n",
    "Parameters:\n",
    "lambda_gamma (prev cell): penalty on the size of the dimensionless number\n",
    "lambda_beta: penalty on the number of basis functions used in the polynomial regression\n",
    "\n",
    "n_dimensionless: # of dimensionless numbers to discover.\n",
    "poly_order: polynomial order. Linear is order 1.\n",
    "poly_mapping: for the dimensionless number, what power each one has in order. See below.\n",
    "\n",
    "para_threshold: threshold for gamma. Below this threshold, the gamma is treated as if it were zero.\n",
    "beta_threshold: threshold for beta. Belwo this threshold, the beta is treated as if it were zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.314030549771643 at iteration 9999, val_loss: -0.05525771521283929, best_val_loss: -2.8140152741329416e-07: 100%|██████████| 10000/10000 [00:45<00:00, 221.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.8140152741329416e-07\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[-0. -0. -0. -0. -0.  0. -0.  0.]\n",
      "[-0. -0. -0. -0. -0.  0. -0.  0.]\n",
      "[Parameter containing:\n",
      "tensor([[-0.0008],\n",
      "        [-0.0015],\n",
      "        [-0.0006],\n",
      "        [-0.0062],\n",
      "        [-0.0018],\n",
      "        [ 0.0008],\n",
      "        [-0.0009],\n",
      "        [ 0.0009]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3604e+00, -7.2856e-04]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[-0. -0. -0. -0. -0.  0. -0.  0.]\n",
      "[-0. -0. -0. -0. -0.  0. -0.  0.]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3346913903787605 at iteration 9999, val_loss: 0.3657595144261657, best_val_loss: 0.3814933301820638: 100%|██████████| 10000/10000 [00:47<00:00, 211.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3814933301820638\n",
      "-65.16314084735181\n",
      "tensor([[-0.4936],\n",
      "        [ 0.0353],\n",
      "        [ 0.0000],\n",
      "        [ 0.5377],\n",
      "        [-0.1530],\n",
      "        [ 0.4773],\n",
      "        [-0.3670],\n",
      "        [ 0.1820]], dtype=torch.float64)\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "[Parameter containing:\n",
      "tensor([[-0.4936],\n",
      "        [ 0.0353],\n",
      "        [ 0.0024],\n",
      "        [ 0.5377],\n",
      "        [-0.1530],\n",
      "        [ 0.4773],\n",
      "        [-0.3670],\n",
      "        [ 0.1820]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.2480, 0.4382]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.4936],\n",
      "        [ 0.0353],\n",
      "        [ 0.0000],\n",
      "        [ 0.5377],\n",
      "        [-0.1530],\n",
      "        [ 0.4773],\n",
      "        [-0.3670],\n",
      "        [ 0.1820]], dtype=torch.float64)\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "tensor([[-0.4936],\n",
      "        [ 0.0353],\n",
      "        [ 0.0000],\n",
      "        [ 0.5377],\n",
      "        [-0.1530],\n",
      "        [ 0.4773],\n",
      "        [-0.3670],\n",
      "        [ 0.1820]], dtype=torch.float64)\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed1' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed1\\_seed1.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed1\\clamp0.01_1dim_order1_lambda0.003_seed1_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed1\\_extrapolation_seed1.xlsx' does not exist.\n",
      "tensor([[-0.4936],\n",
      "        [ 0.0353],\n",
      "        [ 0.0000],\n",
      "        [ 0.5377],\n",
      "        [-0.1530],\n",
      "        [ 0.4773],\n",
      "        [-0.3670],\n",
      "        [ 0.1820]], dtype=torch.float64)\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n",
      "[-0.49362616  0.03526169  0.          0.53767748 -0.15304898  0.4773034\n",
      " -0.36704109  0.18203403]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.560924857497425 at iteration 9999, val_loss: 0.5907277313066389, best_val_loss: 0.5972401046808544: 100%|██████████| 10000/10000 [00:52<00:00, 191.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5972401046808544\n",
      "tensor([[-0.5434],\n",
      "        [-0.0000],\n",
      "        [-0.1069],\n",
      "        [ 0.7648],\n",
      "        [-0.8027],\n",
      "        [ 1.3194],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.54341953 -0.         -0.10688602  0.76475777 -0.80272354  1.31937184\n",
      "  0.         -0.        ]\n",
      "[-0.54341953 -0.         -0.10688602  0.76475777 -0.80272354  1.31937184\n",
      "  0.         -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-5.4342e-01],\n",
      "        [-2.5009e-05],\n",
      "        [-1.0689e-01],\n",
      "        [ 7.6476e-01],\n",
      "        [-8.0272e-01],\n",
      "        [ 1.3194e+00],\n",
      "        [ 1.9069e-03],\n",
      "        [-1.1395e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[2.6726e-05, 5.1903e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.5434],\n",
      "        [-0.0000],\n",
      "        [-0.1069],\n",
      "        [ 0.7648],\n",
      "        [-0.8027],\n",
      "        [ 1.3194],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.54341953 -0.         -0.10688602  0.76475777 -0.80272354  1.31937184\n",
      "  0.         -0.        ]\n",
      "[-0.54341953 -0.         -0.10688602  0.76475777 -0.80272354  1.31937184\n",
      "  0.         -0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365662116003592 at iteration 9999, val_loss: 0.36957633136502677, best_val_loss: 0.37875931485779657: 100%|██████████| 10000/10000 [00:45<00:00, 218.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37875931485779657\n",
      "-56.678140367108206\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6555],\n",
      "        [-0.0000],\n",
      "        [ 1.3918],\n",
      "        [-0.5186],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1511e-01],\n",
      "        [ 4.9603e-05],\n",
      "        [-6.4761e-03],\n",
      "        [ 6.5555e-01],\n",
      "        [-5.4232e-05],\n",
      "        [ 1.3918e+00],\n",
      "        [-5.1856e-01],\n",
      "        [ 6.1807e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-3.0549e-05,  5.4076e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6555],\n",
      "        [-0.0000],\n",
      "        [ 1.3918],\n",
      "        [-0.5186],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6555],\n",
      "        [-0.0000],\n",
      "        [ 1.3918],\n",
      "        [-0.5186],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed2' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed2\\clamp0.01_1dim_order1_lambda0.003_seed2dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed2\\_seed2.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed2\\clamp0.01_1dim_order1_lambda0.003_seed2_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed2\\_extrapolation_seed2.xlsx' does not exist.\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6555],\n",
      "        [-0.0000],\n",
      "        [ 1.3918],\n",
      "        [-0.5186],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n",
      "[-0.61511108  0.         -0.          0.65554975 -0.          1.39184362\n",
      " -0.5185588   0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3910048629069682 at iteration 9999, val_loss: 0.602549946275815, best_val_loss: 0.6089950591718123: 100%|██████████| 10000/10000 [00:40<00:00, 247.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6089950591718123\n",
      "tensor([[-0.7904],\n",
      "        [ 1.7247],\n",
      "        [ 0.3468],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0035],\n",
      "        [-0.3903],\n",
      "        [-1.2726]], dtype=torch.float64)\n",
      "[-0.79037871  1.72467683  0.34678064  0.         -0.          1.00353986\n",
      " -0.39026888 -1.27262668]\n",
      "[-0.79037871  1.72467683  0.34678064  0.         -0.          1.00353986\n",
      " -0.39026888 -1.27262668]\n",
      "[Parameter containing:\n",
      "tensor([[-7.9038e-01],\n",
      "        [ 1.7247e+00],\n",
      "        [ 3.4678e-01],\n",
      "        [ 4.3598e-03],\n",
      "        [-5.5841e-05],\n",
      "        [ 1.0035e+00],\n",
      "        [-3.9027e-01],\n",
      "        [-1.2726e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2935,  0.1263]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.7904],\n",
      "        [ 1.7247],\n",
      "        [ 0.3468],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0035],\n",
      "        [-0.3903],\n",
      "        [-1.2726]], dtype=torch.float64)\n",
      "[-0.79037871  1.72467683  0.34678064  0.         -0.          1.00353986\n",
      " -0.39026888 -1.27262668]\n",
      "[-0.79037871  1.72467683  0.34678064  0.         -0.          1.00353986\n",
      " -0.39026888 -1.27262668]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365768124724043 at iteration 9999, val_loss: 0.3695877619417738, best_val_loss: 0.37927021397712224: 100%|██████████| 10000/10000 [00:42<00:00, 237.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37927021397712224\n",
      "-56.96828731762439\n",
      "tensor([[-0.6151],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6571],\n",
      "        [-0.0000],\n",
      "        [ 1.3931],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-0.6151],\n",
      "        [-0.0041],\n",
      "        [-0.0073],\n",
      "        [ 0.6571],\n",
      "        [-0.0075],\n",
      "        [ 1.3931],\n",
      "        [-0.5183],\n",
      "        [-0.0027]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.4510e-04, 5.4170e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6151],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6571],\n",
      "        [-0.0000],\n",
      "        [ 1.3931],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "tensor([[-0.6151],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6571],\n",
      "        [-0.0000],\n",
      "        [ 1.3931],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed3' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed3\\clamp0.01_1dim_order1_lambda0.003_seed3dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed3\\_seed3.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed3\\clamp0.01_1dim_order1_lambda0.003_seed3_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed3\\_extrapolation_seed3.xlsx' does not exist.\n",
      "tensor([[-0.6151],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6571],\n",
      "        [-0.0000],\n",
      "        [ 1.3931],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n",
      "[-0.61505608 -0.         -0.          0.6570614  -0.          1.39313408\n",
      " -0.5183152  -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.42067856880043053 at iteration 9999, val_loss: 0.6106552670155758, best_val_loss: 0.6169166202611434: 100%|██████████| 10000/10000 [00:40<00:00, 246.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6169166202611434\n",
      "tensor([[-0.6130],\n",
      "        [ 1.6953],\n",
      "        [ 0.0000],\n",
      "        [ 0.7855],\n",
      "        [-0.1845],\n",
      "        [ 1.5369],\n",
      "        [-0.6424],\n",
      "        [-1.5633]], dtype=torch.float64)\n",
      "[-0.61300241  1.69532267  0.          0.78548959 -0.18446095  1.53687465\n",
      " -0.64239958 -1.56328477]\n",
      "[-0.61300241  1.69532267  0.          0.78548959 -0.18446095  1.53687465\n",
      " -0.64239958 -1.56328477]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1300e-01],\n",
      "        [ 1.6953e+00],\n",
      "        [ 4.2923e-04],\n",
      "        [ 7.8549e-01],\n",
      "        [-1.8446e-01],\n",
      "        [ 1.5369e+00],\n",
      "        [-6.4240e-01],\n",
      "        [-1.5633e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.1863, 0.2116]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6130],\n",
      "        [ 1.6953],\n",
      "        [ 0.0000],\n",
      "        [ 0.7855],\n",
      "        [-0.1845],\n",
      "        [ 1.5369],\n",
      "        [-0.6424],\n",
      "        [-1.5633]], dtype=torch.float64)\n",
      "[-0.61300241  1.69532267  0.          0.78548959 -0.18446095  1.53687465\n",
      " -0.64239958 -1.56328477]\n",
      "[-0.61300241  1.69532267  0.          0.78548959 -0.18446095  1.53687465\n",
      " -0.64239958 -1.56328477]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4105773340148698 at iteration 9999, val_loss: 0.38569724666939265, best_val_loss: 0.395840554071459: 100%|██████████| 10000/10000 [00:41<00:00, 242.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.395840554071459\n",
      "-60.13712132271617\n",
      "tensor([[ 0.2653],\n",
      "        [-0.9492],\n",
      "        [ 0.0000],\n",
      "        [-0.6953],\n",
      "        [-0.0000],\n",
      "        [-1.0408],\n",
      "        [-0.0000],\n",
      "        [ 0.9236]], dtype=torch.float64)\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "[Parameter containing:\n",
      "tensor([[ 2.6529e-01],\n",
      "        [-9.4922e-01],\n",
      "        [ 1.3119e-03],\n",
      "        [-6.9530e-01],\n",
      "        [-2.0519e-04],\n",
      "        [-1.0408e+00],\n",
      "        [-7.8884e-04],\n",
      "        [ 9.2358e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 3.9280, -0.9570]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[ 0.2653],\n",
      "        [-0.9492],\n",
      "        [ 0.0000],\n",
      "        [-0.6953],\n",
      "        [-0.0000],\n",
      "        [-1.0408],\n",
      "        [-0.0000],\n",
      "        [ 0.9236]], dtype=torch.float64)\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "tensor([[ 0.2653],\n",
      "        [-0.9492],\n",
      "        [ 0.0000],\n",
      "        [-0.6953],\n",
      "        [-0.0000],\n",
      "        [-1.0408],\n",
      "        [-0.0000],\n",
      "        [ 0.9236]], dtype=torch.float64)\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed4' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed4\\clamp0.01_1dim_order1_lambda0.003_seed4dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed4\\_seed4.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed4\\clamp0.01_1dim_order1_lambda0.003_seed4_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed4\\_extrapolation_seed4.xlsx' does not exist.\n",
      "tensor([[ 0.2653],\n",
      "        [-0.9492],\n",
      "        [ 0.0000],\n",
      "        [-0.6953],\n",
      "        [-0.0000],\n",
      "        [-1.0408],\n",
      "        [-0.0000],\n",
      "        [ 0.9236]], dtype=torch.float64)\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n",
      "[ 0.26529251 -0.94922359  0.         -0.69529822 -0.         -1.04077403\n",
      " -0.          0.92357919]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5442017147581385 at iteration 9999, val_loss: 0.5805703953564009, best_val_loss: 0.5889085084976764: 100%|██████████| 10000/10000 [00:41<00:00, 238.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5889085084976764\n",
      "tensor([[ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.6066],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.4187],\n",
      "        [-0.7706],\n",
      "        [-0.5563]], dtype=torch.float64)\n",
      "[ 0.         -0.         -0.60658177 -0.         -0.          1.41865944\n",
      " -0.77061229 -0.55628163]\n",
      "[ 0.         -0.         -0.60658177 -0.         -0.          1.41865944\n",
      " -0.77061229 -0.55628163]\n",
      "[Parameter containing:\n",
      "tensor([[ 3.6452e-05],\n",
      "        [-1.6940e-03],\n",
      "        [-6.0658e-01],\n",
      "        [-4.4219e-03],\n",
      "        [-9.8700e-03],\n",
      "        [ 1.4187e+00],\n",
      "        [-7.7061e-01],\n",
      "        [-5.5628e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0003, 0.3311]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.6066],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.4187],\n",
      "        [-0.7706],\n",
      "        [-0.5563]], dtype=torch.float64)\n",
      "[ 0.         -0.         -0.60658177 -0.         -0.          1.41865944\n",
      " -0.77061229 -0.55628163]\n",
      "[ 0.         -0.         -0.60658177 -0.         -0.          1.41865944\n",
      " -0.77061229 -0.55628163]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365623728572808 at iteration 9999, val_loss: 0.3695786458916809, best_val_loss: 0.37915160430259565: 100%|██████████| 10000/10000 [00:40<00:00, 244.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37915160430259565\n",
      "-56.90474797755705\n",
      "tensor([[-0.6152],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6578],\n",
      "        [-0.0000],\n",
      "        [ 1.3925],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1520e-01],\n",
      "        [ 8.2321e-04],\n",
      "        [-4.5259e-03],\n",
      "        [ 6.5775e-01],\n",
      "        [-7.4691e-03],\n",
      "        [ 1.3925e+00],\n",
      "        [-5.1855e-01],\n",
      "        [-1.5686e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[2.3478e-04, 5.4191e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6152],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6578],\n",
      "        [-0.0000],\n",
      "        [ 1.3925],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "tensor([[-0.6152],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6578],\n",
      "        [-0.0000],\n",
      "        [ 1.3925],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed5' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed5\\clamp0.01_1dim_order1_lambda0.003_seed5dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed5\\_seed5.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed5\\clamp0.01_1dim_order1_lambda0.003_seed5_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed5\\_extrapolation_seed5.xlsx' does not exist.\n",
      "tensor([[-0.6152],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6578],\n",
      "        [-0.0000],\n",
      "        [ 1.3925],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n",
      "[-0.6151986   0.         -0.          0.65775142 -0.          1.392499\n",
      " -0.51855258 -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5658183426921188 at iteration 9999, val_loss: 0.46480994186919944, best_val_loss: 0.4650026096581775: 100%|██████████| 10000/10000 [00:44<00:00, 223.60it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4650026096581775\n",
      "tensor([[ 0.1022],\n",
      "        [-1.5784],\n",
      "        [-0.0000],\n",
      "        [-1.0862],\n",
      "        [ 0.1940],\n",
      "        [-0.7242],\n",
      "        [-0.0000],\n",
      "        [ 1.5989]], dtype=torch.float64)\n",
      "[ 0.10215188 -1.5784304  -0.         -1.08616098  0.19403713 -0.72424175\n",
      " -0.          1.59890024]\n",
      "[ 0.10215188 -1.5784304  -0.         -1.08616098  0.19403713 -0.72424175\n",
      " -0.          1.59890024]\n",
      "[Parameter containing:\n",
      "tensor([[ 1.0215e-01],\n",
      "        [-1.5784e+00],\n",
      "        [-6.5017e-04],\n",
      "        [-1.0862e+00],\n",
      "        [ 1.9404e-01],\n",
      "        [-7.2424e-01],\n",
      "        [-2.5819e-03],\n",
      "        [ 1.5989e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 6.2066, -1.5207]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[ 0.1022],\n",
      "        [-1.5784],\n",
      "        [-0.0000],\n",
      "        [-1.0862],\n",
      "        [ 0.1940],\n",
      "        [-0.7242],\n",
      "        [-0.0000],\n",
      "        [ 1.5989]], dtype=torch.float64)\n",
      "[ 0.10215188 -1.5784304  -0.         -1.08616098  0.19403713 -0.72424175\n",
      " -0.          1.59890024]\n",
      "[ 0.10215188 -1.5784304  -0.         -1.08616098  0.19403713 -0.72424175\n",
      " -0.          1.59890024]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.8563649655152287 at iteration 9999, val_loss: -0.09395591753263899, best_val_loss: -0.09395592750413062: 100%|██████████| 10000/10000 [01:00<00:00, 165.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09395591753263899\n",
      "-118.26925789616166\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0008],\n",
      "        [ 0.0005],\n",
      "        [-0.0008],\n",
      "        [ 0.0001],\n",
      "        [-0.0011],\n",
      "        [ 0.0038],\n",
      "        [-0.0024],\n",
      "        [-0.0015]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3771, -0.0019]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed30' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed30\\clamp0.01_1dim_order1_lambda0.003_seed30dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed30\\_seed30.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed30\\clamp0.01_1dim_order1_lambda0.003_seed30_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed30\\_extrapolation_seed30.xlsx' does not exist.\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0.  0. -0.  0. -0. -0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6319040805292861 at iteration 9999, val_loss: 0.35996261984467404, best_val_loss: 0.36745716941930706: 100%|██████████| 10000/10000 [00:43<00:00, 231.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36745716941930706\n",
      "tensor([[-0.0000],\n",
      "        [ 3.2992],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.5691],\n",
      "        [-0.2498],\n",
      "        [-3.2464]], dtype=torch.float64)\n",
      "[-0.          3.2991953   0.         -0.         -0.          1.56906105\n",
      " -0.24979928 -3.24641807]\n",
      "[-0.          3.2991953   0.         -0.         -0.          1.56906105\n",
      " -0.24979928 -3.24641807]\n",
      "[Parameter containing:\n",
      "tensor([[-1.7498e-03],\n",
      "        [ 3.2992e+00],\n",
      "        [ 3.5341e-04],\n",
      "        [-7.7850e-03],\n",
      "        [-9.8951e-03],\n",
      "        [ 1.5691e+00],\n",
      "        [-2.4980e-01],\n",
      "        [-3.2464e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[7.2056e-06, 6.7756e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.0000],\n",
      "        [ 3.2992],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.5691],\n",
      "        [-0.2498],\n",
      "        [-3.2464]], dtype=torch.float64)\n",
      "[-0.          3.2991953   0.         -0.         -0.          1.56906105\n",
      " -0.24979928 -3.24641807]\n",
      "[-0.          3.2991953   0.         -0.         -0.          1.56906105\n",
      " -0.24979928 -3.24641807]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.30851133186715984 at iteration 9999, val_loss: 0.4178804711830699, best_val_loss: 0.42502383965195767: 100%|██████████| 10000/10000 [00:41<00:00, 242.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42502383965195767\n",
      "-54.8598767819285\n",
      "tensor([[-2.1137],\n",
      "        [ 1.5764],\n",
      "        [ 1.4854],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9626],\n",
      "        [-0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-2.1137e+00],\n",
      "        [ 1.5764e+00],\n",
      "        [ 1.4854e+00],\n",
      "        [ 5.1464e-04],\n",
      "        [ 1.8094e-03],\n",
      "        [ 9.6264e-01],\n",
      "        [-9.7582e-04],\n",
      "        [ 1.9209e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0002, 0.0189]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-2.1137],\n",
      "        [ 1.5764],\n",
      "        [ 1.4854],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9626],\n",
      "        [-0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "tensor([[-2.1137],\n",
      "        [ 1.5764],\n",
      "        [ 1.4854],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9626],\n",
      "        [-0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed35' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed35\\clamp0.01_1dim_order1_lambda0.003_seed35dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed35\\_seed35.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed35\\clamp0.01_1dim_order1_lambda0.003_seed35_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed35\\_extrapolation_seed35.xlsx' does not exist.\n",
      "tensor([[-2.1137],\n",
      "        [ 1.5764],\n",
      "        [ 1.4854],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9626],\n",
      "        [-0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n",
      "[-2.11369399  1.5763704   1.48535047  0.          0.          0.962639\n",
      " -0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7540068664064047 at iteration 9999, val_loss: 0.48268802321887905, best_val_loss: 0.48650854586898795: 100%|██████████| 10000/10000 [00:42<00:00, 233.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48650854586898795\n",
      "tensor([[-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2410],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.7832],\n",
      "        [ 0.0000],\n",
      "        [ 0.2279]], dtype=torch.float64)\n",
      "[-0.          0.          0.24095125  0.         -0.         -0.7831505\n",
      "  0.          0.22789544]\n",
      "[-0.          0.          0.24095125  0.         -0.         -0.7831505\n",
      "  0.          0.22789544]\n",
      "[Parameter containing:\n",
      "tensor([[-2.2245e-03],\n",
      "        [ 2.2786e-03],\n",
      "        [ 2.4095e-01],\n",
      "        [ 2.6104e-03],\n",
      "        [-5.5828e-04],\n",
      "        [-7.8315e-01],\n",
      "        [ 1.2552e-03],\n",
      "        [ 2.2790e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 4.7660, -2.2627]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2410],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.7832],\n",
      "        [ 0.0000],\n",
      "        [ 0.2279]], dtype=torch.float64)\n",
      "[-0.          0.          0.24095125  0.         -0.         -0.7831505\n",
      "  0.          0.22789544]\n",
      "[-0.          0.          0.24095125  0.         -0.         -0.7831505\n",
      "  0.          0.22789544]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.308276124724112 at iteration 9999, val_loss: 0.4187811445037417, best_val_loss: 0.4251710571055417: 100%|██████████| 10000/10000 [00:41<00:00, 240.36it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4251710571055417\n",
      "-54.18399782050289\n",
      "tensor([[-2.1182],\n",
      "        [ 1.5806],\n",
      "        [ 1.4897],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.9604],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-2.1182e+00],\n",
      "        [ 1.5806e+00],\n",
      "        [ 1.4897e+00],\n",
      "        [ 3.0934e-03],\n",
      "        [-1.2874e-03],\n",
      "        [ 9.6039e-01],\n",
      "        [ 1.8673e-03],\n",
      "        [ 8.9978e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0022,  0.0188]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-2.1182],\n",
      "        [ 1.5806],\n",
      "        [ 1.4897],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.9604],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "tensor([[-2.1182],\n",
      "        [ 1.5806],\n",
      "        [ 1.4897],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.9604],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed40' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed40\\clamp0.01_1dim_order1_lambda0.003_seed40dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed40\\_seed40.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed40\\clamp0.01_1dim_order1_lambda0.003_seed40_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed40\\_extrapolation_seed40.xlsx' does not exist.\n",
      "tensor([[-2.1182],\n",
      "        [ 1.5806],\n",
      "        [ 1.4897],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.9604],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n",
      "[-2.11815552  1.58056913  1.48966771  0.         -0.          0.9603892\n",
      "  0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4206006484358173 at iteration 9999, val_loss: 0.6098633428331024, best_val_loss: 0.6111691360032221: 100%|██████████| 10000/10000 [00:41<00:00, 240.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111691360032221\n",
      "tensor([[-0.4758],\n",
      "        [ 2.1997],\n",
      "        [ 0.0000],\n",
      "        [ 0.9695],\n",
      "        [ 0.0000],\n",
      "        [ 1.5083],\n",
      "        [-0.6413],\n",
      "        [-2.0833]], dtype=torch.float64)\n",
      "[-0.47578417  2.19968555  0.          0.96953276  0.          1.50827398\n",
      " -0.64127838 -2.08328979]\n",
      "[-0.47578417  2.19968555  0.          0.96953276  0.          1.50827398\n",
      " -0.64127838 -2.08328979]\n",
      "[Parameter containing:\n",
      "tensor([[-4.7578e-01],\n",
      "        [ 2.1997e+00],\n",
      "        [ 1.0797e-03],\n",
      "        [ 9.6953e-01],\n",
      "        [ 1.3366e-04],\n",
      "        [ 1.5083e+00],\n",
      "        [-6.4128e-01],\n",
      "        [-2.0833e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0016,  0.6193]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.4758],\n",
      "        [ 2.1997],\n",
      "        [ 0.0000],\n",
      "        [ 0.9695],\n",
      "        [ 0.0000],\n",
      "        [ 1.5083],\n",
      "        [-0.6413],\n",
      "        [-2.0833]], dtype=torch.float64)\n",
      "[-0.47578417  2.19968555  0.          0.96953276  0.          1.50827398\n",
      " -0.64127838 -2.08328979]\n",
      "[-0.47578417  2.19968555  0.          0.96953276  0.          1.50827398\n",
      " -0.64127838 -2.08328979]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365664400611732 at iteration 9999, val_loss: 0.36958070980627933, best_val_loss: 0.3791694105458503: 100%|██████████| 10000/10000 [00:42<00:00, 233.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3791694105458503\n",
      "-57.63849839290516\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6285],\n",
      "        [-0.0749],\n",
      "        [ 1.3921],\n",
      "        [-0.4320],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1482e-01],\n",
      "        [ 8.6556e-05],\n",
      "        [-6.0517e-03],\n",
      "        [ 6.2854e-01],\n",
      "        [-7.4897e-02],\n",
      "        [ 1.3921e+00],\n",
      "        [-4.3202e-01],\n",
      "        [-1.5414e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[3.3899e-05, 5.3803e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6285],\n",
      "        [-0.0749],\n",
      "        [ 1.3921],\n",
      "        [-0.4320],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6285],\n",
      "        [-0.0749],\n",
      "        [ 1.3921],\n",
      "        [-0.4320],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed45' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed45\\clamp0.01_1dim_order1_lambda0.003_seed45dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed45\\_seed45.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed45\\clamp0.01_1dim_order1_lambda0.003_seed45_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed45\\_extrapolation_seed45.xlsx' does not exist.\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6285],\n",
      "        [-0.0749],\n",
      "        [ 1.3921],\n",
      "        [-0.4320],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n",
      "[-0.61482315  0.         -0.          0.62853803 -0.07489747  1.39209087\n",
      " -0.43202059 -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3140332706543454 at iteration 9999, val_loss: -0.05526070519911408, best_val_loss: -2.2846092615491642e-06: 100%|██████████| 10000/10000 [00:40<00:00, 245.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2846092615491642e-06\n",
      "tensor([[0.0324],\n",
      "        [-0.0000],\n",
      "        [0.0000],\n",
      "        [0.0591],\n",
      "        [0.0000],\n",
      "        [0.1204],\n",
      "        [0.0778],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[ 0.03238564 -0.          0.          0.05911116  0.          0.1204399\n",
      "  0.07781266 -0.        ]\n",
      "[ 0.03238564 -0.          0.          0.05911116  0.          0.1204399\n",
      "  0.07781266 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0324],\n",
      "        [-0.0007],\n",
      "        [ 0.0017],\n",
      "        [ 0.0591],\n",
      "        [ 0.0013],\n",
      "        [ 0.1204],\n",
      "        [ 0.0778],\n",
      "        [-0.0031]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.3592, 0.0021]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.0324],\n",
      "        [-0.0000],\n",
      "        [0.0000],\n",
      "        [0.0591],\n",
      "        [0.0000],\n",
      "        [0.1204],\n",
      "        [0.0778],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[ 0.03238564 -0.          0.          0.05911116  0.          0.1204399\n",
      "  0.07781266 -0.        ]\n",
      "[ 0.03238564 -0.          0.          0.05911116  0.          0.1204399\n",
      "  0.07781266 -0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.32565723670515107 at iteration 9999, val_loss: 0.41843648769079866, best_val_loss: 0.4269410330944602: 100%|██████████| 10000/10000 [00:43<00:00, 228.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4269410330944602\n",
      "-37.2986589600392\n",
      "tensor([[-0.5471],\n",
      "        [ 1.1684],\n",
      "        [-0.0000],\n",
      "        [ 0.6632],\n",
      "        [ 0.0000],\n",
      "        [ 1.5547],\n",
      "        [-0.4468],\n",
      "        [-1.1103]], dtype=torch.float64)\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "[Parameter containing:\n",
      "tensor([[-5.4715e-01],\n",
      "        [ 1.1684e+00],\n",
      "        [-7.4130e-04],\n",
      "        [ 6.6317e-01],\n",
      "        [ 2.8817e-03],\n",
      "        [ 1.5547e+00],\n",
      "        [-4.4684e-01],\n",
      "        [-1.1103e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0007, 0.6159]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.5471],\n",
      "        [ 1.1684],\n",
      "        [-0.0000],\n",
      "        [ 0.6632],\n",
      "        [ 0.0000],\n",
      "        [ 1.5547],\n",
      "        [-0.4468],\n",
      "        [-1.1103]], dtype=torch.float64)\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "tensor([[-0.5471],\n",
      "        [ 1.1684],\n",
      "        [-0.0000],\n",
      "        [ 0.6632],\n",
      "        [ 0.0000],\n",
      "        [ 1.5547],\n",
      "        [-0.4468],\n",
      "        [-1.1103]], dtype=torch.float64)\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed50' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed50\\clamp0.01_1dim_order1_lambda0.003_seed50dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed50\\_seed50.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed50\\clamp0.01_1dim_order1_lambda0.003_seed50_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed50\\_extrapolation_seed50.xlsx' does not exist.\n",
      "tensor([[-0.5471],\n",
      "        [ 1.1684],\n",
      "        [-0.0000],\n",
      "        [ 0.6632],\n",
      "        [ 0.0000],\n",
      "        [ 1.5547],\n",
      "        [-0.4468],\n",
      "        [-1.1103]], dtype=torch.float64)\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n",
      "[-0.54714888  1.16844798 -0.          0.66316993  0.          1.55467804\n",
      " -0.44683807 -1.1103073 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5494488946142293 at iteration 9999, val_loss: 0.5998513881793973, best_val_loss: 0.6071242876102236: 100%|██████████| 10000/10000 [00:42<00:00, 232.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6071242876102236\n",
      "tensor([[-0.6552],\n",
      "        [ 0.1991],\n",
      "        [-0.0000],\n",
      "        [ 0.8874],\n",
      "        [-0.0000],\n",
      "        [ 1.2810],\n",
      "        [-0.7729],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.6552085   0.19910603 -0.          0.88742915 -0.          1.28100458\n",
      " -0.77291136  0.        ]\n",
      "[-0.6552085   0.19910603 -0.          0.88742915 -0.          1.28100458\n",
      " -0.77291136  0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.5521e-01],\n",
      "        [ 1.9911e-01],\n",
      "        [-5.9899e-04],\n",
      "        [ 8.8743e-01],\n",
      "        [-9.8851e-03],\n",
      "        [ 1.2810e+00],\n",
      "        [-7.7291e-01],\n",
      "        [ 1.8882e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[2.8521e-04, 4.7573e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6552],\n",
      "        [ 0.1991],\n",
      "        [-0.0000],\n",
      "        [ 0.8874],\n",
      "        [-0.0000],\n",
      "        [ 1.2810],\n",
      "        [-0.7729],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.6552085   0.19910603 -0.          0.88742915 -0.          1.28100458\n",
      " -0.77291136  0.        ]\n",
      "[-0.6552085   0.19910603 -0.          0.88742915 -0.          1.28100458\n",
      " -0.77291136  0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20407430043321284 at iteration 9999, val_loss: 0.7210453509496946, best_val_loss: 0.7299771427167214: 100%|██████████| 10000/10000 [00:50<00:00, 199.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7299771427167214\n",
      "-31.086674981999728\n",
      "tensor([[-6.2748],\n",
      "        [ 5.6284],\n",
      "        [ 5.5526],\n",
      "        [ 6.1443],\n",
      "        [-0.0000],\n",
      "        [ 0.8443],\n",
      "        [-0.1332],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.2748e+00],\n",
      "        [ 5.6284e+00],\n",
      "        [ 5.5526e+00],\n",
      "        [ 6.1443e+00],\n",
      "        [-3.5447e-03],\n",
      "        [ 8.4434e-01],\n",
      "        [-1.3318e-01],\n",
      "        [ 1.5871e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0006,  0.0740]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-6.2748],\n",
      "        [ 5.6284],\n",
      "        [ 5.5526],\n",
      "        [ 6.1443],\n",
      "        [-0.0000],\n",
      "        [ 0.8443],\n",
      "        [-0.1332],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "tensor([[-6.2748],\n",
      "        [ 5.6284],\n",
      "        [ 5.5526],\n",
      "        [ 6.1443],\n",
      "        [-0.0000],\n",
      "        [ 0.8443],\n",
      "        [-0.1332],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed55' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed55\\clamp0.01_1dim_order1_lambda0.003_seed55dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed55\\_seed55.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed55\\clamp0.01_1dim_order1_lambda0.003_seed55_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed55\\_extrapolation_seed55.xlsx' does not exist.\n",
      "tensor([[-6.2748],\n",
      "        [ 5.6284],\n",
      "        [ 5.5526],\n",
      "        [ 6.1443],\n",
      "        [-0.0000],\n",
      "        [ 0.8443],\n",
      "        [-0.1332],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n",
      "[-6.27475158  5.62844662  5.55260024  6.14425517 -0.          0.84434298\n",
      " -0.1331808   0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3140580849075254 at iteration 9999, val_loss: -0.05525862411189242, best_val_loss: -1.3078990386272693e-06: 100%|██████████| 10000/10000 [00:40<00:00, 248.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3078990386272693e-06\n",
      "tensor([[0.3402],\n",
      "        [0.6795],\n",
      "        [0.2771],\n",
      "        [0.7074],\n",
      "        [0.2895],\n",
      "        [0.8689],\n",
      "        [0.1892],\n",
      "        [0.6324]], dtype=torch.float64)\n",
      "[0.34017757 0.67951999 0.27707249 0.70735867 0.28951814 0.86889372\n",
      " 0.18923428 0.63238869]\n",
      "[0.34017757 0.67951999 0.27707249 0.70735867 0.28951814 0.86889372\n",
      " 0.18923428 0.63238869]\n",
      "[Parameter containing:\n",
      "tensor([[0.3402],\n",
      "        [0.6795],\n",
      "        [0.2771],\n",
      "        [0.7074],\n",
      "        [0.2895],\n",
      "        [0.8689],\n",
      "        [0.1892],\n",
      "        [0.6324]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.3623e+00, 3.9210e-04]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.3402],\n",
      "        [0.6795],\n",
      "        [0.2771],\n",
      "        [0.7074],\n",
      "        [0.2895],\n",
      "        [0.8689],\n",
      "        [0.1892],\n",
      "        [0.6324]], dtype=torch.float64)\n",
      "[0.34017757 0.67951999 0.27707249 0.70735867 0.28951814 0.86889372\n",
      " 0.18923428 0.63238869]\n",
      "[0.34017757 0.67951999 0.27707249 0.70735867 0.28951814 0.86889372\n",
      " 0.18923428 0.63238869]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365547365343138 at iteration 9999, val_loss: 0.36957840483756876, best_val_loss: 0.37929782893191877: 100%|██████████| 10000/10000 [00:41<00:00, 241.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37929782893191877\n",
      "-56.996604625685585\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6574],\n",
      "        [-0.0000],\n",
      "        [ 1.3933],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0049],\n",
      "        [-0.0054],\n",
      "        [ 0.6574],\n",
      "        [-0.0076],\n",
      "        [ 1.3933],\n",
      "        [-0.5183],\n",
      "        [-0.0022]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-4.1420e-05,  5.4186e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6574],\n",
      "        [-0.0000],\n",
      "        [ 1.3933],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6574],\n",
      "        [-0.0000],\n",
      "        [ 1.3933],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed60' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed60\\clamp0.01_1dim_order1_lambda0.003_seed60dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed60\\_seed60.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed60\\clamp0.01_1dim_order1_lambda0.003_seed60_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed60\\_extrapolation_seed60.xlsx' does not exist.\n",
      "tensor([[-0.6151],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6574],\n",
      "        [-0.0000],\n",
      "        [ 1.3933],\n",
      "        [-0.5183],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n",
      "[-0.61511067  0.         -0.          0.6574263  -0.          1.39331364\n",
      " -0.5183289  -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3140436243181566 at iteration 9999, val_loss: -0.05526133619386697, best_val_loss: -4.677521481255553e-07: 100%|██████████| 10000/10000 [00:41<00:00, 243.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.677521481255553e-07\n",
      "tensor([[0.5260],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.5178],\n",
      "        [-0.0000],\n",
      "        [0.2791],\n",
      "        [-0.0000],\n",
      "        [0.0000]], dtype=torch.float64)\n",
      "[ 0.52595395  0.          0.          0.51776807 -0.          0.27913349\n",
      " -0.          0.        ]\n",
      "[ 0.52595395  0.          0.          0.51776807 -0.          0.27913349\n",
      " -0.          0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[ 5.2595e-01],\n",
      "        [ 1.0520e-03],\n",
      "        [ 8.4598e-04],\n",
      "        [ 5.1777e-01],\n",
      "        [-5.1454e-03],\n",
      "        [ 2.7913e-01],\n",
      "        [-4.3640e-03],\n",
      "        [ 5.5476e-05]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.3617, 0.0028]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.5260],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.5178],\n",
      "        [-0.0000],\n",
      "        [0.2791],\n",
      "        [-0.0000],\n",
      "        [0.0000]], dtype=torch.float64)\n",
      "[ 0.52595395  0.          0.          0.51776807 -0.          0.27913349\n",
      " -0.          0.        ]\n",
      "[ 0.52595395  0.          0.          0.51776807 -0.          0.27913349\n",
      " -0.          0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3366724400035393 at iteration 9999, val_loss: 0.36806601313715726, best_val_loss: 0.3748686241539564: 100%|██████████| 10000/10000 [00:43<00:00, 231.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3748686241539564\n",
      "-56.86600323725916\n",
      "tensor([[-0.6178],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6246],\n",
      "        [-0.0000],\n",
      "        [ 1.3495],\n",
      "        [-0.5158],\n",
      "        [ 0.0510]], dtype=torch.float64)\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1778e-01],\n",
      "        [-2.1725e-03],\n",
      "        [-5.5080e-05],\n",
      "        [ 6.2457e-01],\n",
      "        [-7.0209e-03],\n",
      "        [ 1.3495e+00],\n",
      "        [-5.1581e-01],\n",
      "        [ 5.0959e-02]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.7076e-04, 5.2889e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6178],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6246],\n",
      "        [-0.0000],\n",
      "        [ 1.3495],\n",
      "        [-0.5158],\n",
      "        [ 0.0510]], dtype=torch.float64)\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "tensor([[-0.6178],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6246],\n",
      "        [-0.0000],\n",
      "        [ 1.3495],\n",
      "        [-0.5158],\n",
      "        [ 0.0510]], dtype=torch.float64)\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed65' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed65\\clamp0.01_1dim_order1_lambda0.003_seed65dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed65\\_seed65.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed65\\clamp0.01_1dim_order1_lambda0.003_seed65_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed65\\_extrapolation_seed65.xlsx' does not exist.\n",
      "tensor([[-0.6178],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6246],\n",
      "        [-0.0000],\n",
      "        [ 1.3495],\n",
      "        [-0.5158],\n",
      "        [ 0.0510]], dtype=torch.float64)\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n",
      "[-0.61777761 -0.         -0.          0.62457423 -0.          1.34949379\n",
      " -0.51580944  0.05095926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.314029572711606 at iteration 9999, val_loss: -0.0552517692770238, best_val_loss: -1.0211004715099392e-06: 100%|██████████| 10000/10000 [00:41<00:00, 243.23it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0211004715099392e-06\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[-0. -0.  0. -0. -0. -0.  0.  0.]\n",
      "[-0. -0.  0. -0. -0. -0.  0.  0.]\n",
      "[Parameter containing:\n",
      "tensor([[-0.0010],\n",
      "        [-0.0018],\n",
      "        [ 0.0015],\n",
      "        [-0.0031],\n",
      "        [-0.0006],\n",
      "        [-0.0010],\n",
      "        [ 0.0005],\n",
      "        [ 0.0008]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3598e+00, -7.0430e-04]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[-0. -0.  0. -0. -0. -0.  0.  0.]\n",
      "[-0. -0.  0. -0. -0. -0.  0.  0.]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3050538016362864 at iteration 9999, val_loss: 0.4440183614134494, best_val_loss: 0.4496093642760194: 100%|██████████| 10000/10000 [00:41<00:00, 240.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4496093642760194\n",
      "-43.34140882510917\n",
      "tensor([[-1.9860],\n",
      "        [ 2.1259],\n",
      "        [ 1.3986],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0560],\n",
      "        [ 0.0000],\n",
      "        [-0.6492]], dtype=torch.float64)\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "[Parameter containing:\n",
      "tensor([[-1.9860e+00],\n",
      "        [ 2.1259e+00],\n",
      "        [ 1.3986e+00],\n",
      "        [-2.0233e-03],\n",
      "        [-1.3644e-03],\n",
      "        [ 1.0560e+00],\n",
      "        [ 2.3987e-04],\n",
      "        [-6.4919e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0003,  0.0214]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-1.9860],\n",
      "        [ 2.1259],\n",
      "        [ 1.3986],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0560],\n",
      "        [ 0.0000],\n",
      "        [-0.6492]], dtype=torch.float64)\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "tensor([[-1.9860],\n",
      "        [ 2.1259],\n",
      "        [ 1.3986],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0560],\n",
      "        [ 0.0000],\n",
      "        [-0.6492]], dtype=torch.float64)\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed70' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed70\\clamp0.01_1dim_order1_lambda0.003_seed70dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed70\\_seed70.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed70\\clamp0.01_1dim_order1_lambda0.003_seed70_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed70\\_extrapolation_seed70.xlsx' does not exist.\n",
      "tensor([[-1.9860],\n",
      "        [ 2.1259],\n",
      "        [ 1.3986],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.0560],\n",
      "        [ 0.0000],\n",
      "        [-0.6492]], dtype=torch.float64)\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n",
      "[-1.98596485  2.12592366  1.39864364 -0.         -0.          1.05601468\n",
      "  0.         -0.64918542]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24529725020365362 at iteration 9999, val_loss: 0.7777581566572951, best_val_loss: 0.7795519974199407: 100%|██████████| 10000/10000 [00:47<00:00, 212.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7795519974199407\n",
      "tensor([[-6.0895],\n",
      "        [ 6.8469],\n",
      "        [ 5.4708],\n",
      "        [ 6.3518],\n",
      "        [ 0.0000],\n",
      "        [ 0.8265],\n",
      "        [-0.3067],\n",
      "        [-1.2721]], dtype=torch.float64)\n",
      "[-6.08952794  6.84694922  5.47079984  6.35178846  0.          0.82654768\n",
      " -0.3066628  -1.27208657]\n",
      "[-6.08952794  6.84694922  5.47079984  6.35178846  0.          0.82654768\n",
      " -0.3066628  -1.27208657]\n",
      "[Parameter containing:\n",
      "tensor([[-6.0895e+00],\n",
      "        [ 6.8469e+00],\n",
      "        [ 5.4708e+00],\n",
      "        [ 6.3518e+00],\n",
      "        [ 5.8383e-04],\n",
      "        [ 8.2655e-01],\n",
      "        [-3.0666e-01],\n",
      "        [-1.2721e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0016, 0.0761]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-6.0895],\n",
      "        [ 6.8469],\n",
      "        [ 5.4708],\n",
      "        [ 6.3518],\n",
      "        [ 0.0000],\n",
      "        [ 0.8265],\n",
      "        [-0.3067],\n",
      "        [-1.2721]], dtype=torch.float64)\n",
      "[-6.08952794  6.84694922  5.47079984  6.35178846  0.          0.82654768\n",
      " -0.3066628  -1.27208657]\n",
      "[-6.08952794  6.84694922  5.47079984  6.35178846  0.          0.82654768\n",
      " -0.3066628  -1.27208657]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3373139226330715 at iteration 9999, val_loss: 0.36715671953840845, best_val_loss: 0.3753917609376374: 100%|██████████| 10000/10000 [00:42<00:00, 233.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3753917609376374\n",
      "-56.48892134268069\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.5767],\n",
      "        [-0.5007],\n",
      "        [ 1.3853],\n",
      "        [-0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1527e-01],\n",
      "        [ 5.8392e-04],\n",
      "        [-4.9844e-03],\n",
      "        [ 5.7665e-01],\n",
      "        [-5.0073e-01],\n",
      "        [ 1.3853e+00],\n",
      "        [-5.0584e-04],\n",
      "        [-1.5795e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-1.8442e-04,  5.5805e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.5767],\n",
      "        [-0.5007],\n",
      "        [ 1.3853],\n",
      "        [-0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.5767],\n",
      "        [-0.5007],\n",
      "        [ 1.3853],\n",
      "        [-0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed75' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed75\\clamp0.01_1dim_order1_lambda0.003_seed75dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed75\\_seed75.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed75\\clamp0.01_1dim_order1_lambda0.003_seed75_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed75\\_extrapolation_seed75.xlsx' does not exist.\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.5767],\n",
      "        [-0.5007],\n",
      "        [ 1.3853],\n",
      "        [-0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n",
      "[-0.61527341  0.         -0.          0.57665196 -0.50072966  1.38525204\n",
      " -0.         -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4206212053761574 at iteration 9999, val_loss: 0.6098685147160798, best_val_loss: 0.6111815486115542: 100%|██████████| 10000/10000 [00:41<00:00, 238.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111815486115542\n",
      "tensor([[-0.4759],\n",
      "        [ 2.1996],\n",
      "        [-0.0000],\n",
      "        [ 0.9697],\n",
      "        [-0.0000],\n",
      "        [ 1.5082],\n",
      "        [-0.6414],\n",
      "        [-2.0835]], dtype=torch.float64)\n",
      "[-0.47592779  2.19956569 -0.          0.96968593 -0.          1.50823785\n",
      " -0.64143246 -2.0834611 ]\n",
      "[-0.47592779  2.19956569 -0.          0.96968593 -0.          1.50823785\n",
      " -0.64143246 -2.0834611 ]\n",
      "[Parameter containing:\n",
      "tensor([[-4.7593e-01],\n",
      "        [ 2.1996e+00],\n",
      "        [-1.3711e-03],\n",
      "        [ 9.6969e-01],\n",
      "        [-2.5920e-04],\n",
      "        [ 1.5082e+00],\n",
      "        [-6.4143e-01],\n",
      "        [-2.0835e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0037,  0.6197]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.4759],\n",
      "        [ 2.1996],\n",
      "        [-0.0000],\n",
      "        [ 0.9697],\n",
      "        [-0.0000],\n",
      "        [ 1.5082],\n",
      "        [-0.6414],\n",
      "        [-2.0835]], dtype=torch.float64)\n",
      "[-0.47592779  2.19956569 -0.          0.96968593 -0.          1.50823785\n",
      " -0.64143246 -2.0834611 ]\n",
      "[-0.47592779  2.19956569 -0.          0.96968593 -0.          1.50823785\n",
      " -0.64143246 -2.0834611 ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.33469420941195144 at iteration 9999, val_loss: 0.3657566149518613, best_val_loss: 0.37508876868627195: 100%|██████████| 10000/10000 [00:40<00:00, 246.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37508876868627195\n",
      "-55.10274762058606\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0894],\n",
      "        [-0.0000],\n",
      "        [ 0.6069],\n",
      "        [-0.0000],\n",
      "        [ 1.3257],\n",
      "        [-0.5120],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1481e-01],\n",
      "        [ 8.9449e-02],\n",
      "        [-1.9899e-05],\n",
      "        [ 6.0692e-01],\n",
      "        [-7.4882e-03],\n",
      "        [ 1.3257e+00],\n",
      "        [-5.1197e-01],\n",
      "        [-1.3328e-05]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-4.5664e-05,  5.2230e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0894],\n",
      "        [-0.0000],\n",
      "        [ 0.6069],\n",
      "        [-0.0000],\n",
      "        [ 1.3257],\n",
      "        [-0.5120],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0894],\n",
      "        [-0.0000],\n",
      "        [ 0.6069],\n",
      "        [-0.0000],\n",
      "        [ 1.3257],\n",
      "        [-0.5120],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed80' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed80\\clamp0.01_1dim_order1_lambda0.003_seed80dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed80\\_seed80.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed80\\clamp0.01_1dim_order1_lambda0.003_seed80_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed80\\_extrapolation_seed80.xlsx' does not exist.\n",
      "tensor([[-0.6148],\n",
      "        [ 0.0894],\n",
      "        [-0.0000],\n",
      "        [ 0.6069],\n",
      "        [-0.0000],\n",
      "        [ 1.3257],\n",
      "        [-0.5120],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n",
      "[-0.61480645  0.08944905 -0.          0.6069176  -0.          1.32570659\n",
      " -0.51196616 -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3140243477697493 at iteration 9999, val_loss: -0.05524409105594019, best_val_loss: -1.581425388419433e-07: 100%|██████████| 10000/10000 [00:42<00:00, 235.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.581425388419433e-07\n",
      "tensor([[0.2734],\n",
      "        [-0.0000],\n",
      "        [0.0000],\n",
      "        [0.5481],\n",
      "        [0.0000],\n",
      "        [0.2719],\n",
      "        [0.2644],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[ 0.27335077 -0.          0.          0.54810339  0.          0.27186053\n",
      "  0.26435186 -0.        ]\n",
      "[ 0.27335077 -0.          0.          0.54810339  0.          0.27186053\n",
      "  0.26435186 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[ 2.7335e-01],\n",
      "        [-4.5573e-04],\n",
      "        [ 4.1330e-03],\n",
      "        [ 5.4810e-01],\n",
      "        [ 2.9008e-03],\n",
      "        [ 2.7186e-01],\n",
      "        [ 2.6435e-01],\n",
      "        [-1.8836e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3605e+00, -1.1022e-03]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.2734],\n",
      "        [-0.0000],\n",
      "        [0.0000],\n",
      "        [0.5481],\n",
      "        [0.0000],\n",
      "        [0.2719],\n",
      "        [0.2644],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[ 0.27335077 -0.          0.          0.54810339  0.          0.27186053\n",
      "  0.26435186 -0.        ]\n",
      "[ 0.27335077 -0.          0.          0.54810339  0.          0.27186053\n",
      "  0.26435186 -0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.8563884297141097 at iteration 9999, val_loss: -0.09397462398935286, best_val_loss: -0.09397464332356864: 100%|██████████| 10000/10000 [00:54<00:00, 182.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09397462398935286\n",
      "-118.27120156527542\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "[Parameter containing:\n",
      "tensor([[ 7.2170e-05],\n",
      "        [ 8.2001e-05],\n",
      "        [-1.4803e-04],\n",
      "        [-5.6602e-03],\n",
      "        [-3.1138e-03],\n",
      "        [ 3.8738e-03],\n",
      "        [-2.8420e-03],\n",
      "        [-2.4204e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3771, -0.0025]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed85' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed85\\clamp0.01_1dim_order1_lambda0.003_seed85dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed85\\_seed85.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed85\\clamp0.01_1dim_order1_lambda0.003_seed85_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\bad_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed85\\_extrapolation_seed85.xlsx' does not exist.\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.]], dtype=torch.float64)\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n",
      "[ 0.  0. -0. -0. -0.  0. -0. -0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4205834222751453 at iteration 9999, val_loss: 0.6098156820636018, best_val_loss: 0.6111451674316193: 100%|██████████| 10000/10000 [00:43<00:00, 230.42it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111451674316193\n",
      "tensor([[-0.4758],\n",
      "        [ 2.1998],\n",
      "        [ 0.0000],\n",
      "        [ 0.9690],\n",
      "        [-0.0000],\n",
      "        [ 1.5081],\n",
      "        [-0.6412],\n",
      "        [-2.0833]], dtype=torch.float64)\n",
      "[-0.47575387  2.199789    0.          0.96897923 -0.          1.50808821\n",
      " -0.64124728 -2.08332137]\n",
      "[-0.47575387  2.199789    0.          0.96897923 -0.          1.50808821\n",
      " -0.64124728 -2.08332137]\n",
      "[Parameter containing:\n",
      "tensor([[-4.7575e-01],\n",
      "        [ 2.1998e+00],\n",
      "        [ 2.6201e-03],\n",
      "        [ 9.6898e-01],\n",
      "        [-5.7566e-04],\n",
      "        [ 1.5081e+00],\n",
      "        [-6.4125e-01],\n",
      "        [-2.0833e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-4.3758e-04,  6.1827e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.4758],\n",
      "        [ 2.1998],\n",
      "        [ 0.0000],\n",
      "        [ 0.9690],\n",
      "        [-0.0000],\n",
      "        [ 1.5081],\n",
      "        [-0.6412],\n",
      "        [-2.0833]], dtype=torch.float64)\n",
      "[-0.47575387  2.199789    0.          0.96897923 -0.          1.50808821\n",
      " -0.64124728 -2.08332137]\n",
      "[-0.47575387  2.199789    0.          0.96897923 -0.          1.50808821\n",
      " -0.64124728 -2.08332137]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.33601579330498044 at iteration 9999, val_loss: 0.36526369644594914, best_val_loss: 0.37375714624712486: 100%|██████████| 10000/10000 [00:41<00:00, 240.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37375714624712486\n",
      "-56.26325038267313\n",
      "tensor([[-0.6183],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.6240],\n",
      "        [-0.0000],\n",
      "        [ 1.3496],\n",
      "        [-0.5162],\n",
      "        [ 0.0505]], dtype=torch.float64)\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1826e-01],\n",
      "        [ 7.7050e-04],\n",
      "        [ 1.3879e-03],\n",
      "        [ 6.2396e-01],\n",
      "        [-4.1062e-03],\n",
      "        [ 1.3496e+00],\n",
      "        [-5.1618e-01],\n",
      "        [ 5.0511e-02]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0007,  0.5295]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6183],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.6240],\n",
      "        [-0.0000],\n",
      "        [ 1.3496],\n",
      "        [-0.5162],\n",
      "        [ 0.0505]], dtype=torch.float64)\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "tensor([[-0.6183],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.6240],\n",
      "        [-0.0000],\n",
      "        [ 1.3496],\n",
      "        [-0.5162],\n",
      "        [ 0.0505]], dtype=torch.float64)\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed90' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed90\\clamp0.01_1dim_order1_lambda0.003_seed90dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed90\\_seed90.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed90\\clamp0.01_1dim_order1_lambda0.003_seed90_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed90\\_extrapolation_seed90.xlsx' does not exist.\n",
      "tensor([[-0.6183],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.6240],\n",
      "        [-0.0000],\n",
      "        [ 1.3496],\n",
      "        [-0.5162],\n",
      "        [ 0.0505]], dtype=torch.float64)\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n",
      "[-0.61825578  0.          0.          0.62396301 -0.          1.34960982\n",
      " -0.51618341  0.05051132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5556088283848206 at iteration 9999, val_loss: 0.5774095568804447, best_val_loss: 0.5831026416817078: 100%|██████████| 10000/10000 [00:42<00:00, 233.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5831026416817078\n",
      "tensor([[-0.6355],\n",
      "        [ 0.2171],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.1165],\n",
      "        [-0.6582],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.63550661  0.21710337  0.          0.         -0.          1.116504\n",
      " -0.65819695 -0.        ]\n",
      "[-0.63550661  0.21710337  0.          0.         -0.          1.116504\n",
      " -0.65819695 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.3551e-01],\n",
      "        [ 2.1710e-01],\n",
      "        [ 9.6303e-04],\n",
      "        [ 3.5782e-03],\n",
      "        [-1.2344e-03],\n",
      "        [ 1.1165e+00],\n",
      "        [-6.5820e-01],\n",
      "        [-1.2601e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0006,  0.1263]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6355],\n",
      "        [ 0.2171],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.1165],\n",
      "        [-0.6582],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.63550661  0.21710337  0.          0.         -0.          1.116504\n",
      " -0.65819695 -0.        ]\n",
      "[-0.63550661  0.21710337  0.          0.         -0.          1.116504\n",
      " -0.65819695 -0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.8563566528459888 at iteration 9999, val_loss: -0.09396562643535522, best_val_loss: -0.09396564126592266: 100%|██████████| 10000/10000 [00:54<00:00, 184.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09396562643535522\n",
      "-118.27026670873774\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[Parameter containing:\n",
      "tensor([[9.6745e-04],\n",
      "        [8.7813e-04],\n",
      "        [1.3346e-03],\n",
      "        [1.9788e-04],\n",
      "        [2.2330e-04],\n",
      "        [2.9999e-05],\n",
      "        [8.2799e-04],\n",
      "        [1.6585e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3771e+00, -5.5076e-04]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed95' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed95\\clamp0.01_1dim_order1_lambda0.003_seed95dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed95\\_seed95.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed95\\clamp0.01_1dim_order1_lambda0.003_seed95_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed95\\_extrapolation_seed95.xlsx' does not exist.\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.42820185708474756 at iteration 9999, val_loss: 0.5851823565107357, best_val_loss: 0.5906516139576792: 100%|██████████| 10000/10000 [00:41<00:00, 240.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5906516139576792\n",
      "tensor([[-0.3059],\n",
      "        [ 1.5427],\n",
      "        [-0.3027],\n",
      "        [-0.2762],\n",
      "        [-0.4683],\n",
      "        [ 1.5581],\n",
      "        [-0.3456],\n",
      "        [-1.6978]], dtype=torch.float64)\n",
      "[-0.30585227  1.54265246 -0.30269707 -0.27616938 -0.46826678  1.55805939\n",
      " -0.34561405 -1.69781183]\n",
      "[-0.30585227  1.54265246 -0.30269707 -0.27616938 -0.46826678  1.55805939\n",
      " -0.34561405 -1.69781183]\n",
      "[Parameter containing:\n",
      "tensor([[-0.3059],\n",
      "        [ 1.5427],\n",
      "        [-0.3027],\n",
      "        [-0.2762],\n",
      "        [-0.4683],\n",
      "        [ 1.5581],\n",
      "        [-0.3456],\n",
      "        [-1.6978]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.2631, 0.0636]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.3059],\n",
      "        [ 1.5427],\n",
      "        [-0.3027],\n",
      "        [-0.2762],\n",
      "        [-0.4683],\n",
      "        [ 1.5581],\n",
      "        [-0.3456],\n",
      "        [-1.6978]], dtype=torch.float64)\n",
      "[-0.30585227  1.54265246 -0.30269707 -0.27616938 -0.46826678  1.55805939\n",
      " -0.34561405 -1.69781183]\n",
      "[-0.30585227  1.54265246 -0.30269707 -0.27616938 -0.46826678  1.55805939\n",
      " -0.34561405 -1.69781183]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19947197677186806 at iteration 9999, val_loss: 0.7228065164926643, best_val_loss: 0.7228013895352607: 100%|██████████| 10000/10000 [00:48<00:00, 207.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7228065164926643\n",
      "-31.211166929671414\n",
      "tensor([[-6.1339],\n",
      "        [ 5.4902],\n",
      "        [ 5.4151],\n",
      "        [ 5.9819],\n",
      "        [ 0.0000],\n",
      "        [ 0.8640],\n",
      "        [-0.1292],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1339e+00],\n",
      "        [ 5.4902e+00],\n",
      "        [ 5.4151e+00],\n",
      "        [ 5.9819e+00],\n",
      "        [ 5.0562e-04],\n",
      "        [ 8.6398e-01],\n",
      "        [-1.2916e-01],\n",
      "        [ 4.8066e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0017,  0.0777]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-6.1339],\n",
      "        [ 5.4902],\n",
      "        [ 5.4151],\n",
      "        [ 5.9819],\n",
      "        [ 0.0000],\n",
      "        [ 0.8640],\n",
      "        [-0.1292],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "tensor([[-6.1339],\n",
      "        [ 5.4902],\n",
      "        [ 5.4151],\n",
      "        [ 5.9819],\n",
      "        [ 0.0000],\n",
      "        [ 0.8640],\n",
      "        [-0.1292],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed100' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed100\\clamp0.01_1dim_order1_lambda0.003_seed100dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed100\\_seed100.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed100\\clamp0.01_1dim_order1_lambda0.003_seed100_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed100\\_extrapolation_seed100.xlsx' does not exist.\n",
      "tensor([[-6.1339],\n",
      "        [ 5.4902],\n",
      "        [ 5.4151],\n",
      "        [ 5.9819],\n",
      "        [ 0.0000],\n",
      "        [ 0.8640],\n",
      "        [-0.1292],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n",
      "[-6.13391545  5.49022829  5.41511037  5.98188206  0.          0.8639843\n",
      " -0.12915986  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2501678640449835 at iteration 9999, val_loss: 0.7721284292253862, best_val_loss: 0.7750376727642478: 100%|██████████| 10000/10000 [00:47<00:00, 211.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7750376727642478\n",
      "tensor([[-5.9127],\n",
      "        [ 6.6964],\n",
      "        [ 5.2994],\n",
      "        [ 6.1563],\n",
      "        [-0.0595],\n",
      "        [ 0.8403],\n",
      "        [-0.2535],\n",
      "        [-1.2932]], dtype=torch.float64)\n",
      "[-5.91274434  6.69638482  5.29935035  6.15629644 -0.0594612   0.84034206\n",
      " -0.25352513 -1.2932436 ]\n",
      "[-5.91274434  6.69638482  5.29935035  6.15629644 -0.0594612   0.84034206\n",
      " -0.25352513 -1.2932436 ]\n",
      "[Parameter containing:\n",
      "tensor([[-5.9127],\n",
      "        [ 6.6964],\n",
      "        [ 5.2994],\n",
      "        [ 6.1563],\n",
      "        [-0.0595],\n",
      "        [ 0.8403],\n",
      "        [-0.2535],\n",
      "        [-1.2932]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0011, 0.0801]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-5.9127],\n",
      "        [ 6.6964],\n",
      "        [ 5.2994],\n",
      "        [ 6.1563],\n",
      "        [-0.0595],\n",
      "        [ 0.8403],\n",
      "        [-0.2535],\n",
      "        [-1.2932]], dtype=torch.float64)\n",
      "[-5.91274434  6.69638482  5.29935035  6.15629644 -0.0594612   0.84034206\n",
      " -0.25352513 -1.2932436 ]\n",
      "[-5.91274434  6.69638482  5.29935035  6.15629644 -0.0594612   0.84034206\n",
      " -0.25352513 -1.2932436 ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3365578196422291 at iteration 9999, val_loss: 0.36958309381473287, best_val_loss: 0.37873002434446534: 100%|██████████| 10000/10000 [00:42<00:00, 237.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37873002434446534\n",
      "-56.697413095651065\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6566],\n",
      "        [-0.0000],\n",
      "        [ 1.3926],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.1529e-01],\n",
      "        [ 9.6797e-04],\n",
      "        [-5.8089e-03],\n",
      "        [ 6.5656e-01],\n",
      "        [-7.4611e-03],\n",
      "        [ 1.3926e+00],\n",
      "        [-5.1858e-01],\n",
      "        [-3.2131e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-3.6768e-05,  5.4154e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6566],\n",
      "        [-0.0000],\n",
      "        [ 1.3926],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6566],\n",
      "        [-0.0000],\n",
      "        [ 1.3926],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed105' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed105\\clamp0.01_1dim_order1_lambda0.003_seed105dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed105\\_seed105.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed105\\clamp0.01_1dim_order1_lambda0.003_seed105_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed105\\_extrapolation_seed105.xlsx' does not exist.\n",
      "tensor([[-0.6153],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [ 0.6566],\n",
      "        [-0.0000],\n",
      "        [ 1.3926],\n",
      "        [-0.5186],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n",
      "[-0.61528961  0.         -0.          0.65656229 -0.          1.39259453\n",
      " -0.51857867 -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5506043578346234 at iteration 9999, val_loss: 0.5967334812059941, best_val_loss: 0.603675029394289: 100%|██████████| 10000/10000 [00:46<00:00, 214.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603675029394289\n",
      "tensor([[-0.6520],\n",
      "        [ 0.2034],\n",
      "        [-0.0000],\n",
      "        [ 0.8191],\n",
      "        [-0.7752],\n",
      "        [ 1.2608],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.65195479  0.20344822 -0.          0.81911377 -0.77518679  1.26080194\n",
      "  0.          0.        ]\n",
      "[-0.65195479  0.20344822 -0.          0.81911377 -0.77518679  1.26080194\n",
      "  0.          0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.5195e-01],\n",
      "        [ 2.0345e-01],\n",
      "        [-1.4428e-03],\n",
      "        [ 8.1911e-01],\n",
      "        [-7.7519e-01],\n",
      "        [ 1.2608e+00],\n",
      "        [ 4.4816e-04],\n",
      "        [ 7.0655e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-1.5513e-04,  5.0306e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6520],\n",
      "        [ 0.2034],\n",
      "        [-0.0000],\n",
      "        [ 0.8191],\n",
      "        [-0.7752],\n",
      "        [ 1.2608],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.65195479  0.20344822 -0.          0.81911377 -0.77518679  1.26080194\n",
      "  0.          0.        ]\n",
      "[-0.65195479  0.20344822 -0.          0.81911377 -0.77518679  1.26080194\n",
      "  0.          0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.30845226484516963 at iteration 9999, val_loss: 0.4180547270587732, best_val_loss: 0.42499149625716093: 100%|██████████| 10000/10000 [00:41<00:00, 238.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42499149625716093\n",
      "-53.671560950959496\n",
      "tensor([[-2.1174],\n",
      "        [ 1.5797],\n",
      "        [ 1.4890],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9606],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-2.1174e+00],\n",
      "        [ 1.5797e+00],\n",
      "        [ 1.4890e+00],\n",
      "        [-1.9777e-03],\n",
      "        [ 2.4555e-03],\n",
      "        [ 9.6055e-01],\n",
      "        [ 3.0861e-03],\n",
      "        [-1.1420e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0008, 0.0189]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-2.1174],\n",
      "        [ 1.5797],\n",
      "        [ 1.4890],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9606],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "tensor([[-2.1174],\n",
      "        [ 1.5797],\n",
      "        [ 1.4890],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9606],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed110' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed110\\clamp0.01_1dim_order1_lambda0.003_seed110dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed110\\_seed110.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed110\\clamp0.01_1dim_order1_lambda0.003_seed110_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed110\\_extrapolation_seed110.xlsx' does not exist.\n",
      "tensor([[-2.1174],\n",
      "        [ 1.5797],\n",
      "        [ 1.4890],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.9606],\n",
      "        [ 0.0000],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n",
      "[-2.11740032  1.57966477  1.48895205 -0.          0.          0.96055343\n",
      "  0.         -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5558424601753322 at iteration 9999, val_loss: 0.5776431676742441, best_val_loss: 0.5829244369653592: 100%|██████████| 10000/10000 [00:46<00:00, 217.22it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5829244369653592\n",
      "tensor([[-0.6361],\n",
      "        [ 0.2169],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.6604],\n",
      "        [ 1.1131],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.63609923  0.21694499  0.         -0.         -0.66042798  1.11308707\n",
      "  0.          0.        ]\n",
      "[-0.63609923  0.21694499  0.         -0.         -0.66042798  1.11308707\n",
      "  0.          0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.3610e-01],\n",
      "        [ 2.1694e-01],\n",
      "        [ 1.6100e-03],\n",
      "        [-3.7255e-03],\n",
      "        [-6.6043e-01],\n",
      "        [ 1.1131e+00],\n",
      "        [ 2.9817e-04],\n",
      "        [ 5.2704e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0003, 0.1478]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6361],\n",
      "        [ 0.2169],\n",
      "        [ 0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.6604],\n",
      "        [ 1.1131],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n",
      "[-0.63609923  0.21694499  0.         -0.         -0.66042798  1.11308707\n",
      "  0.          0.        ]\n",
      "[-0.63609923  0.21694499  0.         -0.         -0.66042798  1.11308707\n",
      "  0.          0.        ]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.33709000904637876 at iteration 9999, val_loss: 0.35054985053663845, best_val_loss: 0.35691599187838274: 100%|██████████| 10000/10000 [00:44<00:00, 227.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35691599187838274\n",
      "-56.50074555570564\n",
      "tensor([[-0.6059],\n",
      "        [ 0.0972],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.3010],\n",
      "        [-0.3559],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.0589e-01],\n",
      "        [ 9.7212e-02],\n",
      "        [-1.6222e-04],\n",
      "        [-3.6561e-03],\n",
      "        [-7.9318e-03],\n",
      "        [ 1.3010e+00],\n",
      "        [-3.5587e-01],\n",
      "        [-5.9269e-04]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[1.5717e-04, 2.6812e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6059],\n",
      "        [ 0.0972],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.3010],\n",
      "        [-0.3559],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "tensor([[-0.6059],\n",
      "        [ 0.0972],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.3010],\n",
      "        [-0.3559],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed115' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed115\\clamp0.01_1dim_order1_lambda0.003_seed115dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed115\\_seed115.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed115\\clamp0.01_1dim_order1_lambda0.003_seed115_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed115\\_extrapolation_seed115.xlsx' does not exist.\n",
      "tensor([[-0.6059],\n",
      "        [ 0.0972],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [-0.0000],\n",
      "        [ 1.3010],\n",
      "        [-0.3559],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n",
      "[-0.60589423  0.09721168 -0.         -0.         -0.          1.30100933\n",
      " -0.35587051 -0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4019092078067161 at iteration 9999, val_loss: 0.5982732720329027, best_val_loss: 0.6060162540176102: 100%|██████████| 10000/10000 [00:43<00:00, 230.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6060162540176102\n",
      "tensor([[-0.6321],\n",
      "        [ 1.5431],\n",
      "        [ 0.1031],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.2458],\n",
      "        [-0.5591],\n",
      "        [-1.3124]], dtype=torch.float64)\n",
      "[-0.63209538  1.54309112  0.10305317 -0.          0.          1.2457669\n",
      " -0.55913805 -1.31235742]\n",
      "[-0.63209538  1.54309112  0.10305317 -0.          0.          1.2457669\n",
      " -0.55913805 -1.31235742]\n",
      "[Parameter containing:\n",
      "tensor([[-6.3210e-01],\n",
      "        [ 1.5431e+00],\n",
      "        [ 1.0305e-01],\n",
      "        [-4.7169e-03],\n",
      "        [ 9.4016e-06],\n",
      "        [ 1.2458e+00],\n",
      "        [-5.5914e-01],\n",
      "        [-1.3124e+00]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[2.5511e-05, 1.0795e-01]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-0.6321],\n",
      "        [ 1.5431],\n",
      "        [ 0.1031],\n",
      "        [-0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.2458],\n",
      "        [-0.5591],\n",
      "        [-1.3124]], dtype=torch.float64)\n",
      "[-0.63209538  1.54309112  0.10305317 -0.          0.          1.2457669\n",
      " -0.55913805 -1.31235742]\n",
      "[-0.63209538  1.54309112  0.10305317 -0.          0.          1.2457669\n",
      " -0.55913805 -1.31235742]\n",
      "flag 1\n",
      "Y.shape  = (235, 1)\n",
      "X.shape  = (235, 1)\n",
      "test_size =  11\n",
      "sorted indices =  [ 59  60  55 123 151  61  47 119  63  57  49]\n",
      "flag 2\n",
      "Y_test.shape  =  (11, 1)\n",
      "X_test.shape  = (11, 11)\n",
      "flag 3\n",
      "Y_train.shape  = (224, 1)\n",
      "X_train.shape  = (224, 11)\n",
      "flag 4\n",
      "Y_test.shape  = torch.Size([11, 1])\n",
      "X_test.shape  = torch.Size([11, 11])\n",
      "Y_train.shape  = torch.Size([224, 1])\n",
      "X_train.shape  = torch.Size([224, 11])\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 1])\n",
      "tensor([[5.2704],\n",
      "        [5.2078],\n",
      "        [5.1027],\n",
      "        [4.8692],\n",
      "        [4.8596],\n",
      "        [4.7389],\n",
      "        [4.4955],\n",
      "        [4.4885],\n",
      "        [4.4840],\n",
      "        [4.4658],\n",
      "        [4.3644]], dtype=torch.float64)\n",
      "tensor([[0.5229],\n",
      "        [1.2257],\n",
      "        [1.0226],\n",
      "        [0.6348],\n",
      "        [0.7313],\n",
      "        [0.7380],\n",
      "        [0.0448],\n",
      "        [0.7804],\n",
      "        [0.8222],\n",
      "        [1.0031],\n",
      "        [0.9383],\n",
      "        [1.0536],\n",
      "        [1.1369],\n",
      "        [1.1706],\n",
      "        [1.2126],\n",
      "        [1.4234],\n",
      "        [1.2742],\n",
      "        [1.4300],\n",
      "        [1.5602],\n",
      "        [1.6502],\n",
      "        [1.6436],\n",
      "        [1.8900],\n",
      "        [1.9436],\n",
      "        [2.0679],\n",
      "        [2.1295],\n",
      "        [2.0489],\n",
      "        [1.8697],\n",
      "        [1.7977],\n",
      "        [1.8884],\n",
      "        [1.8498],\n",
      "        [1.9803],\n",
      "        [2.1901],\n",
      "        [2.2562],\n",
      "        [2.8351],\n",
      "        [2.1063],\n",
      "        [2.2362],\n",
      "        [2.4225],\n",
      "        [2.2363],\n",
      "        [2.3305],\n",
      "        [2.4016],\n",
      "        [3.4086],\n",
      "        [3.0264],\n",
      "        [3.5970],\n",
      "        [2.7772],\n",
      "        [3.0086],\n",
      "        [3.0365],\n",
      "        [3.5697],\n",
      "        [3.7354],\n",
      "        [0.7603],\n",
      "        [0.6742],\n",
      "        [1.5386],\n",
      "        [1.9591],\n",
      "        [3.1504],\n",
      "        [3.6856],\n",
      "        [3.6409],\n",
      "        [2.6524],\n",
      "        [0.3148],\n",
      "        [0.3607],\n",
      "        [0.3535],\n",
      "        [0.7502],\n",
      "        [0.6030],\n",
      "        [0.9290],\n",
      "        [0.9022],\n",
      "        [1.1000],\n",
      "        [1.5725],\n",
      "        [1.6319],\n",
      "        [1.8864],\n",
      "        [1.4580],\n",
      "        [1.7801],\n",
      "        [1.9202],\n",
      "        [2.1545],\n",
      "        [0.1984],\n",
      "        [0.3057],\n",
      "        [0.3423],\n",
      "        [0.3483],\n",
      "        [0.6694],\n",
      "        [0.9192],\n",
      "        [1.0741],\n",
      "        [1.1641],\n",
      "        [0.8407],\n",
      "        [1.1884],\n",
      "        [1.2066],\n",
      "        [1.6308],\n",
      "        [1.5733],\n",
      "        [1.7753],\n",
      "        [1.9937],\n",
      "        [2.2230],\n",
      "        [0.3325],\n",
      "        [0.5419],\n",
      "        [0.0481],\n",
      "        [0.6194],\n",
      "        [1.0576],\n",
      "        [1.2197],\n",
      "        [1.3081],\n",
      "        [1.5139],\n",
      "        [1.7043],\n",
      "        [2.1380],\n",
      "        [1.9438],\n",
      "        [3.3392],\n",
      "        [2.1987],\n",
      "        [2.5479],\n",
      "        [2.6805],\n",
      "        [4.0431],\n",
      "        [0.5221],\n",
      "        [0.7320],\n",
      "        [1.6561],\n",
      "        [1.3523],\n",
      "        [1.6178],\n",
      "        [2.0932],\n",
      "        [3.6580],\n",
      "        [2.8382],\n",
      "        [0.7367],\n",
      "        [1.8101],\n",
      "        [4.2600],\n",
      "        [0.2789],\n",
      "        [0.3886],\n",
      "        [0.4255],\n",
      "        [0.4725],\n",
      "        [0.6803],\n",
      "        [0.9384],\n",
      "        [0.8733],\n",
      "        [1.0639],\n",
      "        [1.0900],\n",
      "        [1.4785],\n",
      "        [1.5100],\n",
      "        [1.8422],\n",
      "        [1.4963],\n",
      "        [1.9863],\n",
      "        [2.2699],\n",
      "        [2.5492],\n",
      "        [0.4197],\n",
      "        [0.5144],\n",
      "        [0.5791],\n",
      "        [0.8707],\n",
      "        [0.9648],\n",
      "        [1.3411],\n",
      "        [1.5351],\n",
      "        [2.4470],\n",
      "        [1.7411],\n",
      "        [2.1070],\n",
      "        [1.9769],\n",
      "        [2.1443],\n",
      "        [2.9415],\n",
      "        [2.9991],\n",
      "        [0.5346],\n",
      "        [0.8925],\n",
      "        [1.5190],\n",
      "        [2.8222],\n",
      "        [2.0402],\n",
      "        [2.8292],\n",
      "        [1.0148],\n",
      "        [3.1639],\n",
      "        [2.4008],\n",
      "        [0.3780],\n",
      "        [0.6600],\n",
      "        [1.3788],\n",
      "        [0.3334],\n",
      "        [0.7445],\n",
      "        [1.1838],\n",
      "        [0.5352],\n",
      "        [1.1283],\n",
      "        [1.9435],\n",
      "        [0.3360],\n",
      "        [0.5606],\n",
      "        [0.9574],\n",
      "        [1.6488],\n",
      "        [0.7273],\n",
      "        [1.2190],\n",
      "        [0.5098],\n",
      "        [0.2807],\n",
      "        [1.2759],\n",
      "        [1.8439],\n",
      "        [0.3017],\n",
      "        [0.6551],\n",
      "        [1.1109],\n",
      "        [0.2990],\n",
      "        [0.8623],\n",
      "        [2.0818],\n",
      "        [0.4187],\n",
      "        [1.2246],\n",
      "        [1.8464],\n",
      "        [2.3710],\n",
      "        [2.3254],\n",
      "        [0.4225],\n",
      "        [2.3466],\n",
      "        [1.8994],\n",
      "        [0.5534],\n",
      "        [1.7706],\n",
      "        [1.9708],\n",
      "        [1.3536],\n",
      "        [0.4484],\n",
      "        [1.2621],\n",
      "        [2.6716],\n",
      "        [2.1490],\n",
      "        [0.6287],\n",
      "        [1.9702],\n",
      "        [3.6084],\n",
      "        [0.3209],\n",
      "        [0.7450],\n",
      "        [1.0997],\n",
      "        [0.7207],\n",
      "        [0.6717],\n",
      "        [0.3140],\n",
      "        [0.5268],\n",
      "        [0.2977],\n",
      "        [0.6753],\n",
      "        [0.5939],\n",
      "        [0.2427],\n",
      "        [0.5178],\n",
      "        [0.4525],\n",
      "        [0.6105],\n",
      "        [0.7444],\n",
      "        [0.7341],\n",
      "        [0.3507],\n",
      "        [0.5229],\n",
      "        [0.2701],\n",
      "        [0.1425],\n",
      "        [0.3899],\n",
      "        [0.6402],\n",
      "        [1.0425],\n",
      "        [1.1992],\n",
      "        [0.1317],\n",
      "        [0.4121],\n",
      "        [0.8038]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19333294901602027 at iteration 9999, val_loss: 0.7418238564821564, best_val_loss: 0.7418167950494992: 100%|██████████| 10000/10000 [00:52<00:00, 189.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7418238564821564\n",
      "-29.604150132269826\n",
      "tensor([[-6.5219],\n",
      "        [ 5.8699],\n",
      "        [ 5.7939],\n",
      "        [ 6.4271],\n",
      "        [-0.0000],\n",
      "        [ 0.8241],\n",
      "        [-0.1325],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "[Parameter containing:\n",
      "tensor([[-6.5219e+00],\n",
      "        [ 5.8699e+00],\n",
      "        [ 5.7939e+00],\n",
      "        [ 6.4271e+00],\n",
      "        [-5.2487e-04],\n",
      "        [ 8.2410e-01],\n",
      "        [-1.3252e-01],\n",
      "        [-1.1332e-03]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[0.0019, 0.0704]], dtype=torch.float64, requires_grad=True)]\n",
      "tensor([[-6.5219],\n",
      "        [ 5.8699],\n",
      "        [ 5.7939],\n",
      "        [ 6.4271],\n",
      "        [-0.0000],\n",
      "        [ 0.8241],\n",
      "        [-0.1325],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "tensor([[-6.5219],\n",
      "        [ 5.8699],\n",
      "        [ 5.7939],\n",
      "        [ 6.4271],\n",
      "        [-0.0000],\n",
      "        [ 0.8241],\n",
      "        [-0.1325],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed120' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed120\\clamp0.01_1dim_order1_lambda0.003_seed120dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed120\\_seed120.xlsx' does not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed120\\clamp0.01_1dim_order1_lambda0.003_seed120_extrapolation_dim1\n",
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\He_code_new\\dim_num_mb_val_test\\packed\\5_rates\\good_rates_recheck\\clamp0.01_1dim_order1_lambda0.003_seed120\\_extrapolation_seed120.xlsx' does not exist.\n",
      "tensor([[-6.5219],\n",
      "        [ 5.8699],\n",
      "        [ 5.7939],\n",
      "        [ 6.4271],\n",
      "        [-0.0000],\n",
      "        [ 0.8241],\n",
      "        [-0.1325],\n",
      "        [-0.0000]], dtype=torch.float64)\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n",
      "[-6.52187108  5.86992151  5.79393123  6.4270538  -0.          0.82409752\n",
      " -0.13251551 -0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in g:\n",
    "    seed = i\n",
    "    #find which hyperparameter performed best\n",
    "\n",
    "    # idx = np.arange(lg_set.shape[0])\n",
    "    # best_hp = idx[array_metric_num[:,0]==np.max(array_metric_num)]\n",
    "\n",
    "    # if len(best_hp.shape)>1:\n",
    "    #     print('the following are equivalent ',best_hp)\n",
    "    #     print(best_hp.shape)\n",
    "    #     best_hp = best_hp[0,1]\n",
    "\n",
    "\n",
    "    #22 best so far, R2 = 0.998 for lambda = 0.001\n",
    "    #14 best so far, R2 = 0.945\n",
    "    ndimensionless = 1\n",
    "    #lambda_gamma = 0.003 #lambda_gamma = 0.01, #replaced\n",
    "\n",
    "    poly_order = 1 #was 2? but linear is a first order poly.\n",
    "\n",
    "    poly_mapping = np.array([[0],\n",
    "                            [1]])\n",
    "\n",
    "    #poly_mapping = np.array([[0, 0],\n",
    "    #                         [1, 0],\n",
    "    #                         [0, 1],\n",
    "    #                         [2, 0],\n",
    "    #                         [1, 1],\n",
    "    #                         [0, 2]])\n",
    "    lambda_beta = 0.01 #maybe cut in half idk\n",
    "    w_array = np.array(fff._basis_col)  #this is the w array, that is, the columns in Null(D)\n",
    "    gamma_name = ['y'+str(id) for id in range(0,fff.basis_col.shape[1]) ]\n",
    "    beta_name = ['b'+str(id) for id in range(0,poly_mapping.shape[0]) ]\n",
    "    poly_name = ['dim'+str(id+1) for id in range(0,ndimensionless)]\n",
    "\n",
    "\n",
    "\n",
    "    metric = 'r2'\n",
    "    para_threshold = 0.01 #vs 0.1?\n",
    "    beta_threshold = 0.005\n",
    "    training_epochs =10000\n",
    "    score = []\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "     #This first part of the cell, we fit without testing the performance on extrapolation. That comes later.\n",
    "        \n",
    "        \n",
    "\n",
    "    #create test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.15, random_state=42)\n",
    "\n",
    "    # create validation and training set  ##unused!\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    r2_reg = r2\n",
    "    print(r2)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack((best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name =  [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### BELOW MUST BE CORRECTED#####\n",
    "    if ndimensionless >=2:\n",
    "        \n",
    "        best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "        df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "    \n",
    "    \n",
    "    # store the data in data_frames\n",
    "    df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    \n",
    "\n",
    "    \n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "\n",
    "    #PREPARE TO DO THE EXTRAPOLATION TEST\n",
    "    \n",
    "    \n",
    "\n",
    "    # delete old variables\n",
    "\n",
    "    variable_names = ['X_test', 'Y_test', 'X_train', 'X_val', 'y_train', 'y_val', 'X_train_val', 'Y_train_val']\n",
    "    for var_name in variable_names:\n",
    "        if var_name in locals() or var_name in globals():\n",
    "            target_dict = locals() if var_name in locals() else globals()\n",
    "            del target_dict[var_name]\n",
    "            #print(f\"Deleted variable: {var_name}\")\n",
    "            \n",
    "    \n",
    "    # Refit, but we test extrapolation.\n",
    "\n",
    "    \n",
    "    X_test, Y_test, X_train_val, y_train_val = top_split_y(fff.X,fff.y,5)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "\n",
    "    \n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "\n",
    "    x_test = X_test\n",
    "    y_test = Y_test\n",
    "    print(y_test)\n",
    "    print(y_train_val)\n",
    "    training_epochs =10000\n",
    "\n",
    "\n",
    "    best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "    model_train = PiLinearRegressionViaTorch.TrainHolder(X_train, y_train, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "    metric_num = model_train.train(training_epochs, True, val_x=X_val, val_y =y_val , metric=metric, norm_on='null_space')\n",
    "\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "    r2 = model_train.get_validation_metric(X_val,y_val,metric)\n",
    "    r2_ext = r2\n",
    "    print(r2)\n",
    "    \n",
    "    #check its performance on the test set\n",
    "    r2_ext_test = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "    print(r2_ext_test)\n",
    "\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "    #store values from training\n",
    "    best_metric_num = np.array(metric_num)\n",
    "    best_beta = beta_prune\n",
    "\n",
    "    best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "    best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "    best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "    if ndimensionless >=2:\n",
    "        best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "        best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "        best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "    #r2, lambda, beta vector , cardinality \n",
    "    best_data_dim1 = np.hstack(( r2_ext_test.reshape(1,1), best_metric_num.reshape(1,1), np.array([para_threshold]).reshape(1,1)  ,np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "    data_name = ['r2_ext_test'] + [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "    df_best_dim1_ext = pd.DataFrame(data=best_data_dim1, columns =['r2_ext_test'] + [ metric] + ['clamp'] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    ##### BELOW MUST BE CORRECTED#####\n",
    "    if ndimensionless >=2:\n",
    "        \n",
    "        best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "        df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "    #df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "    #####\n",
    "    model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "    print([i for i in model_train.model.parameters()])\n",
    "    paras = [j for j in model_train.model.parameters()]\n",
    "    gamma = paras[0].detach()\n",
    "    beta = paras[1].detach()\n",
    "\n",
    "    #clamp them\n",
    "    t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "    beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # if you need the orignial pis\n",
    "    readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "    ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "    ori_pis[0]\n",
    "    #ori_pis[1]\n",
    "\n",
    "    ori_pis[0]\n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "    # put save_path decision here\n",
    "        #set save path--remove this chunk of code to the bottom?\n",
    "\n",
    "    #current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\"\n",
    "    if r2_reg >0.3 or r2_ext >0.3:\n",
    "        current_path = good_path\n",
    "    else:\n",
    "        current_path = bad_path\n",
    "    #new_folder =r'mb_1dim_v1_pt0001_pt_01_order1_golden_child'\n",
    "    new_folder = r'clamp'+str(para_threshold)+r'_' + str(ndimensionless)+r'dim_order'+str(poly_order)+r'_lambda'+str(lambda_gamma)+r\"_seed\"+str(seed)\n",
    "    new_path = current_path+'\\\\'+new_folder\n",
    "    if os.path.exists(new_path):\n",
    "        print(f\"File '{new_path}' already exists.\")\n",
    "    else:\n",
    "        os.mkdir(new_path)       \n",
    "        print(f\"File '{new_path}' did not exist.\")\n",
    "\n",
    "    file_path = new_path+'\\\\' +'_seed'+str(seed)+ '.xlsx'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "    # File path of the Excel file\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4.to_excel(writer, sheet_name='beta')\n",
    "        df5.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary and performance of this 'reg dim #'\n",
    "    if 'df_loop_reg' in globals():\n",
    "        df_loop_reg = pd.concat([df_loop_reg, df_best_dim1 ], ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        df_loop_reg = df_best_dim1.copy(deep=True)\n",
    "    \n",
    "            \n",
    "    # Now store the data from the extrapolation attempt. \n",
    "    # select file path to save xlsx sheet\n",
    "    # and overwrite the dataframes (df's) \n",
    "    # then store these new data frames in a separate excel file\n",
    "    \n",
    "    \n",
    "    # File path of the Excel file\n",
    "    file_path = new_path+'\\\\' + '_extrapolation' +'_seed'+str(seed)+ '.xlsx'\n",
    "    fig_path_dim1 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim1\"\n",
    "    fig_path_dim2 = new_path+'\\\\'+new_folder+'_extrapolation_' +\"dim2\"\n",
    "    print(fig_path_dim1)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"File '{file_path}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "    df0_ext = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "    df1_ext = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "    df4_ext =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "    df5_ext = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "    with pd.ExcelWriter(file_path) as writer:  \n",
    "        df0_ext.to_excel(writer, sheet_name='dimension_matrix')\n",
    "        df1_ext.to_excel(writer, sheet_name='null_space_matrix')\n",
    "        df4_ext.to_excel(writer, sheet_name='beta')\n",
    "        df5_ext.to_excel(writer, sheet_name='polynomial')\n",
    "        df_best_dim1_ext.to_excel(writer, sheet_name='best_dim_1')\n",
    "    \n",
    "    # now to store the summary of the 'ext dim #' for placement in an excel so I can compare them all!\n",
    "    if 'df_loop_ext' in globals():\n",
    "        df_loop_ext = pd.concat([df_loop_ext, df_best_dim1_ext ], ignore_index = True)\n",
    "        \n",
    "    else:\n",
    "        df_loop_ext = df_best_dim1_ext.copy(deep=True)\n",
    "    \n",
    "        \n",
    "    print(t)\n",
    "    print(np.asarray(t.transpose(0,1)[0]))\n",
    "    print(best_dim1_gamma)\n",
    "#write df loops to good path!\n",
    "with pd.ExcelWriter(summary_path) as writer:  \n",
    "    df_loop_reg.to_excel(writer, sheet_name='reg dimension number')\n",
    "    df_loop_ext.to_excel(writer, sheet_name='ext dimension number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(summary_path) as writer:  \n",
    "    df_loop_reg.to_excel(writer, sheet_name='reg dimension number')\n",
    "    df_loop_ext.to_excel(writer, sheet_name='ext dimension number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Define the sample rate and duration of each tone\n",
    "sample_rate = 44100  # Adjust as needed\n",
    "duration = 0.5  # Adjust as needed\n",
    "\n",
    "# Define notes and their corresponding frequencies\n",
    "notes = {\n",
    "    'C4': 261.63,\n",
    "    'D4': 293.66,\n",
    "    'E4': 329.63,\n",
    "    'F4': 349.23,\n",
    "    'G4': 392.00,\n",
    "    'A4': 440.00,\n",
    "    'B4': 493.88,\n",
    "    'C5': 523.25\n",
    "}\n",
    "\n",
    "# Define the melody\n",
    "melody = ['E4', 'D4', 'C4', 'D4', 'E4', 'E4', 'E4', 'D4', 'D4', 'D4', 'E4', 'G4', 'G4', 'E4', 'D4', 'C4', 'D4', 'E4', 'E4', 'E4', 'E4', 'D4', 'D4', 'E4', 'D4', 'C4']\n",
    "\n",
    "# Play the melody\n",
    "for note in melody:\n",
    "    frequency = notes[note]\n",
    "    waveform = pygame.sndarray.make_sound(pygame.sndarray.array([[int(4096 * 0.5 * np.sin(2.0 * np.pi * frequency * x / sample_rate)) for _ in range(2)] for x in range(int(sample_rate * duration))]))\n",
    "    waveform.play()\n",
    "    time.sleep(duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Define notes and their corresponding frequencies\n",
    "notes = {\n",
    "    'D4': 293.66,\n",
    "    'A4': 440.00,\n",
    "    'B4': 493.88,\n",
    "    'F#4': 369.99,\n",
    "    'G4': 392.00,\n",
    "    'E4': 329.63\n",
    "}\n",
    "\n",
    "# Define the melody for the first few bars of Pachelbel's Canon\n",
    "melody = ['D4', 'A4', 'B4', 'F#4', 'G4', 'D4', 'G4', 'A4', 'D4', 'B4', 'G4', 'D4', 'F#4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'F#4', 'E4', 'A4', 'D4', 'D4', 'B4', 'G4', 'D4', 'A4', 'A4', 'B4', 'F#4', 'G4', 'D4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'A4', 'B4', 'A4', 'G4', 'F#4', 'G4', 'F#4', 'E4']\n",
    "\n",
    "# Play the melody\n",
    "for note in melody:\n",
    "    frequency = notes[note]\n",
    "    waveform = pygame.sndarray.make_sound(pygame.sndarray.array([[int(4096 * 0.5 * np.sin(2.0 * np.pi * frequency * x / sample_rate)) for _ in range(2)] for x in range(int(sample_rate * duration))]))\n",
    "    waveform.play()\n",
    "    time.sleep(0.3)  # Adjust the duration of each note as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test, X_train_val, y_train_val = top_split_y(fff.X,fff.y,5)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_train_val.shape)\n",
    "print(y_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to check the D matrix used in the paper (6 rates, packed) and the one used above are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3 -3  3  1  0  2 -3  2 -3  1 -3  3]\n",
      " [ 1 -1  0  0 -1  1  0  0  0  0 -1  0 -1]\n",
      " [ 0 -1  1  0  0  0  0  1  0  1  0  1 -1]]\n",
      "[[ 1  1  2  3  0  0  3  3 -3 -3 -3 -3  2]\n",
      " [-1 -1  0  0  1  1 -1 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1 -1  1  1  1  1  0]]\n"
     ]
    }
   ],
   "source": [
    "D_in = np.array(\n",
    "[\n",
    "    [ 0 , 3, -3,  3,  1,  0,  2, -3,  2, -3,  1, -3,  3],\n",
    "    [ 1, -1,  0,  0, -1,  1,  0,  0,  0,  0, -1,  0, -1],\n",
    "    [ 0, -1,  1,  0,  0,  0,  0,  1,  0,  1,  0,  1, -1]\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# D_in: find W for paper\n",
    "D_in_paper = np.array(\n",
    "[\n",
    "    [ 1,  1,  2,  3,  0,  0,  3,  3, -3, -3, -3, -3,  2],\n",
    "    [-1, -1,  0,  0,  1,  1, -1, -1,  0,  0,  0,  0,  0],\n",
    "    [ 0,  0,  0,  0,  0,  0, -1, -1,  1,  1,  1,  1,  0]\n",
    "]\n",
    ")\n",
    "\n",
    "print(D_in)\n",
    "print(D_in_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.     1.    -1.     0.     1.     0.     1.     1.     1.     0.   ]\n",
      " [ 1.     0.     0.     0.     1.     0.     1.     0.     1.    -1.   ]\n",
      " [ 1.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.333  0.    -0.667  0.    -0.667  0.    -0.333  0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "(13, 10)\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_g = np.array(df_out.iloc[:,4])\n",
    "n_e_no_dim = n_e/n_g\n",
    "#print(n_e)\n",
    "#print(n_e_no_dim)\n",
    "output = rescale_vec(n_e_no_dim)\n",
    "#print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "with np.printoptions(precision = 3):\n",
    "    print(fff.basis_col.numpy())\n",
    "    print(fff.basis_col.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 3.   7.5  2.5  1.5 -5.   2.5 -2.5  1.5 -2.5  6. ]\n",
      " [-2.   0.   0.   2.   0.  -1.  -1.  -2.  -1.  -1. ]\n",
      " [-1.  -1.5 -0.5 -0.5  1.   0.5  1.5  0.5  1.5 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "mat = fff.basis_col.numpy()\n",
    "print(np.matmul(D_in_paper,mat))\n",
    "print(np.matmul(D_in,mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.270352083652026e-13\n",
      "4.4805095763361205e-15\n"
     ]
    }
   ],
   "source": [
    "r = n_e/n_g\n",
    "print(max(r))\n",
    "print(min(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def have_same_columns(a, b):\n",
    "    # Set of column-tuples from each array\n",
    "    set_a = {tuple(col) for col in a.T}\n",
    "    set_b = {tuple(col) for col in b.T}\n",
    "    return set_a == set_b\n",
    "result = have_same_columns(D_in, D_in_paper)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "array1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "array2 = np.array([[3, 1, 2], [6, 4, 5]])\n",
    "\n",
    "# Function to check if two arrays have the same columns regardless of order\n",
    "def have_same_columns(a, b):\n",
    "    # Set of column-tuples from each array\n",
    "    set_a = {tuple(col) for col in a.T}\n",
    "    set_b = {tuple(col) for col in b.T}\n",
    "    return set_a == set_b\n",
    "\n",
    "# Check if the arrays have the same columns\n",
    "result = have_same_columns(array1, array2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 -3  3  1  0  2 -3  2 -3  1 -3  3]\n",
      " [-1  0  0 -1  1  0  0  0  0 -1  0 -1]\n",
      " [-1  1  0  0  0  0  1  0  1  0  1 -1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "[[ 1  1  2  3  0 -3 -3 -3 -3 -3 -3  2]\n",
      " [-1 -1  0  0  1 -1 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1 -1  1  1  1  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "D_in = np.delete(D_in, 0, axis = 1)\n",
    "D_in_paper = np.delete(D_in_paper, 4, axis = 1)\n",
    "print(D_in)\n",
    "print(D_in_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
