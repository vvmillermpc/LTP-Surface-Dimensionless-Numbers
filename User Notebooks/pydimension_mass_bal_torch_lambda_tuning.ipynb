{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.163733Z",
     "start_time": "2023-05-31T02:42:22.636891Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim.utils import DimensionlessLearning\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.572932Z",
     "start_time": "2023-05-31T02:42:27.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.ExcelFile(r\"mass_balance_params.xlsx\")\n",
    "df_2 = pd.ExcelFile(r\"data_from_EB_looping.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.660878Z",
     "start_time": "2023-05-31T02:42:27.634288Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_x['u_B_m_s'] = df_train_x['u_B_m_s'] / 1000 if np.min(df_train_x['u_B_m_s']) > 0 else df_train_x['u_B_m_s'] / 1000 - (np.min(df_train_x['u_B_m_s']) / 1000-1e-5)\n",
    "# df_train_x['A_tot_m2'] = 1.\n",
    "# df_train_x['t_a_s'] = df_train_x['t_a_s'] / 1e-7 if np.min(df_train_x['t_a_s']) > 0 else df_train_x['t_a_s'] / 1e-7 - (np.min(df_train_x['t_a_s']) / 1e-7-1e-5)\n",
    "# df_train_x['t_b_s'] = df_train_x['t_b_s'] / 1e-4 if np.min(df_train_x['t_b_s']) > 0 else df_train_x['t_b_s'] / 1e-4 - (np.min(df_train_x['t_b_s']) / 1e-4-1e-5)\n",
    "# df_train_x['Volume_m3'] = 1.\n",
    "# df_train_x['K_iz_a_m3_s_atom'] = df_train_x['K_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_iz_a_m3_s_atom']) > 0 else df_train_x['K_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_2_iz_a_m3_s_atom'] = df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 if np.min(df_train_x['K_2_iz_a_m3_s_atom']) > 0 else df_train_x['K_2_iz_a_m3_s_atom'] / 1e-15 - (np.min(df_train_x['K_2_iz_a_m3_s_atom']) / 1e-15-1e-5)\n",
    "\n",
    "# df_train_x['K_iz_exc_a_m3_s_atom'] = df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 if np.min(df_train_x['K_iz_exc_a_m3_s_atom']) > 0 else df_train_x['K_iz_exc_a_m3_s_atom'] / 1e-14 - (np.min(df_train_x['K_iz_exc_a_m3_s_atom']) / 1e-14-1e-5)\n",
    "\n",
    "# df_train_x['n_sa_atoms_m3'] = df_train_x['n_sa_atoms_m3'] / 1e10 if np.min(df_train_x['n_sa_atoms_m3']) > 0 else df_train_x['n_sa_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sa_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['n_sb_atoms_m3'] = df_train_x['n_sb_atoms_m3'] / 1e10 if np.min(df_train_x['n_sb_atoms_m3']) > 0 else df_train_x['n_sb_atoms_m3'] / 1e10 - (np.min(df_train_x['n_sb_atoms_m3']) / 1e10-1e-5)\n",
    "\n",
    "# df_train_x['A_a_m2'] = 1.\n",
    "\n",
    "# df_train_x['A_b_m2'] = 1.\n",
    "\n",
    "# df_train_x['n_He_exc_a_atoms_m3'] = df_train_x['n_He_exc_a_atoms_m3'] / 1e17 if np.min(df_train_x['n_He_exc_a_atoms_m3']) > 0 else df_train_x['n_He_exc_a_atoms_m3'] / 1e17 - (np.min(df_train_x['n_He_exc_a_atoms_m3']) / 1e17-1e-5)\n",
    "\n",
    "def rescale(g):\n",
    "    for i in range(0,g.shape[1]):\n",
    "        if np.min(np.abs(g[:,i])) ==0:\n",
    "            n = 0\n",
    "        else:\n",
    "            n = np.mean((np.log10(np.min(np.abs(g[:,i]))), np.log10(np.max(np.abs(g[:,i])))))\n",
    "        if n<0:\n",
    "                g[:,i] = g[:,i]/10**np.ceil(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.ceil(n) - (np.min(g[:,i]) / 10**np.ceil(n)-1e-5)\n",
    "        else:\n",
    "            g[:,i] = g[:,i]/10**np.floor(n) if np.min(g[:,i]) > 0 else g[:,i]/10**np.floor(n) - (np.min(g[:,i]) / 10**np.floor(n)-1e-5)\n",
    "    return g\n",
    "\n",
    "def rescale_vec(g):\n",
    "    if np.min(np.abs(g)) ==0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = np.mean((np.log10(np.min(np.abs(g))), np.log10(np.max(np.abs(g)))))\n",
    "    if n<0:\n",
    "            g= g/10**np.ceil(n) if np.min(g) > 0 else g/10**np.ceil(n) - (np.min(g) / 10**np.ceil(n)-1e-5)\n",
    "    else:\n",
    "        g = g/10**np.floor(n) if np.min(g) > 0 else g/10**np.floor(n) - (np.min(g) / 10**np.floor(n)-1e-5)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.706539Z",
     "start_time": "2023-05-31T02:42:27.668685Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u_B_m_s', 'A_tot_m3', 't_a_seconds', 't_b_seconds', 'Volume_m3', 'K_iz_a_m3_s_atom', 'K_2_iz_a_m3_s_atom', 'K_iz_exc_a_m3_s_atom', 'n_sa_atoms_m3', 'n_sb_atoms_m3', 'n_He_exc_a_atoms_m3']\n",
      "[[ 1  2  0  0  3  3  3  3 -3 -3 -3]\n",
      " [-1  0  1  1  0 -1 -1 -1  0  0  0]\n",
      " [ 0  0  0  0  0 -1 -1 -1  1  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#                   0             1           2                    3                   4                   5           6                7                    8                     9                        10                  11                  12                      13                          \n",
    "#df1_names = [ 't_a_seconds', 'Q_a_As', 'V_p_ta_kgm2_s3_A','T_e_a_kgm2_s3_A', 'n_He_exc_a_atoms_m3', 'u_B_a_m_s', 'v_e_a_m_s', 'K_2_iz_a_m3_s_atom','K_loss_a_m6_s_atom2', 'K_iz_exc_a_m3_s_atom', 'K_exc_a_m3_s_atom', 'K_iz_a_m3_s_atom', 'K_elastic_a_m3_s_atom', 'E_elastic_a_kgm2_s2' ]\n",
    "\n",
    "\n",
    "df_time_a = rescale(np.array(pd.read_excel(df_2, sheet_name='time_a_data').iloc[:,1:]))\n",
    "df_time_a_units = np.array(pd.read_excel(df_2, sheet_name='time_a_data_units').iloc[:,1:])\n",
    "df_time_a_n = pd.read_excel(df_2, sheet_name='time_a_data_names').iloc[:,1:]\n",
    "time_a_n= [df_time_a_n.iloc[0,i] for i in range(0,df_time_a_n.shape[1])]\n",
    "#print(df_time_a_n)\n",
    "#print(df_time_a_units)\n",
    "\n",
    "\n",
    "#                   0             1                2               3                   4              5          6             7                   8                    9                       10              11                   12                   13                  \n",
    "#df2_names =  [ 't_b_seconds', 'Q_b_As', 'V_p_tb_kgm2_s3_A','T_e_kgm2_s3_A', 'n_He_exc_atoms_m3', 'u_B_m_s', 'v_e_m_s','K_2_iz_m3_s_atom','K_loss_m6_s_atom2', 'K_iz_exc_m3_s_atom', 'K_exc_m3_s_atom', 'K_iz_m3_s_atom', 'K_elastic_m3_s_atom', 'E_elastic_kgm2_s2' ]\n",
    "df_time_b = rescale(np.array(pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]))\n",
    "df_time_b_units = np.array(pd.read_excel(df_2, sheet_name='time_b_data_units').iloc[:,1:])\n",
    "df_time_b_n = pd.read_excel(df_2, sheet_name='time_b_data_names').iloc[:,1:]\n",
    "time_b_n= [df_time_b_n.iloc[0,i] for i in range(0,df_time_b_n.shape[1])]\n",
    "\n",
    "\n",
    "#                    0                   1                 2               3                 4               5               6                 7                  8           9        10        11                12                   13               14                   \n",
    "#df3_names = ['E_period_kgm2_s2', 'n_sa_atoms_m3','n_sb_atoms_m3', 'n_e_electrons_m3', 'n_g_atoms_m3', 'T_g_kelvin', 'E_iz_kgm2_s2', 'E_iz_exc_kgm2_s2', 'E_exc_kgm2_s2', 'e_c_As', 'm_e_kg', 'M_He_kg',  'epsilon_A2s4_kg_m3', 'eps_0_A2s4_kg_m3', 'k_b_kgm2_s2_K']\n",
    "df_other = rescale(np.array(pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]))\n",
    "df_other_units = np.array(pd.read_excel(df_2, sheet_name='other_data_units').iloc[:,1:])\n",
    "df_other_n = pd.read_excel(df_2, sheet_name='other_data_names').iloc[:,1:]\n",
    "other_n= [df_other_n.iloc[0,i] for i in range(0,df_other_n.shape[1])]\n",
    "\n",
    "#                    0                 1             2        3        4         5            6            7            8               9              10                 11                      12                      \n",
    "#df4_names = ['Volume_rxtor_m2', 'V_all_beads_m2','A_a_m2','A_b_m2', 'h_m', 'Volume_m3', 'A_bead_m2', 'A_tot_m3', 'frequency_Hz', 'Flow_m3_s', 'temp_C_gas_K', 'Set_Voltage_kgm2_s3_A', 'pulse_time_seconds' ]\n",
    "df_exp = rescale(np.array(pd.read_excel(df_2, sheet_name='Experiment_Design_data').iloc[:,1:]))\n",
    "df_exp_units = np.array(pd.read_excel(df_2, sheet_name='Experiment_Data_units').iloc[:,1:])\n",
    "df_exp_n = pd.read_excel(df_2, sheet_name='Experiment_Data_names').iloc[:,1:]\n",
    "exp_n= [df_exp_n.iloc[0,i] for i in range(0,df_exp_n.shape[1])]\n",
    "\n",
    "\n",
    "\n",
    "#base terms + F +K_iz, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9], K_iz is time_b_n[11]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1),df_time_b[:,11].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1),df_time_b_units[:,11].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9], time_b_n[11]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "#base terms + F, base terms: 'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "#F is exp_n[9]\n",
    "#a = df_time_b[:,5].shape[0]\n",
    "#inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1),df_exp[:,9].reshape(a,1)))\n",
    "#D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1),df_exp_units[:,9].reshape(6,1)))\n",
    "#variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4],exp_n[9]]\n",
    "#print(variables)\n",
    "#print(D_in)\n",
    "\n",
    "\n",
    "#base terms  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "a = df_time_b[:,5].shape[0]\n",
    "inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "print(variables)\n",
    "print(D_in)\n",
    "\n",
    "#base terms -A,V 'uB',  'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb',  'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0]\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_exp[:,2].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],time_a_n[0],time_b_n[0],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n",
    "\n",
    "#to compare to ketong's original code only\n",
    "#not to construct  'uB', 'Atot', 'ta', 'tb', 'Volume', 'Ka', 'K2a', 'Kexca', 'nsa', 'nsb', 'Aa', 'Ab', 'nHeexca,\n",
    "# a = df_time_b[:,5].shape[0] #this is just the number of data points\n",
    "# inputs = np.hstack((df_time_b[:,5].reshape(a,1),df_exp[:,7].reshape(a,1),df_time_a[:,0].reshape(a,1),df_time_b[:,0].reshape(a,1),df_exp[:,5].reshape(a,1),df_time_a[:,11].reshape(a,1),df_time_a[:,7].reshape(a,1),df_time_a[:,9].reshape(a,1),df_other[:,1].reshape(a,1),df_other[:,2].reshape(a,1),df_exp[:,2].reshape(a,1),df_exp[:,3].reshape(a,1),df_time_a[:,4].reshape(a,1)))\n",
    "# D_in = np.hstack((df_time_b_units[:,5].reshape(6,1),df_exp_units[:,7].reshape(6,1),df_time_a_units[:,0].reshape(6,1),df_time_b_units[:,0].reshape(6,1),df_exp_units[:,5].reshape(6,1),df_time_a_units[:,11].reshape(6,1),df_time_a_units[:,7].reshape(6,1),df_time_a_units[:,9].reshape(6,1),df_other_units[:,1].reshape(6,1),df_other_units[:,2].reshape(6,1),df_exp_units[:,2].reshape(6,1),df_exp_units[:,3].reshape(6,1),df_time_a_units[:,4].reshape(6,1)))\n",
    "# variables = [time_b_n[5],exp_n[7],time_a_n[0],time_b_n[0],exp_n[5],time_a_n[11],time_a_n[7],time_a_n[9],other_n[1],other_n[2],exp_n[2],exp_n[3],time_a_n[4]]\n",
    "# print(variables)\n",
    "# print(D_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 11)\n",
      "(6, 11)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(D_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.26113486 3.10828496 2.54307505 2.05619627 1.20402122 1.2993149\n",
      " 1.30526692 1.15823665 1.34387547 1.42088813 1.60032635 1.33582501\n",
      " 1.48405186 1.48523378 1.44093946 1.4505196  1.60338152 1.98006436\n",
      " 1.9619605  1.78849676 1.83543198 2.01147344 1.91580601 1.98361517\n",
      " 1.91017132 1.72902634 2.20856355 1.97487417 1.90063144 1.76579201\n",
      " 1.80436684 1.82210651 1.83056756 2.27180795 2.1612181  2.18948516\n",
      " 2.16765091 1.897429   1.81537044 1.77567056 2.42912499 1.65745035\n",
      " 2.30081106 2.64058556 2.62069997 2.47770082 2.43801579 2.61674862\n",
      " 3.01682255 2.98551177 2.42120857 2.5121597  1.45937712 2.41146737\n",
      " 2.69539813 3.5406239  4.76007049 3.61957343 3.51873218 3.25990083\n",
      " 3.90437076 3.24477505 3.27964645 6.95023274 3.5316073  3.89892172\n",
      " 3.9778323  5.33198036 0.67153854 0.66277982 0.56847795 1.06035583\n",
      " 0.69904743 0.91830486 0.7752149  1.03536608 1.23430341 1.096202\n",
      " 1.13535006 1.11327591 1.13980388 1.03550709 1.02522166 0.48361467\n",
      " 0.63737485 0.61924916 0.55005514 0.89612288 1.03649535 1.03615097\n",
      " 0.99197405 0.80294313 0.91496083 0.80767458 0.98374083 1.1845759\n",
      " 1.12684647 1.05722953 1.07874106 0.85193389 1.06838328 0.93993951\n",
      " 0.96551607 1.44521063 1.37667945 1.23351769 1.2934096  1.53586776\n",
      " 1.66388291 1.32666554 1.93486709 1.75743922 1.6776613  1.51749749\n",
      " 1.95352179 1.33612408 1.53688942 2.80712037 1.83604207 1.26926064\n",
      " 1.97859452 2.94283471 2.3307824  5.68882283 1.89891151 8.06763562\n",
      " 1.58254021 4.30299282 0.68712824 0.81792997 0.76436847 0.74197954\n",
      " 0.93121743 1.07798586 0.64295157 0.8279213  1.06175462 1.17312793\n",
      " 1.02481254 1.08105055 1.17529906 1.27036814 1.24899603 1.23216845\n",
      " 1.06646094 1.0733857  1.04183041 1.33313868 1.33225621 1.27711808\n",
      " 1.49784616 2.06078406 1.64988605 1.7164319  1.34623123 2.81076062\n",
      " 1.70575266 1.93471703 1.70663914 1.28080975 1.76272931 2.02978938\n",
      " 3.11273374 1.95103037 2.34204227 2.50029467]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.707010Z",
     "start_time": "2023-05-31T02:42:27.700438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.58309060e+12 4.09791587e+12 3.70354896e+12 3.09336680e+12\n",
      " 1.91952999e+12 2.21022048e+12 2.23385412e+12 2.13639165e+12\n",
      " 2.35983478e+12 2.49036030e+12 3.03167523e+12 2.84210352e+12\n",
      " 3.18668511e+12 3.44177523e+12 3.54603983e+12 3.67296116e+12\n",
      " 4.31186248e+12 4.33912928e+12 4.73376403e+12 5.00551030e+12\n",
      " 4.98515118e+12 5.72909992e+12 5.89117393e+12 6.26848555e+12\n",
      " 6.45399992e+12 6.20983876e+12 5.67551914e+12 5.45777510e+12\n",
      " 5.72671834e+12 5.61130951e+12 6.00917947e+12 6.64918696e+12\n",
      " 6.84402475e+12 8.57360434e+12 6.40318992e+12 6.75711285e+12\n",
      " 7.31469123e+12 6.75996292e+12 7.03896384e+12 7.29018020e+12\n",
      " 1.03134954e+13 9.16691521e+12 1.08864078e+13 8.43394263e+12\n",
      " 9.14620112e+12 9.22868952e+12 1.08388820e+13 1.36449201e+13\n",
      " 1.13943161e+13 1.32970690e+13 1.25674808e+13 1.53250521e+13\n",
      " 2.04441246e+12 4.12911430e+12 5.94276348e+12 9.54453782e+12\n",
      " 1.54484782e+13 1.35689855e+13 1.51597471e+13 1.11442229e+13\n",
      " 1.61075216e+13 1.58773965e+13 3.97798005e+12 1.06795461e+13\n",
      " 8.02225240e+12 1.21279390e+13 1.36550997e+13 2.15119856e+13\n",
      " 9.54603081e+11 1.09286705e+12 1.06895300e+12 2.27908501e+12\n",
      " 1.82962663e+12 2.80345219e+12 2.72280991e+12 3.36168637e+12\n",
      " 4.77716095e+12 4.95653271e+12 5.72872363e+12 4.47937234e+12\n",
      " 5.44322828e+12 5.74694958e+12 6.46346414e+12 6.02051833e+11\n",
      " 9.26303469e+11 1.03633786e+12 1.05463979e+12 2.03406026e+12\n",
      " 2.79055108e+12 3.26011545e+12 3.53340298e+12 2.53146906e+12\n",
      " 3.56468377e+12 3.61208449e+12 4.90264288e+12 4.82734562e+12\n",
      " 5.43850250e+12 6.09070618e+12 6.76641252e+12 1.00682543e+12\n",
      " 1.63779679e+12 1.64501598e+12 1.87510454e+12 3.19534798e+12\n",
      " 3.68237532e+12 3.95141956e+12 4.56896201e+12 5.19291198e+12\n",
      " 6.50645462e+12 5.91988629e+12 1.01313693e+13 6.70742491e+12\n",
      " 7.76592958e+12 8.16379486e+12 1.22913890e+13 1.58134408e+12\n",
      " 2.21685079e+12 5.01091086e+12 4.08247406e+12 4.88877319e+12\n",
      " 6.38128465e+12 1.10995794e+13 8.66661034e+12 3.07498545e+13\n",
      " 2.23450684e+12 1.22168749e+13 5.48043755e+12 1.48521595e+13\n",
      " 8.45422979e+11 1.17730143e+12 1.28790306e+12 1.43005980e+12\n",
      " 2.05422521e+12 2.85026012e+12 2.63453343e+12 3.21237719e+12\n",
      " 3.33181170e+12 4.50783723e+12 4.58792783e+12 5.53610802e+12\n",
      " 4.59290474e+12 6.06728568e+12 6.90531116e+12 7.77906258e+12\n",
      " 1.27048979e+12 1.55694376e+12 1.75367009e+12 2.63536091e+12\n",
      " 2.91554759e+12 4.05298272e+12 4.64420672e+12 7.40173995e+12\n",
      " 5.30327737e+12 6.41469994e+12 5.96161956e+12 1.47084221e+13\n",
      " 6.57218355e+12 8.87005422e+12 9.13162198e+12 1.61805551e+12\n",
      " 2.70030114e+12 4.61197461e+12 8.55483764e+12 6.15545958e+12\n",
      " 8.66444286e+12 3.06792249e+12]\n",
      "[5.44796880e-14 1.41023627e-13 1.27452081e-13 1.06453578e-13\n",
      " 6.60577446e-14 7.60614218e-14 7.68747382e-14 7.35207133e-14\n",
      " 8.12101736e-14 8.57020134e-14 1.04330555e-13 9.78067286e-14\n",
      " 1.09664987e-13 1.18443531e-13 1.22031641e-13 1.26399448e-13\n",
      " 1.48386278e-13 1.49324624e-13 1.62905387e-13 1.72257127e-13\n",
      " 1.71556499e-13 1.97158378e-13 2.02735912e-13 2.15720526e-13\n",
      " 2.22104724e-13 2.13702284e-13 1.95314476e-13 1.87821142e-13\n",
      " 1.97076420e-13 1.93104798e-13 2.06796896e-13 2.28821794e-13\n",
      " 2.35526844e-13 2.95047731e-13 2.20356175e-13 2.32535901e-13\n",
      " 2.51724124e-13 2.32633982e-13 2.42235380e-13 2.50880614e-13\n",
      " 3.54923471e-13 3.15465634e-13 3.74639391e-13 2.90241482e-13\n",
      " 3.14752790e-13 3.17591504e-13 3.73003862e-13 4.69569453e-13\n",
      " 3.92118295e-13 4.57598682e-13 4.32490996e-13 5.27388675e-13\n",
      " 7.03553875e-14 1.42097274e-13 2.04511289e-13 3.28460949e-13\n",
      " 5.31636202e-13 4.66956279e-13 5.21699952e-13 3.83511710e-13\n",
      " 5.54316190e-13 5.46396780e-13 1.36896215e-13 3.67520556e-13\n",
      " 2.76073780e-13 4.17364824e-13 4.69919770e-13 7.40302711e-13\n",
      " 3.28512329e-14 3.76093800e-14 3.67864136e-14 7.84312915e-14\n",
      " 6.29638554e-14 9.64766010e-14 9.37014179e-14 1.15687393e-13\n",
      " 1.64398827e-13 1.70571637e-13 1.97145429e-13 1.54150879e-13\n",
      " 1.87320535e-13 1.97772648e-13 2.22430421e-13 2.07187106e-14\n",
      " 3.18773441e-14 3.56640126e-14 3.62938462e-14 6.99991323e-14\n",
      " 9.60326288e-14 1.12191982e-13 1.21596762e-13 8.71167097e-14\n",
      " 1.22673244e-13 1.24304468e-13 1.68717099e-13 1.66125857e-13\n",
      " 1.87157904e-13 2.09602515e-13 2.32855935e-13 3.46483865e-14\n",
      " 5.63623195e-14 5.66107570e-14 6.45289096e-14 1.09963107e-13\n",
      " 1.26723422e-13 1.35982176e-13 1.57233973e-13 1.78706275e-13\n",
      " 2.23909874e-13 2.03724005e-13 3.48655874e-13 2.30825965e-13\n",
      " 2.67252815e-13 2.80944752e-13 4.22989713e-13 5.44195842e-14\n",
      " 7.62895941e-14 1.72442979e-13 1.40492220e-13 1.68239795e-13\n",
      " 2.19602337e-13 3.81975373e-13 2.98248393e-13 1.05821011e-12\n",
      " 7.68972005e-14 4.20425421e-13 1.88601036e-13 5.11114786e-13\n",
      " 2.90939634e-14 4.05150625e-14 4.43212515e-14 4.92133624e-14\n",
      " 7.06930786e-14 9.80874259e-14 9.06635153e-14 1.10549142e-13\n",
      " 1.14659301e-13 1.55130455e-13 1.57886653e-13 1.90516852e-13\n",
      " 1.58057926e-13 2.08796534e-13 2.37635924e-13 2.67704768e-13\n",
      " 4.37219999e-14 5.35798838e-14 6.03499251e-14 9.06919920e-14\n",
      " 1.00334196e-13 1.39477320e-13 1.59823407e-13 2.54719775e-13\n",
      " 1.82504334e-13 2.20752275e-13 2.05160193e-13 5.06168278e-13\n",
      " 2.26171837e-13 3.05249608e-13 3.14251070e-13 5.56829529e-14\n",
      " 9.29268124e-14 1.58714187e-13 2.94401903e-13 2.11830906e-13\n",
      " 2.98173802e-13 1.05577950e-13]\n",
      "[0.05447969 0.14102363 0.12745208 0.10645358 0.06605774 0.07606142\n",
      " 0.07687474 0.07352071 0.08121017 0.08570201 0.10433055 0.09780673\n",
      " 0.10966499 0.11844353 0.12203164 0.12639945 0.14838628 0.14932462\n",
      " 0.16290539 0.17225713 0.1715565  0.19715838 0.20273591 0.21572053\n",
      " 0.22210472 0.21370228 0.19531448 0.18782114 0.19707642 0.1931048\n",
      " 0.2067969  0.22882179 0.23552684 0.29504773 0.22035617 0.2325359\n",
      " 0.25172412 0.23263398 0.24223538 0.25088061 0.35492347 0.31546563\n",
      " 0.37463939 0.29024148 0.31475279 0.3175915  0.37300386 0.46956945\n",
      " 0.39211829 0.45759868 0.432491   0.52738867 0.07035539 0.14209727\n",
      " 0.20451129 0.32846095 0.5316362  0.46695628 0.52169995 0.38351171\n",
      " 0.55431619 0.54639678 0.13689622 0.36752056 0.27607378 0.41736482\n",
      " 0.46991977 0.74030271 0.03285123 0.03760938 0.03678641 0.07843129\n",
      " 0.06296386 0.0964766  0.09370142 0.11568739 0.16439883 0.17057164\n",
      " 0.19714543 0.15415088 0.18732053 0.19777265 0.22243042 0.02071871\n",
      " 0.03187734 0.03566401 0.03629385 0.06999913 0.09603263 0.11219198\n",
      " 0.12159676 0.08711671 0.12267324 0.12430447 0.1687171  0.16612586\n",
      " 0.1871579  0.20960252 0.23285594 0.03464839 0.05636232 0.05661076\n",
      " 0.06452891 0.10996311 0.12672342 0.13598218 0.15723397 0.17870627\n",
      " 0.22390987 0.20372401 0.34865587 0.23082597 0.26725282 0.28094475\n",
      " 0.42298971 0.05441958 0.07628959 0.17244298 0.14049222 0.16823979\n",
      " 0.21960234 0.38197537 0.29824839 1.05821011 0.0768972  0.42042542\n",
      " 0.18860104 0.51111479 0.02909396 0.04051506 0.04432125 0.04921336\n",
      " 0.07069308 0.09808743 0.09066352 0.11054914 0.1146593  0.15513046\n",
      " 0.15788665 0.19051685 0.15805793 0.20879653 0.23763592 0.26770477\n",
      " 0.043722   0.05357988 0.06034993 0.09069199 0.1003342  0.13947732\n",
      " 0.15982341 0.25471978 0.18250433 0.22075227 0.20516019 0.50616828\n",
      " 0.22617184 0.30524961 0.31425107 0.05568295 0.09292681 0.15871419\n",
      " 0.2944019  0.21183091 0.2981738  0.10557795]\n"
     ]
    }
   ],
   "source": [
    "#For predicting Te/Tg: keep top block, comment bottom block. For predicting ne/ng, keep bottom block, comment top block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TOP BLOCK ####\n",
    "\n",
    "# df_out = pd.read_excel(df_2, sheet_name='time_b_data').iloc[:,1:]\n",
    "# T_e = np.array(df_out.iloc[:,3])\n",
    "# df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "# T_g = np.array(df_out.iloc[:,5])*0.026/297\n",
    "# T_e_no_dim = T_e/T_g\n",
    "# print(T_e)\n",
    "# output = rescale_vec(T_e_no_dim)\n",
    "# print(output)\n",
    "# D_out = np.array(\n",
    "#     [\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### BOTTOM BLOCK ####\n",
    "\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_e = np.array(df_out.iloc[:,3])\n",
    "df_out = pd.read_excel(df_2, sheet_name='other_data').iloc[:,1:]\n",
    "n_g = np.array(df_out.iloc[:,4])\n",
    "n_e_no_dim = n_e/n_g\n",
    "print(n_e)\n",
    "print(n_e_no_dim)\n",
    "output = rescale_vec(n_e_no_dim)\n",
    "print(output)\n",
    "D_out = np.array(\n",
    "    [\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0,],\n",
    "        [0.]\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.754325Z",
     "start_time": "2023-05-31T02:42:27.738571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [-0.5000, -0.5000, -1.5000,  0.0000,  0.0000,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff = DimensionlessLearning(D_in, D_out)\n",
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [-0.5000, -0.5000, -1.5000,  0.0000,  0.0000,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.basis_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.843064Z",
     "start_time": "2023-05-31T02:42:27.750866Z"
    }
   },
   "outputs": [],
   "source": [
    "fff.read_data(inputs, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.883791Z",
     "start_time": "2023-05-31T02:42:27.758677Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDBDdim import PiLinearRegressionViaTorch\n",
    "\n",
    "\n",
    "#the code does five fold to ensure the R2 is worth while.\n",
    "# we then want to rerun the fitting on all data instead of 80% of it. The 5fold find the right number of epochs.\n",
    "# so, we re run to find the right number of epochs, and then we must store the parameters and the R2 from the 5-fold fitting.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:42:27.947167Z",
     "start_time": "2023-05-31T02:42:27.779139Z"
    }
   },
   "outputs": [],
   "source": [
    "#UPDATE RUN PARAMETERS. POLYNOMIAL ORDER? # of dim groups? Which variables are called? etc.\n",
    "\n",
    "\n",
    "ndimensionless = 1\n",
    "\n",
    "lambda_gamma = 0.0003 #lambda_gamma = 0.01, #replaced\n",
    "seed = 22\n",
    "\n",
    "poly_order = 1\n",
    "\n",
    "poly_mapping = np.array([[0],\n",
    "                         [1]])\n",
    "\n",
    "#poly_mapping = np.array([[0, 0],\n",
    "#                         [1, 0],\n",
    "#                         [0, 1],\n",
    "#                         [2, 0],\n",
    "#                         [1, 1],\n",
    "#                         [0, 2]])\n",
    "lambda_beta = 0.01 #maybe cut in half idk\n",
    "w_array = np.array(fff._basis_col)  #this is the w array, that is, the columns in Null(D)\n",
    "gamma_name = ['y'+str(id) for id in range(0,fff.basis_col.shape[1]) ]\n",
    "beta_name = ['b'+str(id) for id in range(0,poly_mapping.shape[0]) ]\n",
    "poly_name = ['dim'+str(id+1) for id in range(0,ndimensionless)]\n",
    "\n",
    "\n",
    "\n",
    "metric = 'r2'\n",
    "para_threshold = 0.005\n",
    "beta_threshold = 0.005\n",
    "training_epochs =10000\n",
    "score = []\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#create test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create validation and training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "####################################################\n",
    "# SOLVE FOR DIMENSIONLESS NUMBERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\\1dim_order1_lambda0.0003_seed22' did not exist.\n",
      "C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\\1dim_order1_lambda0.0003_seed22\\1dim_order1_lambda0.0003_seed22dim1\n"
     ]
    }
   ],
   "source": [
    "#set save path\n",
    "\n",
    "current_path = r\"C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\"\n",
    "#new_folder =r'mb_1dim_v1_pt0001_pt_01_order1_golden_child'\n",
    "new_folder = str(ndimensionless)+r'dim_order'+str(poly_order)+r'_lambda'+str(lambda_gamma)+r\"_seed\"+str(seed)\n",
    "new_path = current_path+'\\\\'+new_folder\n",
    "if os.path.exists(new_path):\n",
    "    print(f\"File '{new_path}' already exists.\")\n",
    "else:\n",
    "    os.mkdir(new_path)       \n",
    "    print(f\"File '{new_path}' did not exist.\")\n",
    "\n",
    "file_path = new_path+'\\\\'+new_folder +'_seed'+str(seed)+ '.xlsx'\n",
    "fig_path_dim1 = new_path+'\\\\'+new_folder+\"dim1\"\n",
    "fig_path_dim2 = new_path+'\\\\'+new_folder+\"dim2\"\n",
    "print(fig_path_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0009089191133882072 at iteration 9999, val_loss: 0.996077240363745, best_val_loss: 0.9981022642558471: 100%|██████████| 10000/10000 [00:33<00:00, 297.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9981022642558471\n",
      "tensor([[-0.0000],\n",
      "        [-0.1764],\n",
      "        [-0.4219],\n",
      "        [ 0.0000],\n",
      "        [ 0.4845],\n",
      "        [-0.1064],\n",
      "        [-0.8641],\n",
      "        [ 0.9589]], dtype=torch.float64)\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#find which hyperparameter performed best\n",
    "\n",
    "# idx = np.arange(lg_set.shape[0])\n",
    "# best_hp = idx[array_metric_num[:,0]==np.max(array_metric_num)]\n",
    "\n",
    "# if len(best_hp.shape)>1:\n",
    "#     print('the following are equivalent ',best_hp)\n",
    "#     print(best_hp.shape)\n",
    "#     best_hp = best_hp[0,1]\n",
    "\n",
    "\n",
    "#22 best so far, R2 = 0.998 for lambda = 0.001\n",
    "#14 best so far, R2 = 0.945\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(fff.X, fff.y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "best_hp = lambda_gamma\n",
    "\n",
    "\n",
    "model_train = PiLinearRegressionViaTorch.TrainHolder(X_train_val, y_train_val, poly_mapping.shape[0], fff.y.shape[-1], poly_mapping, fff.basis_col, ndimensionless, lambda_gamma, lambda_beta, lowest_para_threshold=para_threshold)\n",
    "metric_num = model_train.train(training_epochs, True, val_x=X_test, val_y =y_test , metric=metric, norm_on='null_space')\n",
    "\n",
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "r2 = model_train.get_validation_metric(X_test,y_test,metric)\n",
    "print(r2)\n",
    "\n",
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "\n",
    "\n",
    "#store values from training\n",
    "best_metric_num = np.array(metric_num)\n",
    "best_beta = beta_prune\n",
    "\n",
    "best_dim1_gamma = np.asarray(t.transpose(0,1)[0]) # gamma corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_gamma = np.sum(np.where(best_dim1_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 1\n",
    "best_dim1_w = np.matmul(w_array, best_dim1_gamma)  # w corresponding to dimensionless number 1\n",
    "best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1)) #non-zero elements in w corresponding to dim #1\n",
    "\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)\n",
    "\n",
    "\n",
    "\n",
    "if ndimensionless >=2:\n",
    "    best_dim2_gamma = np.asarray(t.transpose(0,1)[1]) # gamma corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_gamma = np.sum(np.where(best_dim2_gamma==0,0,1)) #non-zero elements corresponding to gamma for dim # 2\n",
    "    best_dim2_w = np.matmul(w_array, best_dim2_gamma)  # w corresponding to dimensionless number 2\n",
    "    best_cardinality_dim2_w = np.sum(np.where(best_dim2_w==0,0,1)) #non-zero elements in w corresponding to dim #2\n",
    "\n",
    "\n",
    "#r2, lambda, beta vector , cardinality \n",
    "best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,      best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_w.reshape(1,1)))\n",
    "data_name =  [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "\n",
    "df_best_dim1 = pd.DataFrame(data=best_data_dim1, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "##### BELOW MUST BE CORRECTED#####\n",
    "if ndimensionless >=2:\n",
    "    \n",
    "    best_data_dim2 = np.hstack((best_metric_num.reshape(1,1),   best_hp.reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1),    best_dim2_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim2_gamma.reshape(1,1)))\n",
    "    df_best_dim2 = pd.DataFrame(data=best_data_dim2, columns = [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])\n",
    "\n",
    "#df_best_dim1 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1)\n",
      "torch.Size([1, 2])\n",
      "(1, 1)\n",
      "(1, 11)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "print(best_metric_num.reshape(1,1).shape)\n",
    "print(np.array([best_hp]).reshape(1,1).shape)\n",
    "print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 11)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "#best_data_dim1 = np.hstack((best_metric_num.reshape(1,1),   np.array([best_hp]).reshape(1,1),  best_beta[0].reshape(1,poly_mapping.shape[0]),  best_cardinality_dim1_gamma.reshape(1,1) ,best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]) ,    best_cardinality_dim1_gamma.reshape(1,1),    best_dim1_w.reshape(1,best_dim1_w.shape[0]),   best_cardinality_dim1_gamma.reshape(1,1)))\n",
    "#data_name =  [ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1']\n",
    "#print(best_metric_num.reshape(1,1).shape)\n",
    "#print(np.array([best_hp]).reshape(1,1).shape)\n",
    "#print(best_beta[0].reshape(1,poly_mapping.shape[0]).shape)\n",
    "#print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "#print(best_dim1_gamma.reshape(1,best_dim1_gamma.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)\n",
    "print(best_dim1_w.reshape(1,best_dim1_w.shape[0]).shape)\n",
    "print(best_cardinality_dim1_gamma.reshape(1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#[ metric] + ['lambda']+ [beta_name[i] for i in range(0,len(beta_name))]+['cardinality_of_gamma_dim1']+[gamma_name[i] for i in range(0,len(gamma_name))]+[variables[i] for i in range(0,len(variables))]+['cardinality_of_w_dim1'])print(len([ metric]))\n",
    "#print(len([metric]))\n",
    "#print(len( ['lambda']))\n",
    "#print(len( [beta_name[i] for i in range(0,len(beta_name))]))\n",
    "#print(len(['cardinality_of_gamma_dim1']))\n",
    "#print(len([gamma_name[i] for i in range(0,len(gamma_name))]))\n",
    "print(len([variables[i] for i in range(0,len(variables))]))\n",
    "print(len(['cardinality_of_w_dim1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[-0.1648139   0.71526042  0.         -0.17638469 -0.42190232 -0.49605429\n",
      "  0.          0.48448349 -0.10637623 -0.86407044  0.95887587]\n"
     ]
    }
   ],
   "source": [
    "print(best_cardinality_dim1_w)\n",
    "print(best_dim1_w)\n",
    "#best_cardinality_dim1_w = np.sum(np.where(best_dim1_w==0,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-7.0663e-04],\n",
      "        [-1.7638e-01],\n",
      "        [-4.2190e-01],\n",
      "        [ 1.0740e-04],\n",
      "        [ 4.8448e-01],\n",
      "        [-1.0638e-01],\n",
      "        [-8.6407e-01],\n",
      "        [ 9.5888e-01]], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0048,  0.0121]], dtype=torch.float64, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model_train.model.load_state_dict(torch.load('best_model.pt'))\n",
    "print([i for i in model_train.model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000],\n",
      "        [-0.1764],\n",
      "        [-0.4219],\n",
      "        [ 0.0000],\n",
      "        [ 0.4845],\n",
      "        [-0.1064],\n",
      "        [-0.8641],\n",
      "        [ 0.9589]], dtype=torch.float64)\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n"
     ]
    }
   ],
   "source": [
    "paras = [j for j in model_train.model.parameters()]\n",
    "gamma = paras[0].detach()\n",
    "beta = paras[1].detach()\n",
    "\n",
    "#clamp them\n",
    "t = gamma*(torch.abs(gamma) > para_threshold)\n",
    "beta_prune = beta*(torch.abs(beta) > beta_threshold)\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{A_{tot m3}^{0.71526042488953} K_{iz exc a m3 s atom}^{0.484483488098862} n_{He exc a atoms m3}^{0.958875872823819}}{K_{iz a m3 s atom}^{0.496054285912162} Volume_{m3}^{0.421902317793513} n_{sa atoms m3}^{0.106376230038761} n_{sb atoms m3}^{0.864070440598358} t_{b seconds}^{0.176384694211821} u_{B m s}^{0.164813896398521}}$"
      ],
      "text/plain": [
       "A_tot_m3**0.71526042488953*K_iz_exc_a_m3_s_atom**0.484483488098862*n_He_exc_a_atoms_m3**0.958875872823819/(K_iz_a_m3_s_atom**0.496054285912162*Volume_m3**0.421902317793513*n_sa_atoms_m3**0.106376230038761*n_sb_atoms_m3**0.864070440598358*t_b_seconds**0.176384694211821*u_B_m_s**0.164813896398521)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you need the orignial pis\n",
    "readable_para = torch.cat((t.transpose(0,1).reshape(-1), torch.tensor([-8, poly_order])))\n",
    "ori_pis = fff.get_symbolic_pis(readable_para, variables)\n",
    "ori_pis[0]\n",
    "#ori_pis[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000],\n",
      "        [-0.1764],\n",
      "        [-0.4219],\n",
      "        [ 0.0000],\n",
      "        [ 0.4845],\n",
      "        [-0.1064],\n",
      "        [-0.8641],\n",
      "        [ 0.9589]], dtype=torch.float64)\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n"
     ]
    }
   ],
   "source": [
    "ori_pis[0]\n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.22044605e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(np.matmul(D_in,best_dim1_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\vvmil\\Documents\\Python_Vmil\\Jupyter_Notebooks\\Plasma_He_calcs\\dimensionless_numbers_mb\\limited_terms\\single_lambdas\\1dim_order1_lambda0.0003_seed22\\1dim_order1_lambda0.0003_seed22_seed22.xlsx' does not exist.\n",
      "tensor([[-0.0000],\n",
      "        [-0.1764],\n",
      "        [-0.4219],\n",
      "        [ 0.0000],\n",
      "        [ 0.4845],\n",
      "        [-0.1064],\n",
      "        [-0.8641],\n",
      "        [ 0.9589]], dtype=torch.float64)\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n",
      "[-0.         -0.17638469 -0.42190232  0.          0.48448349 -0.10637623\n",
      " -0.86407044  0.95887587]\n"
     ]
    }
   ],
   "source": [
    "# File path of the Excel file\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "\n",
    "\n",
    "df0 = pd.DataFrame(data=D_in,columns=variables) #first sheet, list dimension vector\n",
    "\n",
    "df1 = pd.DataFrame(data=np.transpose(w_array),columns=variables) #second sheet, list nullspace vectors and associated variables\n",
    "\n",
    "\n",
    "df4 =  pd.DataFrame(data = best_beta, columns =  [beta_name[i] for i in range(0,len(beta_name))] )\n",
    "df5 = pd.DataFrame(data = poly_mapping, columns = [poly_name[i] for i in range(0,ndimensionless)])\n",
    "with pd.ExcelWriter(file_path) as writer:  \n",
    "    df0.to_excel(writer, sheet_name='dimension_matrix')\n",
    "    df1.to_excel(writer, sheet_name='null_space_matrix')\n",
    "    df4.to_excel(writer, sheet_name='beta')\n",
    "    df5.to_excel(writer, sheet_name='polynomial')\n",
    "    df_best_dim1.to_excel(writer, sheet_name='best_dim_1')\n",
    "\n",
    "    \n",
    "    \n",
    "print(t)\n",
    "print(np.asarray(t.transpose(0,1)[0]))\n",
    "print(best_dim1_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import winsound\n",
    "#frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "#duration = 150  # Set Duration To 1000 ms == 1 second\n",
    "#winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
